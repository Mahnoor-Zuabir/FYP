{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7609321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'color' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m, in \u001b[0;36mMainWindow.update_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_label\u001b[38;5;241m.\u001b[39msetPixmap(QPixmap\u001b[38;5;241m.\u001b[39mfromImage(qt_image))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Process frame for object detection\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_detections(frame)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Update the alert state\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alert()\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mMainWindow.process_detections\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 tracked_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset elapsed time\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Draw bounding box and label\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(conf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, color, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# If no person or vehicle is detected, reset tracking\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'color' where it is not associated with a value"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PyQt5.QtWidgets import (QApplication, QMainWindow, QMenuBar, QAction, \n",
    "                             QWidget, QVBoxLayout, QHBoxLayout, QLabel, QFrame, \n",
    "                             QGridLayout, QListWidget)\n",
    "from PyQt5.QtGui import QImage, QPixmap, QPainter, QColor, QFont\n",
    "from PyQt5.QtCore import QTimer, QTime\n",
    "import time\n",
    "import torch\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "def get_db_connection():\n",
    "        try:\n",
    "            connection = mysql.connector.connect(\n",
    "                host=\"localhost\",\n",
    "                user=\"root\",\n",
    "                password=\"\",  # Your MySQL password here\n",
    "                database=\"viz_optilytics\"\n",
    "            )\n",
    "            return connection\n",
    "        except mysql.connector.Error as err:\n",
    "            print(f\"Error: {err}\")\n",
    "            return None\n",
    "\n",
    "######################################################################################\n",
    "# Load color reference CSV and create color lookup\n",
    "index = [\"color\", \"color_name\", \"hex\", \"R\", \"G\", \"B\"]\n",
    "csv = pd.read_csv('colors.csv', names=index, header=None)\n",
    "\n",
    "def get_color_name(bgr):\n",
    "    b, g, r = bgr\n",
    "    minimum = 10000\n",
    "    cname = \"Unknown\"\n",
    "    for i in range(len(csv)):\n",
    "        d = abs(r - int(csv.loc[i, \"R\"])) + abs(g - int(csv.loc[i, \"G\"])) + abs(b - int(csv.loc[i, \"B\"]))\n",
    "        if d <= minimum:\n",
    "            minimum = d\n",
    "            cname = csv.loc[i, \"color_name\"]\n",
    "    return cname\n",
    "\n",
    "# Function to store vehicle information in the database\n",
    "def store_vehicle_info(color, vehicle_type, model, timestamp):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    sql = \"INSERT INTO vehicles (color, type, model, timestamp) VALUES (%s, %s, %s, %s)\"\n",
    "    values = (color, vehicle_type, model, timestamp)\n",
    "\n",
    "    cursor.execute(sql, values)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Function to extract color from bounding box\n",
    "def extract_color(frame, bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    average_color = np.mean(roi, axis=(0, 1))\n",
    "    color_name = get_color_name(average_color)\n",
    "    return color_name\n",
    "###############################################        \n",
    "\n",
    "class MainWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Camera Feed and Surveillance System\")\n",
    "        self.setGeometry(100, 100, 1200, 800)  # Increased window size\n",
    "\n",
    "        # Create the menu bar\n",
    "        self.menu_bar = self.menuBar()\n",
    "        file_menu = self.menu_bar.addMenu(\"File\")\n",
    "        \n",
    "        exit_action = QAction(\"Exit\", self)\n",
    "        exit_action.triggered.connect(self.close)\n",
    "        file_menu.addAction(exit_action)\n",
    "\n",
    "        # Create a central widget and layout\n",
    "        central_widget = QWidget(self)\n",
    "        self.setCentralWidget(central_widget)\n",
    "        main_layout = QVBoxLayout(central_widget)\n",
    "\n",
    "        # Add a top section for the alert indicator\n",
    "        self.alert_section = QFrame()\n",
    "        self.alert_section.setFixedHeight(50)\n",
    "        self.alert_section.setStyleSheet(\"background-color: #f2f2f2; border-bottom: 2px solid #cccccc;\")\n",
    "        self.alert_label = QLabel(self.alert_section)\n",
    "        self.alert_label.setGeometry(10, 10, 40, 40)  # Adjusted circle position and size\n",
    "        self.alert_label.setStyleSheet(\"border-radius: 20px; background-color: green;\")  # Green initially\n",
    "        alert_title = QLabel(\"Alert Status\", self.alert_section)\n",
    "        alert_title.setFont(QFont(\"Arial\", 12, QFont.Bold))\n",
    "        alert_title.setGeometry(60, 10, 200, 40)\n",
    "        main_layout.addWidget(self.alert_section)\n",
    "\n",
    "        # Create a horizontal layout for left and right sections\n",
    "        content_layout = QHBoxLayout()\n",
    "\n",
    "        # Create a vertical section on the left\n",
    "        left_section = QFrame()\n",
    "        left_section.setStyleSheet(\"background-color: #e6e6e6; border-right: 2px solid #cccccc;\")\n",
    "        left_section.setFixedWidth(200)\n",
    "        left_layout = QVBoxLayout(left_section)\n",
    "\n",
    "        left_title = QLabel(\"Optimal videos\")\n",
    "        left_title.setFont(QFont(\"Arial\", 14, QFont.Bold))\n",
    "        left_layout.addWidget(left_title)\n",
    "#################################################################################\n",
    "        self.video_list = QListWidget()\n",
    "        left_layout.addWidget(self.video_list)\n",
    "        \n",
    "          # Load videos from database\n",
    "        self.load_videos()\n",
    "#################################################################################3\n",
    "        \n",
    "        left_layout.addWidget(QLabel(\"Item 1\"))\n",
    "        left_layout.addWidget(QLabel(\"Item 2\"))\n",
    "\n",
    "        content_layout.addWidget(left_section)\n",
    "\n",
    "        # Create a grid layout for the right section\n",
    "        right_section = QGridLayout()\n",
    "\n",
    "        # Section 1: Live Camera Feed\n",
    "        self.camera_section = QFrame()\n",
    "        self.camera_section.setStyleSheet(\"border: 1px solid #000000;\")\n",
    "        self.camera_label = QLabel(\"Camera Feed\")\n",
    "        self.camera_label.setFixedSize(500, 375)  # Larger feed size\n",
    "        self.camera_layout = QVBoxLayout(self.camera_section)\n",
    "        self.camera_layout.addWidget(self.camera_label)\n",
    "        right_section.addWidget(self.camera_section, 0, 0)\n",
    "\n",
    "        # Section 2: Object Detection\n",
    "        self.detection_section = QFrame()\n",
    "        self.detection_section.setStyleSheet(\"border: 1px solid #000000;\")\n",
    "        self.detection_label = QLabel(\"Detected Objects\")\n",
    "        self.detection_label.setFixedSize(500, 375)\n",
    "        self.detection_layout = QVBoxLayout(self.detection_section)\n",
    "        self.detection_layout.addWidget(self.detection_label)\n",
    "        right_section.addWidget(self.detection_section, 0, 1)\n",
    "\n",
    "        # Section 3: Graphs\n",
    "        self.graph_section_1 = QFrame()\n",
    "        self.graph_section_1.setStyleSheet(\"border: 1px solid #000000;\")\n",
    "        self.graph_layout_1 = QVBoxLayout(self.graph_section_1)\n",
    "        self.graph_layout_1.addWidget(QLabel(\"Graph 1\"))\n",
    "        right_section.addWidget(self.graph_section_1, 1, 0)\n",
    "\n",
    "        # Section 4: More Graphs\n",
    "        self.graph_section_2 = QFrame()\n",
    "        self.graph_section_2.setStyleSheet(\"border: 1px solid #000000;\")\n",
    "        self.graph_layout_2 = QVBoxLayout(self.graph_section_2)\n",
    "        self.graph_layout_2.addWidget(QLabel(\"Graph 2\"))\n",
    "        right_section.addWidget(self.graph_section_2, 1, 1)\n",
    "\n",
    "        content_layout.addLayout(right_section)\n",
    "        main_layout.addLayout(content_layout)\n",
    "\n",
    "        # Initialize YOLOv5, tracking data, and camera\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 model\n",
    "\n",
    "        self.tracked_objects = {}  # To store object ID and time of detection\n",
    "        self.alert_active = False  # Keep track of alert state\n",
    "        self.init_camera()\n",
    "        \n",
    "\n",
    "    def init_camera(self):\n",
    "        \"\"\"Initialize the camera feed.\"\"\"\n",
    "        self.cap = cv2.VideoCapture(0)  # Open the default camera\n",
    "        self.timer = QTimer(self)\n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.timer.start(20)  # Update the frame every 20 ms\n",
    "\n",
    "    def update_frame(self):\n",
    "        \"\"\"Update the camera feed frame.\"\"\"\n",
    "        ret, frame = self.cap.read()\n",
    "        if ret:\n",
    "            # Convert the frame to RGB format for display\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            h, w, ch = frame.shape\n",
    "            bytes_per_line = ch * w\n",
    "            qt_image = QImage(frame.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "            self.camera_label.setPixmap(QPixmap.fromImage(qt_image))\n",
    "\n",
    "            # Process frame for object detection\n",
    "            self.process_detections(frame)\n",
    "\n",
    "            # Update the alert state\n",
    "            self.update_alert()\n",
    "\n",
    "            \n",
    "\n",
    "    def load_videos(self):\n",
    "        connection = get_db_connection()\n",
    "        if not connection:\n",
    "            QMessageBox.critical(self, \"Database Error\", \"Could not connect to the database.\")\n",
    "            return\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT id, video_name FROM video_storage\")\n",
    "        videos = cursor.fetchall()\n",
    "\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "\n",
    "        for video in videos:\n",
    "            self.video_list.addItem(f\"{video[0]}: {video[1]}\")\n",
    "            \n",
    "      # Function to check if the vehicle is static\n",
    "    def is_static(self, current_position, last_position, threshold=10):\n",
    "        # Check if the bounding box coordinates have not changed significantly\n",
    "        return (abs(current_position[0] - last_position[0]) < threshold and\n",
    "                abs(current_position[1] - last_position[1]) < threshold and\n",
    "                abs(current_position[2] - last_position[2]) < threshold and\n",
    "                abs(current_position[3] - last_position[3]) < threshold)\n",
    "            \n",
    "    def process_detections(self, frame):\n",
    "        \"\"\"Detect objects in the frame using YOLOv5 and track presence of persons and vehicles.\"\"\"\n",
    "        results = self.model(frame)  # Perform YOLOv5 detection\n",
    "        detections = results.xyxy[0].numpy()  # Get bounding boxes and labels\n",
    "\n",
    "        current_time = time.time()\n",
    "        person_detected = False\n",
    "        vehicle_detected = False\n",
    "\n",
    "        for detection in detections:\n",
    "            x1, y1, x2, y2, conf, class_id = detection\n",
    "            class_id = int(class_id)\n",
    "            label = self.model.names[class_id]\n",
    "\n",
    "            # Logic for person detection\n",
    "            if label == 'person' and conf > 0.3:  # Adjust confidence threshold as needed\n",
    "                person_detected = True\n",
    "                object_id = \"person\"  # Simple ID for tracking person\n",
    "                color = (0, 255, 0)  # Green for first detection\n",
    "                # Track detection time\n",
    "                if object_id not in self.tracked_objects:\n",
    "                    self.tracked_objects[object_id] = current_time  # Store the time of first detection\n",
    "                else:\n",
    "                    elapsed_time = current_time - self.tracked_objects[object_id]\n",
    "\n",
    "                    # Generate alert if person has been detected for more than 5 seconds\n",
    "                    if elapsed_time > 5:\n",
    "                        color = (0, 0, 255)  # Red for alert\n",
    "                        self.alert_active = True\n",
    "                        # Add alert handling logic here\n",
    "                    else:\n",
    "                        color = (0, 255, 0)  # Green for normal detection\n",
    "                        self.alert_active = False\n",
    "\n",
    "                # Draw bounding box and label\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "                cv2.putText(frame, f\"{label} {int(conf * 100)}%\", (int(x1), int(y1) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "            elif label in ['car', 'truck', 'bus'] and conf > 0.3:  # Adjust confidence threshold as needed\n",
    "                vehicle_detected = True\n",
    "                object_id = label  # Use vehicle type as ID for tracking\n",
    "\n",
    "                # Track detection time\n",
    "                current_position = (int(x1), int(y1), int(x2), int(y2))  # Store current bounding box coordinates\n",
    "\n",
    "                if object_id not in self.tracked_objects:\n",
    "                    self.tracked_objects[object_id] = {\n",
    "                        'first_detection_time': current_time,\n",
    "                        'last_position': current_position,\n",
    "                        'elapsed_time': 0\n",
    "                    }\n",
    "                    color = (0, 255, 0)  # Green for first detection\n",
    "                else:\n",
    "                    tracked_data = self.tracked_objects[object_id]\n",
    "                    elapsed_time = current_time - tracked_data['first_detection_time']\n",
    "\n",
    "                    # Check if the position is the same (within a threshold)\n",
    "                    if self.is_static(current_position, tracked_data['last_position']):\n",
    "                        tracked_data['elapsed_time'] = elapsed_time\n",
    "\n",
    "                        # Generate alert if vehicle has been static for more than 5 seconds\n",
    "                        if tracked_data['elapsed_time'] > 5:\n",
    "                            color = (0, 0, 255)  # Red for alert\n",
    "                            self.alert_active = True\n",
    "                            # Add alert handling logic here\n",
    "                        else:\n",
    "                            color = (0, 255, 0)  # Green for normal detection\n",
    "                            self.alert_active = False\n",
    "                    else:\n",
    "                        # Reset the detection time if the vehicle has moved\n",
    "                        tracked_data['first_detection_time'] = current_time\n",
    "                        tracked_data['last_position'] = current_position\n",
    "                        tracked_data['elapsed_time'] = 0  # Reset elapsed time\n",
    "\n",
    "                # Draw bounding box and label\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "                cv2.putText(frame, f\"{label} {int(conf * 100)}%\", (int(x1), int(y1) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "\n",
    "        # If no person or vehicle is detected, reset tracking\n",
    "        if not person_detected:\n",
    "            self.tracked_objects.pop(\"person\", None)  # Remove person ID if not detected\n",
    "        if not vehicle_detected:\n",
    "            for vehicle_type in ['car', 'truck', 'bus']:\n",
    "                self.tracked_objects.pop(vehicle_type, None)  # Remove vehicle IDs if not detected\n",
    "\n",
    "        # Update the QPixmap in the detection label\n",
    "        qt_image = QImage(frame.data, frame.shape[1], frame.shape[0], QImage.Format_RGB888)\n",
    "        self.detection_label.setPixmap(QPixmap.fromImage(qt_image))\n",
    "\n",
    "    \n",
    "    def update_alert(self):\n",
    "        \"\"\"Update the alert indicator based on the presence of people/vehicles for too long.\"\"\"\n",
    "        if self.alert_active:\n",
    "            self.alert_label.setStyleSheet(\"border-radius: 20px; background-color: red;\")  # Red when alert is active\n",
    "        else:\n",
    "            self.alert_label.setStyleSheet(\"border-radius: 20px; background-color: green;\")  # Green when no alert\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        \"\"\"Handle the event when the window is closed.\"\"\"\n",
    "        self.timer.stop()  # Stop the timer\n",
    "        self.cap.release()  # Release the camera\n",
    "        cv2.destroyAllWindows()  # Close any OpenCV windows\n",
    "        event.accept()  # Accept the event and proceed with the closing\n",
    "\n",
    "def main():\n",
    "    app = QApplication(sys.argv)\n",
    "    \n",
    "    window = MainWindow()\n",
    "    window.show()\n",
    "    \n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d004c",
   "metadata": {},
   "source": [
    "### tHIS CODE IS WORKING FOR ONLY ONE PERSON DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import torch\n",
    "# import time\n",
    "\n",
    "# # Load YOLOv5 model (using pre-trained model)\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True, device='cpu')\n",
    "# model.classes = [0]  # Only detect people (class 0)\n",
    "\n",
    "# # Initialize the list of trackers using the legacy module\n",
    "# trackers = cv2.legacy.MultiTracker_create()\n",
    "\n",
    "# # Dictionary to store the time of entry for each person\n",
    "# person_times = {}\n",
    "# stay_threshold = 10  # seconds\n",
    "# grace_period = 2  # seconds\n",
    "\n",
    "# # Start capturing video\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# def create_tracker(frame, bbox):\n",
    "#     tracker = cv2.legacy.TrackerCSRT_create()  # Use CSRT tracker\n",
    "#     tracker.init(frame, tuple(bbox))\n",
    "#     return tracker\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "    \n",
    "#     frame = cv2.resize(frame, (640, 480))  # Resize frame to reduce memory load\n",
    "\n",
    "#     # Run YOLOv5 inference\n",
    "#     results = model(frame)\n",
    "    \n",
    "#     # Extract bounding boxes and confidences\n",
    "#     detections = []\n",
    "#     for det in results.xyxy[0]:  # For each detection\n",
    "#         x1, y1, x2, y2 = map(int, det[:4])\n",
    "#         bbox = [x1, y1, x2 - x1, y2 - y1]  # Convert to (x, y, w, h)\n",
    "#         detections.append(bbox)\n",
    "    \n",
    "#     # If no trackers are active, create new trackers for each detection\n",
    "#     if len(trackers.getObjects()) == 0:\n",
    "#         for bbox in detections:\n",
    "#             trackers.add(create_tracker(frame, bbox), frame, tuple(bbox))\n",
    "\n",
    "#     # Update the trackers for each person\n",
    "#     success, boxes = trackers.update(frame)\n",
    "\n",
    "#     current_time = time.time()\n",
    "\n",
    "#     # Loop over the tracked bounding boxes\n",
    "#     for i, box in enumerate(boxes):\n",
    "#         x, y, w, h = map(int, box)\n",
    "#         person_id = i + 1  # Use a simple counter for person ID\n",
    "\n",
    "#         # Draw bounding boxes\n",
    "#         cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "#         cv2.putText(frame, f'Person {person_id}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "#         # Track the time a person has stayed in the frame\n",
    "#         if person_id not in person_times:\n",
    "#             person_times[person_id] = {'start_time': current_time, 'grace_timer': None}\n",
    "\n",
    "#         person_data = person_times[person_id]\n",
    "#         time_in_frame = current_time - person_data['start_time']\n",
    "\n",
    "#         if time_in_frame > stay_threshold:\n",
    "#             print(f\"Alert! Person {person_id} stayed in frame for {time_in_frame:.2f} seconds.\")\n",
    "    \n",
    "#     # Display the output frame\n",
    "#     cv2.imshow('Person Detection and Tracking', frame)\n",
    "\n",
    "#     # Exit on pressing 'q'\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea7674",
   "metadata": {},
   "source": [
    "## tHIS CODE IS WORKING FOR people detection corectly but is not integrating IN THE MAJOR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaecbf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 0 detected at 2024-10-11 23:40:44\n",
      "Person 0 left the frame at 2024-10-11 23:40:45\n",
      "Person 0 detected at 2024-10-11 23:41:03\n",
      "Person 0 left the frame at 2024-10-11 23:41:04\n",
      "Person 0 detected at 2024-10-11 23:41:09\n",
      "Person 0 left the frame at 2024-10-11 23:41:10\n",
      "Person 0 detected at 2024-10-11 23:41:13\n",
      "Person 0 left the frame at 2024-10-11 23:41:15\n",
      "Person 0 detected at 2024-10-11 23:41:16\n",
      "Alert: Person 0 has been in the frame for 2.35 seconds. Head count: 1\n",
      "Person 0 left the frame at 2024-10-11 23:41:19\n",
      "Person 0 detected at 2024-10-11 23:41:19\n",
      "Alert: Person 0 has been in the frame for 2.22 seconds. Head count: 1\n",
      "Person 0 left the frame at 2024-10-11 23:41:22\n",
      "Person 0 detected at 2024-10-11 23:41:22\n",
      "Alert: Person 0 has been in the frame for 2.15 seconds. Head count: 1\n",
      "Person 0 left the frame at 2024-10-11 23:41:24\n",
      "Person 0 detected at 2024-10-11 23:41:25\n",
      "Person 0 left the frame at 2024-10-11 23:41:26\n",
      "Person 0 detected at 2024-10-11 23:41:27\n",
      "Alert: Person 0 has been in the frame for 2.57 seconds. Head count: 1\n",
      "Person 0 left the frame at 2024-10-11 23:41:29\n",
      "Person 0 detected at 2024-10-11 23:41:30\n",
      "Person 0 left the frame at 2024-10-11 23:41:32\n",
      "Person 0 detected at 2024-10-11 23:41:32\n",
      "Alert: Person 0 has been in the frame for 2.33 seconds. Head count: 1\n",
      "Person 0 left the frame at 2024-10-11 23:41:35\n",
      "Person 0 detected at 2024-10-11 23:41:36\n",
      "Person 0 left the frame at 2024-10-11 23:41:38\n",
      "Person 0 detected at 2024-10-11 23:41:38\n",
      "Person 0 left the frame at 2024-10-11 23:41:39\n",
      "Person 0 detected at 2024-10-11 23:41:39\n",
      "Person 0 left the frame at 2024-10-11 23:41:42\n",
      "Person 0 detected at 2024-10-11 23:41:43\n",
      "Person 0 left the frame at 2024-10-11 23:41:43\n",
      "Person 0 detected at 2024-10-11 23:41:44\n",
      "Person 0 left the frame at 2024-10-11 23:41:46\n",
      "Person 0 detected at 2024-10-11 23:41:47\n",
      "Person 0 left the frame at 2024-10-11 23:41:48\n",
      "Person 0 detected at 2024-10-11 23:41:50\n",
      "Person 0 left the frame at 2024-10-11 23:41:51\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import cv2\n",
    "# import time\n",
    "\n",
    "# # Load YOLOv5 model (pretrained on COCO)\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# # Set model to only detect the \"person\" class\n",
    "# model.classes = [0]  # 0 is the class index for \"person\" in COCO\n",
    "\n",
    "# # Load video or camera stream\n",
    "# cap = cv2.VideoCapture(0)  # Change 0 to video path if needed\n",
    "\n",
    "# # Variables for tracking people in the frame\n",
    "# person_enter_time = {}\n",
    "# person_in_frame = set()  # Set to keep track of persons currently in the frame\n",
    "# grace_period = 2  # Time a person needs to be detected to generate an alert (2 seconds)\n",
    "# alert_threshold = 120  # Time in seconds to trigger the alert for prolonged presence\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Perform inference\n",
    "#     results = model(frame)\n",
    "    \n",
    "#     # Extract the bounding boxes and check for person class (ID 0)\n",
    "#     detections = results.xyxy[0].cpu().numpy()  # Raw detections as numpy array\n",
    "#     persons_detected = []\n",
    "\n",
    "#     # Count detected people (heads)\n",
    "#     for det in detections:\n",
    "#         class_id = int(det[5])  # Class ID is the 6th element in detection\n",
    "#         if class_id == 0:  # 0 corresponds to 'person'\n",
    "#             persons_detected.append(det[:4])  # Append bounding box coordinates for persons\n",
    "    \n",
    "#     current_time = time.time()\n",
    "#     head_count = len(persons_detected)  # Count the number of detected people (heads)\n",
    "\n",
    "#     # Handle new detections\n",
    "#     for i, bbox in enumerate(persons_detected):\n",
    "#         if i not in person_in_frame:\n",
    "#             person_enter_time[i] = current_time  # Track when the person enters the frame\n",
    "#             person_in_frame.add(i)  # Mark person as in frame\n",
    "#             print(f\"Person {i} detected at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_time))}\")\n",
    "\n",
    "#         # Check if person has been in the frame for longer than alert threshold\n",
    "#         duration = current_time - person_enter_time[i]\n",
    "#         if duration > grace_period:\n",
    "#             print(f\"Alert: Person {i} has been in the frame for {duration:.2f} seconds. Head count: {head_count}\")\n",
    "#         if duration > alert_threshold:\n",
    "#             print(f\"Warning: Person {i} has exceeded {alert_threshold} seconds! Head count: {head_count}\")\n",
    "\n",
    "#     # Remove persons no longer detected (after the grace period)\n",
    "#     exited = set()\n",
    "#     for person_id in person_in_frame:\n",
    "#         if person_id >= len(persons_detected) or current_time - person_enter_time[person_id] > grace_period:\n",
    "#             exited.add(person_id)\n",
    "#             print(f\"Person {person_id} left the frame at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_time))}\")\n",
    "\n",
    "#     person_in_frame.difference_update(exited)\n",
    "\n",
    "#     # Render detections on the frame\n",
    "#     frame = results.render()[0]\n",
    "\n",
    "#     # Show frame with detections\n",
    "#     cv2.imshow('Person Detection', frame)\n",
    "\n",
    "#     # Press 'q' to exit\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release resources\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e9f61",
   "metadata": {},
   "source": [
    "## PEOPLE DETECTION ADD ALL THE DATA TO THE DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import cv2\n",
    "# import time\n",
    "# import mysql.connector\n",
    "# import requests\n",
    "# import webbrowser\n",
    "\n",
    "# # Load YOLOv5 model (pretrained on COCO)\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# # Set model to only detect the \"person\" class\n",
    "# model.classes = [0]\n",
    "\n",
    "# # Load video or camera stream\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Database connection\n",
    "# conn = mysql.connector.connect(\n",
    "#     host='localhost',\n",
    "#     user='root',\n",
    "#     password='',\n",
    "#     database='viz_optilytics'\n",
    "# )\n",
    "\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# def get_camera_location():\n",
    "#     api_url = 'https://api.opencagedata.com/geocode/v1/json'\n",
    "#     api_key = 'YOUR_API_KEY'  # Replace with your API key\n",
    "#     query = 'Your City, Country'  # Replace with a valid query\n",
    "#     response = requests.get(f'{api_url}?q={query}&key={api_key}')\n",
    "#     if response.status_code == 200:\n",
    "#         data = response.json()\n",
    "#         if data['results']:\n",
    "#             lat = data['results'][0]['geometry']['lat']\n",
    "#             lng = data['results'][0]['geometry']['lng']\n",
    "#             print(f\"Retrieved coordinates: Latitude = {lat}, Longitude = {lng}\")\n",
    "#             return lat, lng\n",
    "#     print(\"Failed to retrieve coordinates.\")\n",
    "#     return None, None\n",
    "\n",
    "# # Variables for tracking people in the frame\n",
    "# person_enter_time = {}\n",
    "# person_in_frame = set()  \n",
    "# alert_threshold = 120  \n",
    "# alert_triggered = set()  \n",
    "# grace_period = 5  # Time (seconds) after which a person will be considered as \"gone\"\n",
    "# buffer_time = 5   # Time (seconds) to keep track of a person who has left the frame\n",
    "\n",
    "# # Time of last detection for each person\n",
    "# buffer_exit_time = {}  \n",
    "\n",
    "# # Get camera location\n",
    "# latitude, longitude = get_camera_location()\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Perform inference\n",
    "#     results = model(frame)\n",
    "\n",
    "#     # Extract the bounding boxes and check for person class (ID 0)\n",
    "#     detections = results.xyxy[0].cpu().numpy()\n",
    "#     persons_detected = []\n",
    "\n",
    "#     # Count detected people (heads)\n",
    "#     for det in detections:\n",
    "#         class_id = int(det[5])  \n",
    "#         if class_id == 0:  \n",
    "#             persons_detected.append(det[:4])  \n",
    "\n",
    "#     current_time = time.time()\n",
    "#     head_count = len(persons_detected)\n",
    "\n",
    "#     # Handle new detections\n",
    "#     for i in range(head_count):\n",
    "#         if i not in person_in_frame:\n",
    "#             person_enter_time[i] = current_time\n",
    "#             person_in_frame.add(i)\n",
    "#             buffer_exit_time[i] = current_time  # Initialize exit time\n",
    "#             print(f\"Person {i} detected at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_time))}\")\n",
    "\n",
    "#         # Check if person has been in the frame for longer than grace period\n",
    "#         duration = current_time - person_enter_time[i]\n",
    "#         if duration > grace_period and i not in alert_triggered:\n",
    "#             print(f\"Alert: Person {i} has been in the frame for {duration:.2f} seconds. Head count: {head_count}\")\n",
    "#             alert_triggered.add(i)  \n",
    "\n",
    "#             # Insert data into the database\n",
    "#             cursor.execute('''INSERT INTO people (id, people_count, timestamp, location_lat, location_long)\n",
    "#                               VALUES (%s, %s, %s, %s, %s)''',\n",
    "#                            (i, head_count, time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_time)), latitude, longitude))\n",
    "#             conn.commit()\n",
    "\n",
    "#             if latitude is not None and longitude is not None:\n",
    "#                 map_url = f\"https://www.google.com/maps?q={latitude},{longitude}\"\n",
    "#                 webbrowser.open(map_url)\n",
    "#             else:\n",
    "#                 print(\"Cannot open map: Invalid location coordinates.\")\n",
    "\n",
    "#     # Remove persons no longer detected\n",
    "#     exited = set()\n",
    "#     for person_id in person_in_frame:\n",
    "#         if person_id >= head_count:\n",
    "#             # Check if the person has been out of the frame for longer than the grace period\n",
    "#             if current_time - buffer_exit_time[person_id] > grace_period:\n",
    "#                 exited.add(person_id)  \n",
    "#                 alert_triggered.discard(person_id)  \n",
    "#                 print(f\"Person {person_id} left the frame at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_time))}\")\n",
    "#         else:\n",
    "#             buffer_exit_time[person_id] = current_time  # Update last seen time\n",
    "\n",
    "#     person_in_frame.difference_update(exited)\n",
    "\n",
    "#     # Render detections on the frame\n",
    "#     frame = results.render()[0]\n",
    "\n",
    "#     # Show frame with detections\n",
    "#     cv2.imshow('Person Detection', frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release resources\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "# cursor.close()\n",
    "# conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18b4c3",
   "metadata": {},
   "source": [
    "## VEHICLE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7abbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import sys\n",
    "import cv2\n",
    "from PyQt5.QtCore import QTimer\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VizOptilyticsApp(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Viz Optilytics\")\n",
    "        self.setGeometry(0, 0, 1200, 700)\n",
    "\n",
    "        # Initialize QTimer (fixing the timer error)\n",
    "        self.timer = QTimer(self)  # Initialize the QTimer object\n",
    "\n",
    "        # Initialize video capture object\n",
    "        self.video_capture = None\n",
    "\n",
    "        # Detection states\n",
    "        self.detecting_person = False\n",
    "        self.detecting_vehicle = False\n",
    "\n",
    "         # Track detected persons and duration\n",
    "        self.person_detected_time = None  # To track when a person is first detected\n",
    "        self.alert_generated = False  # To track if alert is already generated\n",
    "        \n",
    "        # Load YOLOv5 model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 model\n",
    "\n",
    "         # Initialize alert labels for dynamic updates\n",
    "        self.alert_label = None\n",
    "        self.duration_label = None\n",
    "        \n",
    "        self.grace_period_duration = 5  # 5 seconds grace period\n",
    "\n",
    "        \n",
    "        self.vehicle_positions = {}  # Stores vehicle ID -> (last position, detection start time)\n",
    "        self.stationary_alert_generated = {}  # To track if stationary alert has been generated\n",
    "\n",
    "        \n",
    "        self.initUI()\n",
    "    def initUI(self):\n",
    "        # Main Layout\n",
    "        main_layout = QtWidgets.QVBoxLayout(self)\n",
    "        main_layout.setContentsMargins(50, 50, 50, 30)\n",
    "\n",
    "        # Top Section: Title and Menu\n",
    "        top_bar = QtWidgets.QHBoxLayout()\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        title = QtWidgets.QLabel(\"Viz Optilytics\")\n",
    "        title.setFont(QtGui.QFont(\"Arial\", 28, QtGui.QFont.Bold))\n",
    "        title.setStyleSheet(\"color: #FFD700;\")\n",
    "        title.setAlignment(QtCore.Qt.AlignCenter)\n",
    "\n",
    "        top_bar.addWidget(title)\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        menu_button = QtWidgets.QPushButton(\"☰\")\n",
    "        menu_button.setStyleSheet(\"background-color: #333333; color: #FFD700; font-size: 20px; border-radius: 10px;\")\n",
    "        menu_button.setFixedSize(50, 50)\n",
    "        menu_button.setMenu(self.create_menu())\n",
    "\n",
    "        top_bar.addWidget(menu_button)\n",
    "        main_layout.addLayout(top_bar)\n",
    "\n",
    "        title_spacer = QtWidgets.QSpacerItem(20, 40, QtWidgets.QSizePolicy.Minimum, QtWidgets.QSizePolicy.Fixed)\n",
    "        main_layout.addItem(title_spacer)\n",
    "\n",
    "        # Horizontal layout split: Videos, Buttons, Alerts, etc.\n",
    "        content_layout = QtWidgets.QHBoxLayout()\n",
    "        content_layout.setSpacing(20)\n",
    "\n",
    "        # Video Buttons and Info Section\n",
    "        main_vertical_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_list_layout = QtWidgets.QVBoxLayout()\n",
    "        recorded_videos_label = QtWidgets.QLabel(\"Recorded Videos\")\n",
    "        recorded_videos_label.setFont(QtGui.QFont(\"Arial\", 16, QtGui.QFont.Bold))\n",
    "        recorded_videos_label.setStyleSheet(\"color: white;\")\n",
    "        video_list_layout.addWidget(recorded_videos_label)\n",
    "\n",
    "        # Video buttons\n",
    "        for i in range(3):\n",
    "            video_btn = QtWidgets.QPushButton(f\"Video {i + 1}\")\n",
    "            video_btn.setStyleSheet(\"background-color: #555555; color: white; border-radius: 8px; padding: 8px;\")\n",
    "            video_btn.setFixedWidth(200)\n",
    "            video_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "            video_btn.clicked.connect(lambda checked, i=i: self.play_video(i + 1))  # Connect button to function\n",
    "            video_list_layout.addWidget(video_btn)\n",
    "\n",
    "        main_vertical_layout.addLayout(video_list_layout)\n",
    "\n",
    "        detection_info_widget = QtWidgets.QWidget()\n",
    "        detection_info_layout = QtWidgets.QVBoxLayout(detection_info_widget)\n",
    "        detection_info_layout.setSpacing(0)\n",
    "        detection_info_widget.setStyleSheet(\"background-color: #444444; border-radius: 10px;\")\n",
    "\n",
    "        date_time_label = QtWidgets.QLabel(\"Date: 2024-10-11\\nTime: 12:45 PM\")\n",
    "        date_time_label.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(date_time_label)\n",
    "\n",
    "        detection_status = QtWidgets.QLabel(\"Status: Vehicle Detected\")\n",
    "        detection_status.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(detection_status)\n",
    "        main_vertical_layout.addWidget(detection_info_widget)\n",
    "\n",
    "        # Real-time streaming button\n",
    "        real_time_btn = QtWidgets.QPushButton(\"Real-Time Streaming\")\n",
    "        real_time_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        real_time_btn.setFixedWidth(200)\n",
    "        real_time_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        real_time_btn.clicked.connect(self.start_real_time_stream)\n",
    "        main_vertical_layout.addWidget(real_time_btn)\n",
    "\n",
    "        # Analytics button\n",
    "        analytics_btn = QtWidgets.QPushButton(\"Analytics\")\n",
    "        analytics_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        analytics_btn.setFixedWidth(200)\n",
    "        analytics_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        analytics_btn.clicked.connect(self.show_analytics)\n",
    "        main_vertical_layout.addWidget(analytics_btn)\n",
    "\n",
    "        content_layout.addLayout(main_vertical_layout)\n",
    "\n",
    "        # Video display area and other buttons\n",
    "        main_left_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_layout = QtWidgets.QVBoxLayout()\n",
    "        video_controls_layout = QtWidgets.QHBoxLayout()\n",
    "\n",
    "        # Create a layout to position the close button at the top-right corner\n",
    "        close_button = QtWidgets.QPushButton(\"X\")\n",
    "        close_button.setStyleSheet(\"background-color: red; color: white; font-size: 18px; border-radius: 10px;\")\n",
    "        close_button.setFixedSize(30, 30)\n",
    "        close_button.clicked.connect(self.stop_stream)\n",
    "\n",
    "        # Create a wrapper for the video feed and the button\n",
    "        video_feed_wrapper = QtWidgets.QWidget()\n",
    "        video_feed_layout = QtWidgets.QVBoxLayout(video_feed_wrapper)\n",
    "        video_feed_layout.setContentsMargins(0, 0, 0, 0)  # Remove margins for precise alignment\n",
    "\n",
    "        # Create a layout for the close button at the top-right corner\n",
    "        close_button_layout = QtWidgets.QHBoxLayout()\n",
    "        close_button_layout.addStretch(1)  # Add stretch to push the button to the right\n",
    "        close_button_layout.addWidget(close_button)\n",
    "\n",
    "        video_feed_layout.addLayout(close_button_layout)  # Add the close button layout\n",
    "        self.video_feed = QtWidgets.QLabel(\"Video Stream\")\n",
    "        self.video_feed.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.video_feed.setStyleSheet(\"background-color: white; color: black; border: 2px solid;\")\n",
    "        self.video_feed.setFixedSize(970, 480)  # Set a fixed size for the video feed\n",
    "        self.video_feed.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)\n",
    "\n",
    "        video_feed_layout.addWidget(self.video_feed)  # Add the video feed below the button\n",
    "\n",
    "        # Add the video feed wrapper to the main video layout\n",
    "        video_layout.addWidget(video_feed_wrapper)\n",
    "\n",
    "        # Detection buttons\n",
    "        detection_buttons_layout = QtWidgets.QHBoxLayout()\n",
    "        record_btn = QtWidgets.QPushButton(\"Record\")\n",
    "        record_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        record_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        record_btn.clicked.connect(self.record_options)\n",
    "        detection_buttons_layout.addWidget(record_btn)\n",
    "\n",
    "       # Assigning person_detection_btn as an instance variable\n",
    "        self.person_detection_btn = QtWidgets.QPushButton(\"Person Detection\")\n",
    "        self.person_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.person_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.person_detection_btn.clicked.connect(self.toggle_person_detection)\n",
    "        detection_buttons_layout.addWidget(self.person_detection_btn)\n",
    "\n",
    "        self.vehicle_detection_btn = QtWidgets.QPushButton(\"Vehicle Detection\")\n",
    "        self.vehicle_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.vehicle_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.vehicle_detection_btn.clicked.connect(self.toggle_vehicle_detection)\n",
    "        detection_buttons_layout.addWidget(self.vehicle_detection_btn)\n",
    "\n",
    "        video_layout.addLayout(detection_buttons_layout)\n",
    "        main_left_layout.addLayout(video_layout)\n",
    "\n",
    "        # Alerts section\n",
    "        alerts_layout = QtWidgets.QHBoxLayout()\n",
    "        self.alert_label = QtWidgets.QLabel(\"Alert: No Detection\")\n",
    "        self.alert_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.alert_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.alert_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.alert_label)\n",
    "\n",
    "        self.duration_label = QtWidgets.QLabel(\"Time Duration: 00:00:00\")\n",
    "        self.duration_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.duration_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.duration_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.duration_label)\n",
    "\n",
    "        main_left_layout.addLayout(alerts_layout)\n",
    "        content_layout.addLayout(main_left_layout)\n",
    "\n",
    "        main_layout.addLayout(content_layout)\n",
    "        self.setStyleSheet(\"background-color: #222222; color: white;\")\n",
    "\n",
    "        self.setLayout(main_layout)\n",
    "\n",
    "    def create_menu(self):\n",
    "        menu = QtWidgets.QMenu(self)\n",
    "        menu.addAction(\"Option 1\", self.option1_action)\n",
    "        menu.addAction(\"Option 2\", self.option2_action)\n",
    "        return menu\n",
    "\n",
    "    def play_video(self, video_id):\n",
    "        print(f\"Playing Video {video_id}\")\n",
    "\n",
    "    def start_real_time_stream(self):\n",
    "        print(\"Starting real-time video stream\")\n",
    "        # Start video capture (assuming you have a webcam at index 0)\n",
    "        self.video_capture = cv2.VideoCapture(0)\n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.timer.start(20)\n",
    "\n",
    "    def stop_stream(self):\n",
    "        print(\"Stopping video stream\")\n",
    "        if self.video_capture is not None:\n",
    "            self.video_capture.release()\n",
    "            self.video_capture = None\n",
    "            self.video_feed.clear()  # Clear the video feed when stopping\n",
    "            self.timer.stop()  # Stop the timer\n",
    "\n",
    "            \n",
    "    def update_frame(self):\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if ret:\n",
    "            # Convert the frame from BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Check if person detection is active\n",
    "            if self.detecting_person:\n",
    "                frame = self.detect_person(frame)  # Call person detection if active\n",
    "            if self.detecting_vehicle:\n",
    "                frame = self.detect_vehicle(frame)\n",
    "                \n",
    "            # Get the frame dimensions and convert to QImage\n",
    "            h, w, ch = frame.shape\n",
    "            bytes_per_line = ch * w\n",
    "            qimg = QImage(frame.data, frame.shape[1], frame.shape[0], bytes_per_line, QImage.Format_RGB888)\n",
    "\n",
    "            # Display the QImage on the QLabel (video_feed)\n",
    "            self.video_feed.setPixmap(QPixmap.fromImage(qimg))\n",
    "\n",
    "\n",
    "    def toggle_person_detection(self):\n",
    "        self.detecting_person = not self.detecting_person  # Toggle the state\n",
    "        if self.detecting_person:\n",
    "            self.person_detection_btn.setText(\"Stop Person Detection\")\n",
    "        else:\n",
    "            self.person_detection_btn.setText(\"Start Person Detection\")\n",
    "            self.person_detected_time = None  # Reset timer when detection stops\n",
    "            self.alert_generated = False\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "    def detect_person(self, frame):\n",
    "        # Perform inference\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        person_detected = False\n",
    "        for *box, conf, cls in predictions:\n",
    "            if int(cls) == 0:  # Class index for person\n",
    "                person_detected = True\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "                cv2.putText(frame, f'Person {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        if person_detected:\n",
    "            self.update_person_detection_time()\n",
    "        else:\n",
    "            self.reset_person_detection()\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def update_person_detection_time(self):\n",
    "        \"\"\"Track how long a person has been in the frame and update alert if needed.\"\"\"\n",
    "        if self.person_detected_time is None:\n",
    "            self.person_detected_time = time.time()  # Start the timer\n",
    "\n",
    "        duration = time.time() - self.person_detected_time\n",
    "\n",
    "        # Format the duration\n",
    "        hours, rem = divmod(duration, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        duration_str = \"{:02}:{:02}:{:02}\".format(int(hours), int(minutes), int(seconds))\n",
    "\n",
    "        # Update duration label\n",
    "        self.duration_label.setText(f\"Time Duration: {duration_str}\")\n",
    "\n",
    "        # Check if detection duration exceeds the threshold (e.g., 5 minutes or 300 seconds)\n",
    "        if duration > 10 and not self.alert_generated:  # 300 seconds = 5 minutes\n",
    "            self.alert_label.setText(\"ALERT: Person detected for over 5 minutes!\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.alert_generated = True\n",
    "\n",
    "    def reset_person_detection(self):\n",
    "        \"\"\"Reset the detection if no person is found.\"\"\"\n",
    "        if self.person_detected_time is not None:\n",
    "            self.person_detected_time = None  # Reset the timer\n",
    "            self.duration_label.setText(\"Time Duration: 00:00:00\")\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.alert_generated = False\n",
    "\n",
    "            \n",
    "#     def detect_vehicle(self, frame):\n",
    "#     # Perform inference\n",
    "#         results = self.model(frame)  # Use the loaded YOLOv5 model\n",
    "\n",
    "#         # Get predictions\n",
    "#         predictions = results.pred[0]  # Get the first result (frame-wise)\n",
    "\n",
    "#         # Draw boxes for detected persons\n",
    "#         for *box, conf, cls in predictions:\n",
    "#             if int(cls) in [2, 3, 5, 7]: # Class index for   2: Car  3: Motorcycle  5: Bus   7: Truck\n",
    "#                 x1, y1, x2, y2 = map(int, box)\n",
    "#                 cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "#                 vehicle_type = [\"Car\", \"Motorcycle\", \"Bus\", \"Truck\"][int(cls) - 2]  # Assign the appropriate vehicle label\n",
    "#                 cv2.putText(frame, f'{vehicle_type} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "#         return frame  # Return the modified frame with detections\n",
    "\n",
    "\n",
    "    def detect_vehicle(self, frame):\n",
    "        # Perform inference using YOLOv5 model\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        current_time = time.time()  # Get the current time\n",
    "\n",
    "        for *box, conf, cls in predictions:\n",
    "            if int(cls) in [2, 3, 5, 7]:  # Vehicle class: Car (2), Motorcycle (3), Bus (5), Truck (7)\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                vehicle_type = [\"Car\", \"Motorcycle\", \"Bus\", \"Truck\"][int(cls) - 2]\n",
    "\n",
    "                # Calculate the center of the bounding box\n",
    "                vehicle_center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "                # Use the center position to track vehicle movement\n",
    "                if vehicle_type not in self.vehicle_positions:\n",
    "                    # If this is a new vehicle, start tracking it\n",
    "                    self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                    self.stationary_alert_generated[vehicle_type] = False\n",
    "                else:\n",
    "                    # Get the previous position and detection start time\n",
    "                    prev_position, start_time = self.vehicle_positions[vehicle_type]\n",
    "\n",
    "                    # Calculate movement distance between current and previous position\n",
    "                    movement_distance = np.linalg.norm(np.array(vehicle_center) - np.array(prev_position))\n",
    "\n",
    "                    # Define a movement threshold (e.g., 20 pixels)\n",
    "                    movement_threshold = 20\n",
    "\n",
    "                    if movement_distance < movement_threshold:\n",
    "                        # If the vehicle has not moved significantly, check how long it's been stationary\n",
    "                        stationary_duration = current_time - start_time\n",
    "\n",
    "                        # Update the time duration label\n",
    "                        self.duration_label.setText(f\"Vehicle {vehicle_type} stationary for {int(stationary_duration)} seconds\")\n",
    "\n",
    "                        # Check if the stationary duration exceeds 5 minutes (300 seconds)\n",
    "                        if stationary_duration > 10 and not self.stationary_alert_generated[vehicle_type]:\n",
    "                            self.alert_label.setText(f\"ALERT: {vehicle_type} stationary for over 5 minutes!\")\n",
    "                            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.stationary_alert_generated[vehicle_type] = True  # Mark that the alert has been generated\n",
    "                    else:\n",
    "                        # If the vehicle moved, reset the detection start time\n",
    "                        self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                        self.stationary_alert_generated[vehicle_type] = False\n",
    "\n",
    "                # Draw the bounding box and label the vehicle\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'{vehicle_type} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    \n",
    "    def toggle_vehicle_detection(self):\n",
    "        \"\"\"Toggle vehicle detection on and off.\"\"\"\n",
    "        self.detecting_vehicle = not self.detecting_vehicle  # Toggle the state\n",
    "        if self.detecting_vehicle:\n",
    "            self.vehicle_detection_btn.setText(\"Stop vehicle Detection\")\n",
    "        else:\n",
    "            self.vehicle_detection_btn.setText(\"Start vehicle Detection\")\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "    def show_analytics(self):\n",
    "        print(\"Showing analytics\")\n",
    "\n",
    "    def record_options(self):\n",
    "        print(\"Recording options selected\")\n",
    "\n",
    "    def option1_action(self):\n",
    "        print(\"Option 1 selected\")\n",
    "\n",
    "    def option2_action(self):\n",
    "        print(\"Option 2 selected\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    window = VizOptilyticsApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28a524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing Video 1\n",
      "Stopping video stream\n",
      "Stopping video stream\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import sys\n",
    "import cv2\n",
    "from PyQt5.QtCore import QTimer\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VizOptilyticsApp(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Viz Optilytics\")\n",
    "        self.setGeometry(0, 0, 1200, 700)\n",
    "\n",
    "        # Initialize QTimer (fixing the timer error)\n",
    "        self.timer = QTimer(self)  # Initialize the QTimer object\n",
    "\n",
    "        # Initialize video capture object\n",
    "        self.video_capture = None\n",
    "\n",
    "        # Detection states\n",
    "        self.detecting_person = False\n",
    "        self.detecting_vehicle = False\n",
    "\n",
    "         # Track detected persons and duration\n",
    "        self.person_detected_time = None  # To track when a person is first detected\n",
    "        self.alert_generated = False  # To track if alert is already generated\n",
    "        \n",
    "        # Load YOLOv5 model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 model\n",
    "\n",
    "         # Initialize alert labels for dynamic updates\n",
    "        self.alert_label = None\n",
    "        self.duration_label = None\n",
    "        \n",
    "        self.detection_grace_period = 10  # Number of frames to maintain detection box\n",
    "        self.frames_since_last_detection = 0\n",
    "        self.last_detection_box = None  # Store the last detected bounding box\n",
    "\n",
    "        \n",
    "        self.vehicle_positions = {}  # Stores vehicle ID -> (last position, detection start time)\n",
    "        self.stationary_alert_generated = {}  # To track if stationary alert has been generated\n",
    "\n",
    "        self.video1 = False\n",
    "        self.video2 = False\n",
    "        self.video3 = False\n",
    "        self.real_time_stream = False\n",
    "\n",
    "        self.initUI()\n",
    "    def initUI(self):\n",
    "        # Main Layout\n",
    "        main_layout = QtWidgets.QVBoxLayout(self)\n",
    "        main_layout.setContentsMargins(50, 50, 50, 30)\n",
    "\n",
    "        # Top Section: Title and Menu\n",
    "        top_bar = QtWidgets.QHBoxLayout()\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        title = QtWidgets.QLabel(\"Viz Optilytics\")\n",
    "        title.setFont(QtGui.QFont(\"Arial\", 28, QtGui.QFont.Bold))\n",
    "        title.setStyleSheet(\"color: #FFD700;\")\n",
    "        title.setAlignment(QtCore.Qt.AlignCenter)\n",
    "\n",
    "        top_bar.addWidget(title)\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        menu_button = QtWidgets.QPushButton(\"☰\")\n",
    "        menu_button.setStyleSheet(\"background-color: #333333; color: #FFD700; font-size: 20px; border-radius: 10px;\")\n",
    "        menu_button.setFixedSize(50, 50)\n",
    "        menu_button.setMenu(self.create_menu())\n",
    "\n",
    "        top_bar.addWidget(menu_button)\n",
    "        main_layout.addLayout(top_bar)\n",
    "\n",
    "        title_spacer = QtWidgets.QSpacerItem(20, 40, QtWidgets.QSizePolicy.Minimum, QtWidgets.QSizePolicy.Fixed)\n",
    "        main_layout.addItem(title_spacer)\n",
    "\n",
    "        # Horizontal layout split: Videos, Buttons, Alerts, etc.\n",
    "        content_layout = QtWidgets.QHBoxLayout()\n",
    "        content_layout.setSpacing(20)\n",
    "\n",
    "        # Video Buttons and Info Section\n",
    "        main_vertical_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_list_layout = QtWidgets.QVBoxLayout()\n",
    "        recorded_videos_label = QtWidgets.QLabel(\"Recorded Videos\")\n",
    "        recorded_videos_label.setFont(QtGui.QFont(\"Arial\", 16, QtGui.QFont.Bold))\n",
    "        recorded_videos_label.setStyleSheet(\"color: white;\")\n",
    "        video_list_layout.addWidget(recorded_videos_label)\n",
    "\n",
    "        # Video buttons\n",
    "        for i in range(3):\n",
    "            video_btn = QtWidgets.QPushButton(f\"Video {i + 1}\")\n",
    "            video_btn.setStyleSheet(\"background-color: #555555; color: white; border-radius: 8px; padding: 8px;\")\n",
    "            video_btn.setFixedWidth(200)\n",
    "            video_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "\n",
    "            # Connect each button to a separate function based on the index\n",
    "            if i == 0:\n",
    "                video_btn.clicked.connect(self.play_video_1)\n",
    "            elif i == 1:\n",
    "                video_btn.clicked.connect(self.play_video_2)\n",
    "            elif i == 2:\n",
    "                video_btn.clicked.connect(self.play_video_3)\n",
    "\n",
    "            video_list_layout.addWidget(video_btn)\n",
    "\n",
    "        main_vertical_layout.addLayout(video_list_layout)\n",
    "\n",
    "        detection_info_widget = QtWidgets.QWidget()\n",
    "        detection_info_layout = QtWidgets.QVBoxLayout(detection_info_widget)\n",
    "        detection_info_layout.setSpacing(0)\n",
    "        detection_info_widget.setStyleSheet(\"background-color: #444444; border-radius: 10px;\")\n",
    "\n",
    "        date_time_label = QtWidgets.QLabel(\"Date: 2024-10-11\\nTime: 12:45 PM\")\n",
    "        date_time_label.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(date_time_label)\n",
    "\n",
    "        detection_status = QtWidgets.QLabel(\"Status: Vehicle Detected\")\n",
    "        detection_status.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(detection_status)\n",
    "        main_vertical_layout.addWidget(detection_info_widget)\n",
    "\n",
    "        # Real-time streaming button\n",
    "        real_time_btn = QtWidgets.QPushButton(\"Real-Time Streaming\")\n",
    "        real_time_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        real_time_btn.setFixedWidth(200)\n",
    "        real_time_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        real_time_btn.clicked.connect(self.start_real_time_stream)\n",
    "        main_vertical_layout.addWidget(real_time_btn)\n",
    "\n",
    "        # Analytics button\n",
    "        analytics_btn = QtWidgets.QPushButton(\"Analytics\")\n",
    "        analytics_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        analytics_btn.setFixedWidth(200)\n",
    "        analytics_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        analytics_btn.clicked.connect(self.show_analytics)\n",
    "        main_vertical_layout.addWidget(analytics_btn)\n",
    "\n",
    "        content_layout.addLayout(main_vertical_layout)\n",
    "\n",
    "        # Video display area and other buttons\n",
    "        main_left_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_layout = QtWidgets.QVBoxLayout()\n",
    "        video_controls_layout = QtWidgets.QHBoxLayout()\n",
    "\n",
    "        # Create a layout to position the close button at the top-right corner\n",
    "        close_button = QtWidgets.QPushButton(\"X\")\n",
    "        close_button.setStyleSheet(\"background-color: red; color: white; font-size: 18px; border-radius: 10px;\")\n",
    "        close_button.setFixedSize(30, 30)\n",
    "        close_button.clicked.connect(self.stop_stream)\n",
    "\n",
    "        # Create a wrapper for the video feed and the button\n",
    "        video_feed_wrapper = QtWidgets.QWidget()\n",
    "        video_feed_layout = QtWidgets.QVBoxLayout(video_feed_wrapper)\n",
    "        video_feed_layout.setContentsMargins(0, 0, 0, 0)  # Remove margins for precise alignment\n",
    "\n",
    "        # Create a layout for the close button at the top-right corner\n",
    "        close_button_layout = QtWidgets.QHBoxLayout()\n",
    "        close_button_layout.addStretch(1)  # Add stretch to push the button to the right\n",
    "        close_button_layout.addWidget(close_button)\n",
    "\n",
    "        video_feed_layout.addLayout(close_button_layout)  # Add the close button layout\n",
    "        self.video_feed = QtWidgets.QLabel(\"Video Stream\")\n",
    "        self.video_feed.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.video_feed.setStyleSheet(\"background-color: white; color: black; border: 2px solid;\")\n",
    "        self.video_feed.setFixedSize(970, 480)  # Set a fixed size for the video feed\n",
    "        self.video_feed.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)\n",
    "\n",
    "        video_feed_layout.addWidget(self.video_feed)  # Add the video feed below the button\n",
    "\n",
    "        # Add the video feed wrapper to the main video layout\n",
    "        video_layout.addWidget(video_feed_wrapper)\n",
    "\n",
    "        # Detection buttons\n",
    "        detection_buttons_layout = QtWidgets.QHBoxLayout()\n",
    "        record_btn = QtWidgets.QPushButton(\"Record\")\n",
    "        record_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        record_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        record_btn.clicked.connect(self.record_options)\n",
    "        detection_buttons_layout.addWidget(record_btn)\n",
    "\n",
    "       # Assigning person_detection_btn as an instance variable\n",
    "        self.person_detection_btn = QtWidgets.QPushButton(\"Person Detection\")\n",
    "        self.person_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.person_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.person_detection_btn.clicked.connect(self.toggle_person_detection)\n",
    "        detection_buttons_layout.addWidget(self.person_detection_btn)\n",
    "\n",
    "        self.vehicle_detection_btn = QtWidgets.QPushButton(\"Vehicle Detection\")\n",
    "        self.vehicle_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.vehicle_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.vehicle_detection_btn.clicked.connect(self.toggle_vehicle_detection)\n",
    "        detection_buttons_layout.addWidget(self.vehicle_detection_btn)\n",
    "\n",
    "        video_layout.addLayout(detection_buttons_layout)\n",
    "        main_left_layout.addLayout(video_layout)\n",
    "\n",
    "        # Alerts section\n",
    "        alerts_layout = QtWidgets.QHBoxLayout()\n",
    "        self.alert_label = QtWidgets.QLabel(\"Alert: No Detection\")\n",
    "        self.alert_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.alert_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.alert_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.alert_label)\n",
    "\n",
    "        self.duration_label = QtWidgets.QLabel(\"Time Duration: 00:00:00\")\n",
    "        self.duration_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.duration_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.duration_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.duration_label)\n",
    "\n",
    "        main_left_layout.addLayout(alerts_layout)\n",
    "        content_layout.addLayout(main_left_layout)\n",
    "\n",
    "        main_layout.addLayout(content_layout)\n",
    "        self.setStyleSheet(\"background-color: #222222; color: white;\")\n",
    "\n",
    "        self.setLayout(main_layout)\n",
    "\n",
    "    def create_menu(self):\n",
    "        menu = QtWidgets.QMenu(self)\n",
    "        menu.addAction(\"Option 1\", self.option1_action)\n",
    "        menu.addAction(\"Option 2\", self.option2_action)\n",
    "        return menu\n",
    "\n",
    "    def start_real_time_stream(self):\n",
    "        print(\"Starting real-time video stream\")\n",
    "        self.real_time_stream=True\n",
    "        # Start video capture (assuming you have a webcam at index 0)\n",
    "        self.video_display()  # Call video_feed to initiate video capture\n",
    "        \n",
    "    def video_display(self):\n",
    "        if self.real_time_stream:\n",
    "            self.video_capture = cv2.VideoCapture(0)\n",
    "        elif self.video1:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\853889-hd_1920_1080_25fps.mp4\")\n",
    "        elif self.video2:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\2273134-hd_1280_720_30fps.mp4\")\n",
    "        elif self.video3:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\12494476_3840_2160_50fps.mp4\")\n",
    "            \n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.timer.start(20)\n",
    "\n",
    "    def stop_stream(self):\n",
    "        print(\"Stopping video stream\")\n",
    "        if self.video_capture is not None:\n",
    "            self.video_capture.release()\n",
    "            self.video_capture = None\n",
    "            self.video_feed.clear()  # Clear the video feed when stopping\n",
    "            self.timer.stop()  # Stop the timer\n",
    "\n",
    "            \n",
    "    def update_frame(self):\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if ret:\n",
    "            # Convert the frame from BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Check if person detection is active\n",
    "            if self.detecting_person:\n",
    "                frame = self.detect_person(frame)  # Call person detection if active\n",
    "            if self.detecting_vehicle:\n",
    "                frame = self.detect_vehicle(frame)\n",
    "                \n",
    "            # Get the frame dimensions and convert to QImage\n",
    "            h, w, ch = frame.shape\n",
    "            bytes_per_line = ch * w\n",
    "            qimg = QImage(frame.data, frame.shape[1], frame.shape[0], bytes_per_line, QImage.Format_RGB888)\n",
    "\n",
    "            # Display the QImage on the QLabel (video_feed)\n",
    "            self.video_feed.setPixmap(QPixmap.fromImage(qimg))\n",
    "\n",
    "\n",
    "    def toggle_person_detection(self):\n",
    "        self.detecting_person = not self.detecting_person  # Toggle the state\n",
    "        if self.detecting_person:\n",
    "            self.person_detection_btn.setText(\"Stop Person Detection\")\n",
    "        else:\n",
    "            self.person_detection_btn.setText(\"Start Person Detection\")\n",
    "            self.person_detected_time = None  # Reset timer when detection stops\n",
    "            self.alert_generated = False\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "    def detect_person(self, frame):\n",
    "        # Perform inference\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        person_detected = False\n",
    "        for *box, conf, cls in predictions:\n",
    "            if int(cls) == 0:  # Class index for person\n",
    "                person_detected = True\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "                cv2.putText(frame, f'Person {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        if person_detected:\n",
    "            self.update_person_detection_time()\n",
    "        else:\n",
    "            self.reset_person_detection()\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def update_person_detection_time(self):\n",
    "        \"\"\"Track how long a person has been in the frame and update alert if needed.\"\"\"\n",
    "        if self.person_detected_time is None:\n",
    "            self.person_detected_time = time.time()  # Start the timer\n",
    "\n",
    "        duration = time.time() - self.person_detected_time\n",
    "\n",
    "        # Format the duration\n",
    "        hours, rem = divmod(duration, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        duration_str = \"{:02}:{:02}:{:02}\".format(int(hours), int(minutes), int(seconds))\n",
    "\n",
    "        # Update duration label\n",
    "        self.duration_label.setText(f\"Time Duration: {duration_str}\")\n",
    "\n",
    "        # Check if detection duration exceeds the threshold (e.g., 5 minutes or 300 seconds)\n",
    "        if duration > 10 and not self.alert_generated:  # 300 seconds = 5 minutes\n",
    "            self.alert_label.setText(\"ALERT: Person detected for over 5 minutes!\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.alert_generated = True\n",
    "\n",
    "    def reset_person_detection(self):\n",
    "        \"\"\"Reset the detection if no person is found.\"\"\"\n",
    "        if self.person_detected_time is not None:\n",
    "            self.person_detected_time = None  # Reset the timer\n",
    "            self.duration_label.setText(\"Time Duration: 00:00:00\")\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.alert_generated = False\n",
    "\n",
    "            \n",
    "    def detect_vehicle(self, frame):\n",
    "        # Perform inference using YOLOv5 model\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        current_time = time.time()  # Get the current time\n",
    "\n",
    "        vehicle_detected = False  # Flag to check if any vehicle is detected\n",
    "\n",
    "        for *box, conf, cls in predictions:\n",
    "            if int(cls) in [2, 3, 5, 7]:  # Vehicle class: Car (2), Motorcycle (3), Bus (5), Truck (7)\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                vehicle_type = [\"Car\", \"Motorcycle\", \"Bus\", \"Truck\"][int(cls) - 2]\n",
    "\n",
    "                # Calculate the center of the bounding box\n",
    "                vehicle_center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "                vehicle_detected = True  # Set the flag to True since we detected a vehicle\n",
    "\n",
    "                # Use the center position to track vehicle movement\n",
    "                if vehicle_type not in self.vehicle_positions:\n",
    "                    # If this is a new vehicle, start tracking it\n",
    "                    self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                    self.stationary_alert_generated[vehicle_type] = False\n",
    "                else:\n",
    "                    # Get the previous position and detection start time\n",
    "                    prev_position, start_time = self.vehicle_positions[vehicle_type]\n",
    "\n",
    "                    # Calculate movement distance between current and previous position\n",
    "                    movement_distance = np.linalg.norm(np.array(vehicle_center) - np.array(prev_position))\n",
    "\n",
    "                    # Define a movement threshold (e.g., 20 pixels)\n",
    "                    movement_threshold = 20\n",
    "\n",
    "                    if movement_distance < movement_threshold:\n",
    "                        # If the vehicle has not moved significantly, check how long it's been stationary\n",
    "                        stationary_duration = current_time - start_time\n",
    "\n",
    "                        # Update the time duration label\n",
    "                        self.duration_label.setText(f\"Vehicle {vehicle_type} stationary for {int(stationary_duration)} seconds\")\n",
    "\n",
    "                        # Check if the stationary duration exceeds 5 minutes (300 seconds)\n",
    "                        if stationary_duration > 5 and not self.stationary_alert_generated[vehicle_type]:\n",
    "                            self.alert_label.setText(f\"ALERT: {vehicle_type} stationary for over 5 minutes!\")\n",
    "                            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.stationary_alert_generated[vehicle_type] = True  # Mark that the alert has been generated\n",
    "                    else:\n",
    "                        # If the vehicle moved, reset the detection start time\n",
    "                        self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                        self.stationary_alert_generated[vehicle_type] = False\n",
    "\n",
    "                # Draw the bounding box and label the vehicle\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'{vehicle_type} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # If no vehicle is detected, reset the alert and duration labels\n",
    "        if not vehicle_detected:\n",
    "            self.alert_label.setText(\"No vehicle detected\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setText(\"No detection duration: 00 seconds\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def toggle_vehicle_detection(self):\n",
    "        \"\"\"Toggle vehicle detection on and off.\"\"\"\n",
    "        self.detecting_vehicle = not self.detecting_vehicle  # Toggle the state\n",
    "        if self.detecting_vehicle:\n",
    "            self.vehicle_detection_btn.setText(\"Stop vehicle Detection\")\n",
    "        else:\n",
    "            self.vehicle_detection_btn.setText(\"Start vehicle Detection\")\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "   # Define the three separate play video functions\n",
    "    def play_video_1(self):\n",
    "        print(\"Playing Video 1\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = True  # Set instance variable\n",
    "        self.video2 = False  # Reset other video flags\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_2(self):\n",
    "        print(\"Playing Video 2\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = True\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_3(self):\n",
    "        print(\"Playing Video 3\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = False\n",
    "        self.video3 = True\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    # Add your logic to play video 3 here\n",
    "    def show_analytics(self):\n",
    "        print(\"Showing analytics\")\n",
    "\n",
    "    def record_options(self):\n",
    "        print(\"Recording options selected\")\n",
    "\n",
    "    def option1_action(self):\n",
    "        print(\"Option 1 selected\")\n",
    "\n",
    "    def option2_action(self):\n",
    "        print(\"Option 2 selected\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    window = VizOptilyticsApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import sys\n",
    "import cv2\n",
    "from PyQt5.QtCore import QTimer\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VizOptilyticsApp(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Viz Optilytics\")\n",
    "        self.setGeometry(0, 0, 1200, 700)\n",
    "\n",
    "        # Initialize QTimer (fixing the timer error)\n",
    "        self.timer = QTimer(self)  # Initialize the QTimer object\n",
    "\n",
    "        # Initialize video capture object\n",
    "        self.video_capture = None\n",
    "\n",
    "        # Detection states\n",
    "        self.detecting_person = False\n",
    "        self.detecting_vehicle = False\n",
    "\n",
    "         # Track detected persons and duration\n",
    "        self.person_detected_time = None  # To track when a person is first detected\n",
    "        self.alert_generated = False  # To track if alert is already generated\n",
    "        \n",
    "        # Load YOLOv5 model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 model\n",
    "\n",
    "         # Initialize alert labels for dynamic updates\n",
    "        self.alert_label = None\n",
    "        self.duration_label = None\n",
    "        \n",
    "        self.detection_grace_period = 10  # Number of frames to maintain detection box\n",
    "        self.frames_since_last_detection = 0\n",
    "        self.last_detection_box = None  # Store the last detected bounding box\n",
    "\n",
    "        \n",
    "        self.vehicle_positions = {}  # Stores vehicle ID -> (last position, detection start time)\n",
    "        self.stationary_alert_generated = {}  # To track if stationary alert has been generated\n",
    "\n",
    "        self.video1 = False\n",
    "        self.video2 = False\n",
    "        self.video3 = False\n",
    "        self.real_time_stream = False\n",
    "\n",
    "        self.initUI()\n",
    "    def initUI(self):\n",
    "        # Main Layout\n",
    "        main_layout = QtWidgets.QVBoxLayout(self)\n",
    "        main_layout.setContentsMargins(50, 50, 50, 30)\n",
    "\n",
    "        # Top Section: Title and Menu\n",
    "        top_bar = QtWidgets.QHBoxLayout()\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        title = QtWidgets.QLabel(\"Viz Optilytics\")\n",
    "        title.setFont(QtGui.QFont(\"Arial\", 28, QtGui.QFont.Bold))\n",
    "        title.setStyleSheet(\"color: #FFD700;\")\n",
    "        title.setAlignment(QtCore.Qt.AlignCenter)\n",
    "\n",
    "        top_bar.addWidget(title)\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        menu_button = QtWidgets.QPushButton(\"☰\")\n",
    "        menu_button.setStyleSheet(\"background-color: #333333; color: #FFD700; font-size: 20px; border-radius: 10px;\")\n",
    "        menu_button.setFixedSize(50, 50)\n",
    "        menu_button.setMenu(self.create_menu())\n",
    "\n",
    "        top_bar.addWidget(menu_button)\n",
    "        main_layout.addLayout(top_bar)\n",
    "\n",
    "        title_spacer = QtWidgets.QSpacerItem(20, 40, QtWidgets.QSizePolicy.Minimum, QtWidgets.QSizePolicy.Fixed)\n",
    "        main_layout.addItem(title_spacer)\n",
    "\n",
    "        # Horizontal layout split: Videos, Buttons, Alerts, etc.\n",
    "        content_layout = QtWidgets.QHBoxLayout()\n",
    "        content_layout.setSpacing(20)\n",
    "\n",
    "        # Video Buttons and Info Section\n",
    "        main_vertical_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_list_layout = QtWidgets.QVBoxLayout()\n",
    "        recorded_videos_label = QtWidgets.QLabel(\"Recorded Videos\")\n",
    "        recorded_videos_label.setFont(QtGui.QFont(\"Arial\", 16, QtGui.QFont.Bold))\n",
    "        recorded_videos_label.setStyleSheet(\"color: white;\")\n",
    "        video_list_layout.addWidget(recorded_videos_label)\n",
    "\n",
    "        # Video buttons\n",
    "        for i in range(3):\n",
    "            video_btn = QtWidgets.QPushButton(f\"Video {i + 1}\")\n",
    "            video_btn.setStyleSheet(\"background-color: #555555; color: white; border-radius: 8px; padding: 8px;\")\n",
    "            video_btn.setFixedWidth(200)\n",
    "            video_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "\n",
    "            # Connect each button to a separate function based on the index\n",
    "            if i == 0:\n",
    "                video_btn.clicked.connect(self.play_video_1)\n",
    "            elif i == 1:\n",
    "                video_btn.clicked.connect(self.play_video_2)\n",
    "            elif i == 2:\n",
    "                video_btn.clicked.connect(self.play_video_3)\n",
    "\n",
    "            video_list_layout.addWidget(video_btn)\n",
    "\n",
    "        main_vertical_layout.addLayout(video_list_layout)\n",
    "\n",
    "        detection_info_widget = QtWidgets.QWidget()\n",
    "        detection_info_layout = QtWidgets.QVBoxLayout(detection_info_widget)\n",
    "        detection_info_layout.setSpacing(0)\n",
    "        detection_info_widget.setStyleSheet(\"background-color: #444444; border-radius: 10px;\")\n",
    "\n",
    "        date_time_label = QtWidgets.QLabel(\"Date: 2024-10-11\\nTime: 12:45 PM\")\n",
    "        date_time_label.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(date_time_label)\n",
    "\n",
    "        detection_status = QtWidgets.QLabel(\"Status: Vehicle Detected\")\n",
    "        detection_status.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(detection_status)\n",
    "        main_vertical_layout.addWidget(detection_info_widget)\n",
    "\n",
    "        # Real-time streaming button\n",
    "        real_time_btn = QtWidgets.QPushButton(\"Real-Time Streaming\")\n",
    "        real_time_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        real_time_btn.setFixedWidth(200)\n",
    "        real_time_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        real_time_btn.clicked.connect(self.start_real_time_stream)\n",
    "        main_vertical_layout.addWidget(real_time_btn)\n",
    "\n",
    "        # Analytics button\n",
    "        analytics_btn = QtWidgets.QPushButton(\"Analytics\")\n",
    "        analytics_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        analytics_btn.setFixedWidth(200)\n",
    "        analytics_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        analytics_btn.clicked.connect(self.show_analytics)\n",
    "        main_vertical_layout.addWidget(analytics_btn)\n",
    "\n",
    "        content_layout.addLayout(main_vertical_layout)\n",
    "\n",
    "        # Video display area and other buttons\n",
    "        main_left_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_layout = QtWidgets.QVBoxLayout()\n",
    "        video_controls_layout = QtWidgets.QHBoxLayout()\n",
    "\n",
    "        # Create a layout to position the close button at the top-right corner\n",
    "        close_button = QtWidgets.QPushButton(\"X\")\n",
    "        close_button.setStyleSheet(\"background-color: red; color: white; font-size: 18px; border-radius: 10px;\")\n",
    "        close_button.setFixedSize(30, 30)\n",
    "        close_button.clicked.connect(self.stop_stream)\n",
    "\n",
    "        # Create a wrapper for the video feed and the button\n",
    "        video_feed_wrapper = QtWidgets.QWidget()\n",
    "        video_feed_layout = QtWidgets.QVBoxLayout(video_feed_wrapper)\n",
    "        video_feed_layout.setContentsMargins(0, 0, 0, 0)  # Remove margins for precise alignment\n",
    "\n",
    "        # Create a layout for the close button at the top-right corner\n",
    "        close_button_layout = QtWidgets.QHBoxLayout()\n",
    "        close_button_layout.addStretch(1)  # Add stretch to push the button to the right\n",
    "        close_button_layout.addWidget(close_button)\n",
    "\n",
    "        video_feed_layout.addLayout(close_button_layout)  # Add the close button layout\n",
    "        self.video_feed = QtWidgets.QLabel(\"Video Stream\")\n",
    "        self.video_feed.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.video_feed.setStyleSheet(\"background-color: white; color: black; border: 2px solid;\")\n",
    "        self.video_feed.setFixedSize(970, 480)  # Set a fixed size for the video feed\n",
    "        self.video_feed.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)\n",
    "\n",
    "        video_feed_layout.addWidget(self.video_feed)  # Add the video feed below the button\n",
    "\n",
    "        # Add the video feed wrapper to the main video layout\n",
    "        video_layout.addWidget(video_feed_wrapper)\n",
    "\n",
    "        # Detection buttons\n",
    "        detection_buttons_layout = QtWidgets.QHBoxLayout()\n",
    "        record_btn = QtWidgets.QPushButton(\"Record\")\n",
    "        record_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        record_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        record_btn.clicked.connect(self.record_options)\n",
    "        detection_buttons_layout.addWidget(record_btn)\n",
    "\n",
    "       # Assigning person_detection_btn as an instance variable\n",
    "        self.person_detection_btn = QtWidgets.QPushButton(\"Person Detection\")\n",
    "        self.person_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.person_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.person_detection_btn.clicked.connect(self.toggle_person_detection)\n",
    "        detection_buttons_layout.addWidget(self.person_detection_btn)\n",
    "\n",
    "        self.vehicle_detection_btn = QtWidgets.QPushButton(\"Vehicle Detection\")\n",
    "        self.vehicle_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.vehicle_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.vehicle_detection_btn.clicked.connect(self.toggle_vehicle_detection)\n",
    "        detection_buttons_layout.addWidget(self.vehicle_detection_btn)\n",
    "\n",
    "        video_layout.addLayout(detection_buttons_layout)\n",
    "        main_left_layout.addLayout(video_layout)\n",
    "\n",
    "        # Alerts section\n",
    "        alerts_layout = QtWidgets.QHBoxLayout()\n",
    "        self.alert_label = QtWidgets.QLabel(\"Alert: No Detection\")\n",
    "        self.alert_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.alert_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.alert_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.alert_label)\n",
    "\n",
    "        self.duration_label = QtWidgets.QLabel(\"Time Duration: 00:00:00\")\n",
    "        self.duration_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.duration_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.duration_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.duration_label)\n",
    "\n",
    "        main_left_layout.addLayout(alerts_layout)\n",
    "        content_layout.addLayout(main_left_layout)\n",
    "\n",
    "        main_layout.addLayout(content_layout)\n",
    "        self.setStyleSheet(\"background-color: #222222; color: white;\")\n",
    "\n",
    "        self.setLayout(main_layout)\n",
    "\n",
    "    def create_menu(self):\n",
    "        menu = QtWidgets.QMenu(self)\n",
    "        menu.addAction(\"Option 1\", self.option1_action)\n",
    "        menu.addAction(\"Option 2\", self.option2_action)\n",
    "        return menu\n",
    "\n",
    "    def start_real_time_stream(self):\n",
    "        print(\"Starting real-time video stream\")\n",
    "        self.real_time_stream=True\n",
    "        # Start video capture (assuming you have a webcam at index 0)\n",
    "        self.video_display()  # Call video_feed to initiate video capture\n",
    "        \n",
    "    def video_display(self):\n",
    "        if self.real_time_stream:\n",
    "            self.video_capture = cv2.VideoCapture(0)\n",
    "        elif self.video1:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\853889-hd_1920_1080_25fps.mp4\")\n",
    "        elif self.video2:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\2273134-hd_1280_720_30fps.mp4\")\n",
    "        elif self.video3:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\12494476_3840_2160_50fps.mp4\")\n",
    "            \n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.timer.start(20)\n",
    "\n",
    "    def stop_stream(self):\n",
    "        print(\"Stopping video stream\")\n",
    "        if self.video_capture is not None:\n",
    "            self.video_capture.release()\n",
    "            self.video_capture = None\n",
    "            self.video_feed.clear()  # Clear the video feed when stopping\n",
    "            self.timer.stop()  # Stop the timer\n",
    "\n",
    "            \n",
    "    def update_frame(self):\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if ret:\n",
    "            # Convert the frame from BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Check if person detection is active\n",
    "            if self.detecting_person:\n",
    "                frame = self.detect_person(frame)  # Call person detection if active\n",
    "            if self.detecting_vehicle:\n",
    "                frame = self.detect_vehicle(frame)\n",
    "                \n",
    "            # Get the frame dimensions and convert to QImage\n",
    "            h, w, ch = frame.shape\n",
    "            bytes_per_line = ch * w\n",
    "            qimg = QImage(frame.data, frame.shape[1], frame.shape[0], bytes_per_line, QImage.Format_RGB888)\n",
    "\n",
    "            # Display the QImage on the QLabel (video_feed)\n",
    "            self.video_feed.setPixmap(QPixmap.fromImage(qimg))\n",
    "\n",
    "\n",
    "    def toggle_person_detection(self):\n",
    "        self.detecting_person = not self.detecting_person  # Toggle the state\n",
    "        if self.detecting_person:\n",
    "            self.person_detection_btn.setText(\"Stop Person Detection\")\n",
    "        else:\n",
    "            self.person_detection_btn.setText(\"Start Person Detection\")\n",
    "            self.person_detected_time = None  # Reset timer when detection stops\n",
    "            self.alert_generated = False\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "    def detect_person(self, frame):\n",
    "        # Perform inference\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        person_detected = False\n",
    "        for *box, conf, cls in predictions:\n",
    "            if int(cls) == 0:  # Class index for person\n",
    "                person_detected = True\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "                cv2.putText(frame, f'Person {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        if person_detected:\n",
    "            self.update_person_detection_time()\n",
    "        else:\n",
    "            self.reset_person_detection()\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def update_person_detection_time(self):\n",
    "        \"\"\"Track how long a person has been in the frame and update alert if needed.\"\"\"\n",
    "        if self.person_detected_time is None:\n",
    "            self.person_detected_time = time.time()  # Start the timer\n",
    "\n",
    "        duration = time.time() - self.person_detected_time\n",
    "\n",
    "        # Format the duration\n",
    "        hours, rem = divmod(duration, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        duration_str = \"{:02}:{:02}:{:02}\".format(int(hours), int(minutes), int(seconds))\n",
    "\n",
    "        # Update duration label\n",
    "        self.duration_label.setText(f\"Time Duration: {duration_str}\")\n",
    "\n",
    "        # Check if detection duration exceeds the threshold (e.g., 5 minutes or 300 seconds)\n",
    "        if duration > 10 and not self.alert_generated:  # 300 seconds = 5 minutes\n",
    "            self.alert_label.setText(\"ALERT: Person detected for over 5 minutes!\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.alert_generated = True\n",
    "\n",
    "    def reset_person_detection(self):\n",
    "        \"\"\"Reset the detection if no person is found.\"\"\"\n",
    "        if self.person_detected_time is not None:\n",
    "            self.person_detected_time = None  # Reset the timer\n",
    "            self.duration_label.setText(\"Time Duration: 00:00:00\")\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.alert_generated = False\n",
    "\n",
    "            \n",
    "    def detect_vehicle(self, frame):\n",
    "        # Perform inference using YOLOv5 model\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        current_time = time.time()  # Get the current time\n",
    "\n",
    "        vehicle_detected = False  # Flag to check if any vehicle is detected\n",
    "\n",
    "        for *box, conf, cls in predictions:\n",
    "            if int(cls) in [2, 3, 5, 7]:  # Vehicle class: Car (2), Motorcycle (3), Bus (5), Truck (7)\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                vehicle_type = [\"Car\", \"Motorcycle\", \"Bus\", \"Truck\"][int(cls) - 2]\n",
    "\n",
    "                # Calculate the center of the bounding box\n",
    "                vehicle_center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "                vehicle_detected = True  # Set the flag to True since we detected a vehicle\n",
    "\n",
    "                # Use the center position to track vehicle movement\n",
    "                if vehicle_type not in self.vehicle_positions:\n",
    "                    # If this is a new vehicle, start tracking it\n",
    "                    self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                    self.stationary_alert_generated[vehicle_type] = False\n",
    "                else:\n",
    "                    # Get the previous position and detection start time\n",
    "                    prev_position, start_time = self.vehicle_positions[vehicle_type]\n",
    "\n",
    "                    # Calculate movement distance between current and previous position\n",
    "                    movement_distance = np.linalg.norm(np.array(vehicle_center) - np.array(prev_position))\n",
    "\n",
    "                    # Define a movement threshold (e.g., 20 pixels)\n",
    "                    movement_threshold = 20\n",
    "\n",
    "                    if movement_distance < movement_threshold:\n",
    "                        # If the vehicle has not moved significantly, check how long it's been stationary\n",
    "                        stationary_duration = current_time - start_time\n",
    "\n",
    "                        # Update the time duration label\n",
    "                        self.duration_label.setText(f\"Vehicle {vehicle_type} stationary for {int(stationary_duration)} seconds\")\n",
    "\n",
    "                        # Check if the stationary duration exceeds 5 minutes (300 seconds)\n",
    "                        if stationary_duration > 5 and not self.stationary_alert_generated[vehicle_type]:\n",
    "                            self.alert_label.setText(f\"ALERT: {vehicle_type} stationary for over 5 minutes!\")\n",
    "                            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.stationary_alert_generated[vehicle_type] = True  # Mark that the alert has been generated\n",
    "                    else:\n",
    "                        # If the vehicle moved, reset the detection start time\n",
    "                        self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                        self.stationary_alert_generated[vehicle_type] = False\n",
    "\n",
    "                # Draw the bounding box and label the vehicle\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'{vehicle_type} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # If no vehicle is detected, reset the alert and duration labels\n",
    "        if not vehicle_detected:\n",
    "            self.alert_label.setText(\"No vehicle detected\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setText(\"No detection duration: 00 seconds\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def toggle_vehicle_detection(self):\n",
    "        \"\"\"Toggle vehicle detection on and off.\"\"\"\n",
    "        self.detecting_vehicle = not self.detecting_vehicle  # Toggle the state\n",
    "        if self.detecting_vehicle:\n",
    "            self.vehicle_detection_btn.setText(\"Stop vehicle Detection\")\n",
    "        else:\n",
    "            self.vehicle_detection_btn.setText(\"Start vehicle Detection\")\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "   # Define the three separate play video functions\n",
    "    def play_video_1(self):\n",
    "        print(\"Playing Video 1\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = True  # Set instance variable\n",
    "        self.video2 = False  # Reset other video flags\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_2(self):\n",
    "        print(\"Playing Video 2\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = True\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_3(self):\n",
    "        print(\"Playing Video 3\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = False\n",
    "        self.video3 = True\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "        \n",
    "    # Add your logic to play video 3 here\n",
    "    def show_analytics(self):\n",
    "        print(\"Showing analytics\")\n",
    "\n",
    "    def record_options(self):\n",
    "        print(\"Recording options selected\")\n",
    "\n",
    "    def option1_action(self):\n",
    "        print(\"Option 1 selected\")\n",
    "\n",
    "    def option2_action(self):\n",
    "        print(\"Option 2 selected\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    window = VizOptilyticsApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
