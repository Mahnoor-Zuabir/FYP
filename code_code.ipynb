{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7609321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PyQt5.QtWidgets import (QApplication, QMainWindow, QMenuBar, QAction, \n",
    "                             QWidget, QVBoxLayout, QHBoxLayout, QLabel, QFrame, \n",
    "                             QGridLayout, QListWidget)\n",
    "from PyQt5.QtGui import QImage, QPixmap, QPainter, QColor, QFont\n",
    "from PyQt5.QtCore import QTimer, QTime\n",
    "import time\n",
    "import torch\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "def get_db_connection():\n",
    "        try:\n",
    "            connection = mysql.connector.connect(\n",
    "                host=\"localhost\",\n",
    "                user=\"root\",\n",
    "                password=\"\",  # Your MySQL password here\n",
    "                database=\"viz_optilytics\"\n",
    "            )\n",
    "            return connection\n",
    "        except mysql.connector.Error as err:\n",
    "            print(f\"Error: {err}\")\n",
    "            return None\n",
    "\n",
    "######################################################################################\n",
    "# Load color reference CSV and create color lookup\n",
    "index = [\"color\", \"color_name\", \"hex\", \"R\", \"G\", \"B\"]\n",
    "csv = pd.read_csv('colors.csv', names=index, header=None)\n",
    "\n",
    "def get_color_name(bgr):\n",
    "    b, g, r = bgr\n",
    "    minimum = 10000\n",
    "    cname = \"Unknown\"\n",
    "    for i in range(len(csv)):\n",
    "        d = abs(r - int(csv.loc[i, \"R\"])) + abs(g - int(csv.loc[i, \"G\"])) + abs(b - int(csv.loc[i, \"B\"]))\n",
    "        if d <= minimum:\n",
    "            minimum = d\n",
    "            cname = csv.loc[i, \"color_name\"]\n",
    "    return cname\n",
    "\n",
    "# Function to store vehicle information in the database\n",
    "def store_vehicle_info(color, vehicle_type, model, timestamp):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    sql = \"INSERT INTO vehicles (color, type, model, timestamp) VALUES (%s, %s, %s, %s)\"\n",
    "    values = (color, vehicle_type, model, timestamp)\n",
    "\n",
    "    cursor.execute(sql, values)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Function to extract color from bounding box\n",
    "def extract_color(frame, bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    average_color = np.mean(roi, axis=(0, 1))\n",
    "    color_name = get_color_name(average_color)\n",
    "    return color_name\n",
    "###############################################        \n",
    "\n",
    "class MainWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Camera Feed and Surveillance System\")\n",
    "        self.setGeometry(100, 100, 1200, 800)  # Increased window size\n",
    "\n",
    "        # Create the menu bar\n",
    "        self.menu_bar = self.menuBar()\n",
    "        file_menu = self.menu_bar.addMenu(\"File\")\n",
    "        \n",
    "        exit_action = QAction(\"Exit\", self)\n",
    "        exit_action.triggered.connect(self.close)\n",
    "        file_menu.addAction(exit_action)\n",
    "\n",
    "        # Create a central widget and layout\n",
    "        central_widget = QWidget(self)\n",
    "        self.setCentralWidget(central_widget)\n",
    "        main_layout = QVBoxLayout(central_widget)\n",
    "\n",
    "        # Add a top section for the alert indicator\n",
    "        self.alert_section = QFrame()\n",
    "        self.alert_section.setFixedHeight(50)\n",
    "        self.alert_section.setStyleSheet(\"background-color: #f2f2f2; border-bottom: 2px solid #cccccc;\")\n",
    "        self.alert_label = QLabel(self.alert_section)\n",
    "        self.alert_label.setGeometry(10, 10, 40, 40)  # Adjusted circle position and size\n",
    "        self.alert_label.setStyleSheet(\"border-radius: 20px; background-color: green;\")  # Green initially\n",
    "        alert_title = QLabel(\"Alert Status\", self.alert_section)\n",
    "        alert_title.setFont(QFont(\"Arial\", 12, QFont.Bold))\n",
    "        alert_title.setGeometry(60, 10, 200, 40)\n",
    "        main_layout.addWidget(self.alert_section)\n",
    "\n",
    "        # Create a horizontal layout for left and right sections\n",
    "        content_layout = QHBoxLayout()\n",
    "\n",
    "        # Create a vertical section on the left\n",
    "        left_section = QFrame()\n",
    "        left_section.setStyleSheet(\"background-color: #e6e6e6; border-right: 2px solid #cccccc;\")\n",
    "        left_section.setFixedWidth(200)\n",
    "        left_layout = QVBoxLayout(left_section)\n",
    "\n",
    "        left_title = QLabel(\"Optimal videos\")\n",
    "        left_title.setFont(QFont(\"Arial\", 14, QFont.Bold))\n",
    "        left_layout.addWidget(left_title)\n",
    "#################################################################################\n",
    "        self.video_list = QListWidget()\n",
    "        left_layout.addWidget(self.video_list)\n",
    "        \n",
    "          # Load videos from database\n",
    "        self.load_videos()\n",
    "#################################################################################3\n",
    "        \n",
    "        left_layout.addWidget(QLabel(\"Item 1\"))\n",
    "        left_layout.addWidget(QLabel(\"Item 2\"))\n",
    "\n",
    "        content_layout.addWidget(left_section)\n",
    "\n",
    "        # Create a grid layout for the right section\n",
    "        right_section = QGridLayout()\n",
    "\n",
    "        # Section 1: Live Camera Feed\n",
    "        self.camera_section = QFrame()\n",
    "        self.camera_section.setStyleSheet(\"border: 1px solid #000000;\")\n",
    "        self.camera_label = QLabel(\"Camera Feed\")\n",
    "        self.camera_label.setFixedSize(500, 375)  # Larger feed size\n",
    "        self.camera_layout = QVBoxLayout(self.camera_section)\n",
    "        self.camera_layout.addWidget(self.camera_label)\n",
    "        right_section.addWidget(self.camera_section, 0, 0)\n",
    "\n",
    "        # Section 2: Object Detection\n",
    "        self.detection_section = QFrame()\n",
    "        self.detection_section.setStyleSheet(\"border: 1px solid #000000;\")\n",
    "        self.detection_label = QLabel(\"Detected Objects\")\n",
    "        self.detection_label.setFixedSize(500, 375)\n",
    "        self.detection_layout = QVBoxLayout(self.detection_section)\n",
    "        self.detection_layout.addWidget(self.detection_label)\n",
    "        right_section.addWidget(self.detection_section, 0, 1)\n",
    "\n",
    "        # Section 3: Graphs\n",
    "        self.graph_section_1 = QFrame()\n",
    "        self.graph_section_1.setStyleSheet(\"border: 1px solid #000000;\")\n",
    "        self.graph_layout_1 = QVBoxLayout(self.graph_section_1)\n",
    "        self.graph_layout_1.addWidget(QLabel(\"Graph 1\"))\n",
    "        right_section.addWidget(self.graph_section_1, 1, 0)\n",
    "\n",
    "        # Section 4: More Graphs\n",
    "        self.graph_section_2 = QFrame()\n",
    "        self.graph_section_2.setStyleSheet(\"border: 1px solid #000000;\")\n",
    "        self.graph_layout_2 = QVBoxLayout(self.graph_section_2)\n",
    "        self.graph_layout_2.addWidget(QLabel(\"Graph 2\"))\n",
    "        right_section.addWidget(self.graph_section_2, 1, 1)\n",
    "\n",
    "        content_layout.addLayout(right_section)\n",
    "        main_layout.addLayout(content_layout)\n",
    "\n",
    "        # Initialize YOLOv5, tracking data, and camera\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 model\n",
    "\n",
    "        self.tracked_objects = {}  # To store object ID and time of detection\n",
    "        self.alert_active = False  # Keep track of alert state\n",
    "        self.init_camera()\n",
    "        \n",
    "\n",
    "    def init_camera(self):\n",
    "        \"\"\"Initialize the camera feed.\"\"\"\n",
    "        self.cap = cv2.VideoCapture(0)  # Open the default camera\n",
    "        self.timer = QTimer(self)\n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.timer.start(20)  # Update the frame every 20 ms\n",
    "\n",
    "    def update_frame(self):\n",
    "        \"\"\"Update the camera feed frame.\"\"\"\n",
    "        ret, frame = self.cap.read()\n",
    "        if ret:\n",
    "            # Convert the frame to RGB format for display\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            h, w, ch = frame.shape\n",
    "            bytes_per_line = ch * w\n",
    "            qt_image = QImage(frame.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "            self.camera_label.setPixmap(QPixmap.fromImage(qt_image))\n",
    "\n",
    "            # Process frame for object detection\n",
    "            self.process_detections(frame)\n",
    "\n",
    "            # Update the alert state\n",
    "            self.update_alert()\n",
    "\n",
    "            \n",
    "\n",
    "    def load_videos(self):\n",
    "        connection = get_db_connection()\n",
    "        if not connection:\n",
    "            QMessageBox.critical(self, \"Database Error\", \"Could not connect to the database.\")\n",
    "            return\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT id, video_name FROM video_storage\")\n",
    "        videos = cursor.fetchall()\n",
    "\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "\n",
    "        for video in videos:\n",
    "            self.video_list.addItem(f\"{video[0]}: {video[1]}\")\n",
    "            \n",
    "      # Function to check if the vehicle is static\n",
    "    def is_static(self, current_position, last_position, threshold=10):\n",
    "        # Check if the bounding box coordinates have not changed significantly\n",
    "        return (abs(current_position[0] - last_position[0]) < threshold and\n",
    "                abs(current_position[1] - last_position[1]) < threshold and\n",
    "                abs(current_position[2] - last_position[2]) < threshold and\n",
    "                abs(current_position[3] - last_position[3]) < threshold)\n",
    "            \n",
    "    def process_detections(self, frame):\n",
    "        \"\"\"Detect objects in the frame using YOLOv5 and track presence of persons and vehicles.\"\"\"\n",
    "        results = self.model(frame)  # Perform YOLOv5 detection\n",
    "        detections = results.xyxy[0].numpy()  # Get bounding boxes and labels\n",
    "\n",
    "        current_time = time.time()\n",
    "        person_detected = False\n",
    "        vehicle_detected = False\n",
    "\n",
    "        for detection in detections:\n",
    "            x1, y1, x2, y2, conf, class_id = detection\n",
    "            class_id = int(class_id)\n",
    "            label = self.model.names[class_id]\n",
    "\n",
    "            # Logic for person detection\n",
    "            if label == 'person' and conf > 0.3:  # Adjust confidence threshold as needed\n",
    "                person_detected = True\n",
    "                object_id = \"person\"  # Simple ID for tracking person\n",
    "                color = (0, 255, 0)  # Green for first detection\n",
    "                # Track detection time\n",
    "                if object_id not in self.tracked_objects:\n",
    "                    self.tracked_objects[object_id] = current_time  # Store the time of first detection\n",
    "                else:\n",
    "                    elapsed_time = current_time - self.tracked_objects[object_id]\n",
    "\n",
    "                    # Generate alert if person has been detected for more than 5 seconds\n",
    "                    if elapsed_time > 5:\n",
    "                        color = (0, 0, 255)  # Red for alert\n",
    "                        self.alert_active = True\n",
    "                        # Add alert handling logic here\n",
    "                    else:\n",
    "                        color = (0, 255, 0)  # Green for normal detection\n",
    "                        self.alert_active = False\n",
    "\n",
    "                # Draw bounding box and label\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "                cv2.putText(frame, f\"{label} {int(conf * 100)}%\", (int(x1), int(y1) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "            elif label in ['car', 'truck', 'bus'] and conf > 0.3:  # Adjust confidence threshold as needed\n",
    "                vehicle_detected = True\n",
    "                object_id = label  # Use vehicle type as ID for tracking\n",
    "\n",
    "                # Track detection time\n",
    "                current_position = (int(x1), int(y1), int(x2), int(y2))  # Store current bounding box coordinates\n",
    "\n",
    "                if object_id not in self.tracked_objects:\n",
    "                    self.tracked_objects[object_id] = {\n",
    "                        'first_detection_time': current_time,\n",
    "                        'last_position': current_position,\n",
    "                        'elapsed_time': 0\n",
    "                    }\n",
    "                    color = (0, 255, 0)  # Green for first detection\n",
    "                else:\n",
    "                    tracked_data = self.tracked_objects[object_id]\n",
    "                    elapsed_time = current_time - tracked_data['first_detection_time']\n",
    "\n",
    "                    # Check if the position is the same (within a threshold)\n",
    "                    if self.is_static(current_position, tracked_data['last_position']):\n",
    "                        tracked_data['elapsed_time'] = elapsed_time\n",
    "\n",
    "                        # Generate alert if vehicle has been static for more than 5 seconds\n",
    "                        if tracked_data['elapsed_time'] > 5:\n",
    "                            color = (0, 0, 255)  # Red for alert\n",
    "                            self.alert_active = True\n",
    "                            # Add alert handling logic here\n",
    "                        else:\n",
    "                            color = (0, 255, 0)  # Green for normal detection\n",
    "                            self.alert_active = False\n",
    "                    else:\n",
    "                        # Reset the detection time if the vehicle has moved\n",
    "                        tracked_data['first_detection_time'] = current_time\n",
    "                        tracked_data['last_position'] = current_position\n",
    "                        tracked_data['elapsed_time'] = 0  # Reset elapsed time\n",
    "\n",
    "                # Draw bounding box and label\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "                cv2.putText(frame, f\"{label} {int(conf * 100)}%\", (int(x1), int(y1) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "\n",
    "        # If no person or vehicle is detected, reset tracking\n",
    "        if not person_detected:\n",
    "            self.tracked_objects.pop(\"person\", None)  # Remove person ID if not detected\n",
    "        if not vehicle_detected:\n",
    "            for vehicle_type in ['car', 'truck', 'bus']:\n",
    "                self.tracked_objects.pop(vehicle_type, None)  # Remove vehicle IDs if not detected\n",
    "\n",
    "        # Update the QPixmap in the detection label\n",
    "        qt_image = QImage(frame.data, frame.shape[1], frame.shape[0], QImage.Format_RGB888)\n",
    "        self.detection_label.setPixmap(QPixmap.fromImage(qt_image))\n",
    "\n",
    "    \n",
    "    def update_alert(self):\n",
    "        \"\"\"Update the alert indicator based on the presence of people/vehicles for too long.\"\"\"\n",
    "        if self.alert_active:\n",
    "            self.alert_label.setStyleSheet(\"border-radius: 20px; background-color: red;\")  # Red when alert is active\n",
    "        else:\n",
    "            self.alert_label.setStyleSheet(\"border-radius: 20px; background-color: green;\")  # Green when no alert\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        \"\"\"Handle the event when the window is closed.\"\"\"\n",
    "        self.timer.stop()  # Stop the timer\n",
    "        self.cap.release()  # Release the camera\n",
    "        cv2.destroyAllWindows()  # Close any OpenCV windows\n",
    "        event.accept()  # Accept the event and proceed with the closing\n",
    "\n",
    "def main():\n",
    "    app = QApplication(sys.argv)\n",
    "    \n",
    "    window = MainWindow()\n",
    "    window.show()\n",
    "    \n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca267cc",
   "metadata": {},
   "source": [
    "# THIRD VIDEO NOT OVERFLOWING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1038cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing Video 3\n",
      "Stopping video stream\n",
      "Recording options selected\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 1.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 1.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 1.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 7.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 7.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 7.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 7.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 0.0\n",
      "Class detected: 0.0\n",
      "Class detected: 7.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 0.0\n",
      "Class detected: 7.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 7.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 0.0\n",
      "Class detected: 7.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 0.0\n",
      "Class detected: 7.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 0.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 7.0\n",
      "Class detected: 2.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 58.0\n",
      "Class detected: 7.0\n",
      "Class detected: 0.0\n",
      "Class detected: 2.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Class detected: 0.0\n",
      "Class detected: 58.0\n",
      "Class detected: 2.0\n",
      "Class detected: 7.0\n",
      "Stopping video stream\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import sys\n",
    "import cv2\n",
    "from PyQt5.QtCore import QTimer\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QMessageBox\n",
    "import os\n",
    "\n",
    "class VizOptilyticsApp(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Viz Optilytics\")\n",
    "        self.setGeometry(0, 0, 1200, 700)\n",
    "\n",
    "        # Initialize QTimer (fixing the timer error)\n",
    "        self.timer = QTimer(self)  # Initialize the QTimer object\n",
    "\n",
    "        # Initialize video capture object\n",
    "        self.video_capture = None\n",
    "\n",
    "        # Detection states\n",
    "        self.detecting_person = False\n",
    "        self.detecting_vehicle = False\n",
    "\n",
    "         # Track detected persons and duration\n",
    "        self.person_detected_time = None  # To track when a person is first detected\n",
    "        self.alert_generated = False  # To track if alert is already generated\n",
    "        \n",
    "        # Load YOLOv5 model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 model\n",
    "\n",
    "         # Initialize alert labels for dynamic updates\n",
    "        self.alert_label = None\n",
    "        self.duration_label = None\n",
    "        \n",
    "        self.detection_grace_period = 10  # Number of frames to maintain detection box\n",
    "        self.frames_since_last_detection = 0\n",
    "        self.last_detection_box = None  # Store the last detected bounding box\n",
    "\n",
    "        \n",
    "        self.vehicle_positions = {}  # Stores vehicle ID -> (last position, detection start time)\n",
    "        self.stationary_alert_generated = {}  # To track if stationary alert has been generated\n",
    "\n",
    "        self.video1 = False\n",
    "        self.video2 = False\n",
    "        self.video3 = False\n",
    "        self.real_time_stream = False\n",
    "\n",
    "        self.initUI()\n",
    "    def initUI(self):\n",
    "        # Main Layout\n",
    "        main_layout = QtWidgets.QVBoxLayout(self)\n",
    "        main_layout.setContentsMargins(50, 50, 50, 30)\n",
    "\n",
    "        # Top Section: Title and Menu\n",
    "        top_bar = QtWidgets.QHBoxLayout()\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        title = QtWidgets.QLabel(\"Viz Optilytics\")\n",
    "        title.setFont(QtGui.QFont(\"Arial\", 28, QtGui.QFont.Bold))\n",
    "        title.setStyleSheet(\"color: #FFD700;\")\n",
    "        title.setAlignment(QtCore.Qt.AlignCenter)\n",
    "\n",
    "        top_bar.addWidget(title)\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        menu_button = QtWidgets.QPushButton(\"â˜°\")\n",
    "        menu_button.setStyleSheet(\"background-color: #333333; color: #FFD700; font-size: 20px; border-radius: 10px;\")\n",
    "        menu_button.setFixedSize(50, 50)\n",
    "        menu_button.setMenu(self.create_menu())\n",
    "\n",
    "        top_bar.addWidget(menu_button)\n",
    "        main_layout.addLayout(top_bar)\n",
    "\n",
    "        title_spacer = QtWidgets.QSpacerItem(20, 40, QtWidgets.QSizePolicy.Minimum, QtWidgets.QSizePolicy.Fixed)\n",
    "        main_layout.addItem(title_spacer)\n",
    "\n",
    "        # Horizontal layout split: Videos, Buttons, Alerts, etc.\n",
    "        content_layout = QtWidgets.QHBoxLayout()\n",
    "        content_layout.setSpacing(20)\n",
    "\n",
    "        # Video Buttons and Info Section\n",
    "        main_vertical_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_list_layout = QtWidgets.QVBoxLayout()\n",
    "        recorded_videos_label = QtWidgets.QLabel(\"Recorded Videos\")\n",
    "        recorded_videos_label.setFont(QtGui.QFont(\"Arial\", 16, QtGui.QFont.Bold))\n",
    "        recorded_videos_label.setStyleSheet(\"color: white;\")\n",
    "        video_list_layout.addWidget(recorded_videos_label)\n",
    "\n",
    "        # Video buttons\n",
    "        for i in range(3):\n",
    "            video_btn = QtWidgets.QPushButton(f\"Video {i + 1}\")\n",
    "            video_btn.setStyleSheet(\"background-color: #555555; color: white; border-radius: 8px; padding: 8px;\")\n",
    "            video_btn.setFixedWidth(200)\n",
    "            video_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "\n",
    "            # Connect each button to a separate function based on the index\n",
    "            if i == 0:\n",
    "                video_btn.clicked.connect(self.play_video_1)\n",
    "            elif i == 1:\n",
    "                video_btn.clicked.connect(self.play_video_2)\n",
    "            elif i == 2:\n",
    "                video_btn.clicked.connect(self.play_video_3)\n",
    "\n",
    "            video_list_layout.addWidget(video_btn)\n",
    "\n",
    "        main_vertical_layout.addLayout(video_list_layout)\n",
    "\n",
    "        detection_info_widget = QtWidgets.QWidget()\n",
    "        detection_info_layout = QtWidgets.QVBoxLayout(detection_info_widget)\n",
    "        detection_info_layout.setSpacing(0)\n",
    "        detection_info_widget.setStyleSheet(\"background-color: #444444; border-radius: 10px;\")\n",
    "\n",
    "        date_time_label = QtWidgets.QLabel(\"Date: 2024-10-11\\nTime: 12:45 PM\")\n",
    "        date_time_label.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(date_time_label)\n",
    "\n",
    "        detection_status = QtWidgets.QLabel(\"Status: Vehicle Detected\")\n",
    "        detection_status.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(detection_status)\n",
    "        main_vertical_layout.addWidget(detection_info_widget)\n",
    "\n",
    "        # Real-time streaming button\n",
    "        real_time_btn = QtWidgets.QPushButton(\"Real-Time Streaming\")\n",
    "        real_time_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        real_time_btn.setFixedWidth(200)\n",
    "        real_time_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        real_time_btn.clicked.connect(self.start_real_time_stream)\n",
    "        main_vertical_layout.addWidget(real_time_btn)\n",
    "\n",
    "        # Analytics button\n",
    "        analytics_btn = QtWidgets.QPushButton(\"Analytics\")\n",
    "        analytics_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        analytics_btn.setFixedWidth(200)\n",
    "        analytics_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        analytics_btn.clicked.connect(self.show_analytics)\n",
    "        main_vertical_layout.addWidget(analytics_btn)\n",
    "\n",
    "        content_layout.addLayout(main_vertical_layout)\n",
    "\n",
    "        # Video display area and other buttons\n",
    "        main_left_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_layout = QtWidgets.QVBoxLayout()\n",
    "        video_controls_layout = QtWidgets.QHBoxLayout()\n",
    "\n",
    "        # Create a layout to position the close button at the top-right corner\n",
    "        close_button = QtWidgets.QPushButton(\"X\")\n",
    "        close_button.setStyleSheet(\"background-color: red; color: white; font-size: 18px; border-radius: 10px;\")\n",
    "        close_button.setFixedSize(30, 30)\n",
    "        close_button.clicked.connect(self.stop_stream)\n",
    "\n",
    "        # Create a wrapper for the video feed and the button\n",
    "        video_feed_wrapper = QtWidgets.QWidget()\n",
    "        video_feed_layout = QtWidgets.QVBoxLayout(video_feed_wrapper)\n",
    "        video_feed_layout.setContentsMargins(0, 0, 0, 0)  # Remove margins for precise alignment\n",
    "\n",
    "        # Create a layout for the close button at the top-right corner\n",
    "        close_button_layout = QtWidgets.QHBoxLayout()\n",
    "        close_button_layout.addStretch(1)  # Add stretch to push the button to the right\n",
    "        close_button_layout.addWidget(close_button)\n",
    "\n",
    "        video_feed_layout.addLayout(close_button_layout)  # Add the close button layout\n",
    "        self.video_feed = QtWidgets.QLabel(\"Video Stream\")\n",
    "        self.video_feed.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.video_feed.setStyleSheet(\"background-color: white; color: black; border: 2px solid;\")\n",
    "        self.video_feed.setFixedSize(970, 480)  # Set a fixed size for the video feed\n",
    "        self.video_feed.setScaledContents(True)\n",
    "        self.video_feed.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)\n",
    "\n",
    "        video_feed_layout.addWidget(self.video_feed)  # Add the video feed below the button\n",
    "\n",
    "        # Add the video feed wrapper to the main video layout\n",
    "        video_layout.addWidget(video_feed_wrapper)\n",
    "\n",
    "        # Detection buttons\n",
    "        detection_buttons_layout = QtWidgets.QHBoxLayout()\n",
    "        record_btn = QtWidgets.QPushButton(\"Record\")\n",
    "        record_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        record_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        record_btn.clicked.connect(self.record_options)\n",
    "        detection_buttons_layout.addWidget(record_btn)\n",
    "\n",
    "       # Assigning person_detection_btn as an instance variable\n",
    "        self.person_detection_btn = QtWidgets.QPushButton(\"Person Detection\")\n",
    "        self.person_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.person_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.person_detection_btn.clicked.connect(self.toggle_person_detection)\n",
    "        detection_buttons_layout.addWidget(self.person_detection_btn)\n",
    "\n",
    "        self.vehicle_detection_btn = QtWidgets.QPushButton(\"Vehicle Detection\")\n",
    "        self.vehicle_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.vehicle_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.vehicle_detection_btn.clicked.connect(self.toggle_vehicle_detection)\n",
    "        detection_buttons_layout.addWidget(self.vehicle_detection_btn)\n",
    "\n",
    "        video_layout.addLayout(detection_buttons_layout)\n",
    "        main_left_layout.addLayout(video_layout)\n",
    "\n",
    "        # Alerts section\n",
    "        alerts_layout = QtWidgets.QHBoxLayout()\n",
    "        self.alert_label = QtWidgets.QLabel(\"Alert: No Detection\")\n",
    "        self.alert_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.alert_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.alert_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.alert_label)\n",
    "\n",
    "        self.duration_label = QtWidgets.QLabel(\"Time Duration: 00:00:00\")\n",
    "        self.duration_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.duration_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.duration_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.duration_label)\n",
    "\n",
    "        main_left_layout.addLayout(alerts_layout)\n",
    "        content_layout.addLayout(main_left_layout)\n",
    "\n",
    "        main_layout.addLayout(content_layout)\n",
    "        self.setStyleSheet(\"background-color: #222222; color: white;\")\n",
    "\n",
    "        self.setLayout(main_layout)\n",
    "\n",
    "    def create_menu(self):\n",
    "        menu = QtWidgets.QMenu(self)\n",
    "        menu.addAction(\"Option 1\", self.option1_action)\n",
    "        menu.addAction(\"Option 2\", self.option2_action)\n",
    "        return menu\n",
    "\n",
    "    def start_real_time_stream(self):\n",
    "        print(\"Starting real-time video stream\")\n",
    "        self.real_time_stream=True\n",
    "        # Start video capture (assuming you have a webcam at index 0)\n",
    "        self.video_display()  # Call video_feed to initiate video capture\n",
    "        \n",
    "    def video_display(self):\n",
    "        if self.real_time_stream:\n",
    "            self.video_capture = cv2.VideoCapture(0)\n",
    "        elif self.video1:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\853889-hd_1920_1080_25fps.mp4\")\n",
    "        elif self.video2:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\2273134-hd_1280_720_30fps.mp4\")\n",
    "        elif self.video3:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\12494476_3840_2160_50fps.mp4\")\n",
    "            \n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.timer.start(20)\n",
    "\n",
    "    def stop_stream(self):\n",
    "        print(\"Stopping video stream\")\n",
    "        if self.video_capture is not None:\n",
    "            self.video_capture.release()\n",
    "            self.video_capture = None\n",
    "            self.video_feed.clear()  # Clear the video feed when stopping\n",
    "            self.timer.stop()  # Stop the timer\n",
    "\n",
    "            \n",
    "    def update_frame(self):\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if ret:\n",
    "            # Convert the frame from BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Check if person detection is active\n",
    "            if self.detecting_person:\n",
    "                frame = self.detect_person(frame)  # Call person detection if active\n",
    "            if self.detecting_vehicle:\n",
    "                frame = self.detect_vehicle(frame)\n",
    "                \n",
    "            # Get the frame dimensions and convert to QImage\n",
    "            h, w, ch = frame.shape\n",
    "            bytes_per_line = ch * w\n",
    "            qimg = QImage(frame.data, frame.shape[1], frame.shape[0], bytes_per_line, QImage.Format_RGB888)\n",
    "\n",
    "            # Display the QImage on the QLabel (video_feed)\n",
    "            self.video_feed.setPixmap(QPixmap.fromImage(qimg))\n",
    "\n",
    "\n",
    "    def toggle_person_detection(self):\n",
    "        self.detecting_person = not self.detecting_person  # Toggle the state\n",
    "        if self.detecting_person:\n",
    "            self.person_detection_btn.setText(\"Stop Person Detection\")\n",
    "        else:\n",
    "            self.person_detection_btn.setText(\"Start Person Detection\")\n",
    "            self.person_detected_time = None  # Reset timer when detection stops\n",
    "            self.alert_generated = False\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "    def detect_person(self, frame):\n",
    "        # Perform inference\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        person_detected = False\n",
    "        for *box, conf, cls in predictions:\n",
    "            if int(cls) == 0:  # Class index for person\n",
    "                person_detected = True\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "                cv2.putText(frame, f'Person {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        if person_detected:\n",
    "            self.update_person_detection_time()\n",
    "        else:\n",
    "            self.reset_person_detection()\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def update_person_detection_time(self):\n",
    "        \"\"\"Track how long a person has been in the frame and update alert if needed.\"\"\"\n",
    "        if self.person_detected_time is None:\n",
    "            self.person_detected_time = time.time()  # Start the timer\n",
    "\n",
    "        duration = time.time() - self.person_detected_time\n",
    "\n",
    "        # Format the duration\n",
    "        hours, rem = divmod(duration, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        duration_str = \"{:02}:{:02}:{:02}\".format(int(hours), int(minutes), int(seconds))\n",
    "\n",
    "        # Update duration label\n",
    "        self.duration_label.setText(f\"Time Duration: {duration_str}\")\n",
    "\n",
    "        # Check if detection duration exceeds the threshold (e.g., 5 minutes or 300 seconds)\n",
    "        if duration > 10 and not self.alert_generated:  # 300 seconds = 5 minutes\n",
    "            self.alert_label.setText(\"ALERT: Person detected for over 5 minutes!\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.alert_generated = True\n",
    "\n",
    "    def reset_person_detection(self):\n",
    "        \"\"\"Reset the detection if no person is found.\"\"\"\n",
    "        if self.person_detected_time is not None:\n",
    "            self.person_detected_time = None  # Reset the timer\n",
    "            self.duration_label.setText(\"Time Duration: 00:00:00\")\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.alert_generated = False\n",
    "\n",
    "            \n",
    "    def detect_vehicle(self, frame):\n",
    "        # Perform inference using YOLOv5 model\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        current_time = time.time()  # Get the current time\n",
    "        vehicle_detected = False  # Flag to check if any vehicle is detected\n",
    "\n",
    "        # Dictionary to map class ids to vehicle types\n",
    "        vehicle_class_map = {\n",
    "            2: \"Car\",        # Car\n",
    "            3: \"Motorcycle\", # Motorcycle\n",
    "            5: \"Bus\",       # Bus\n",
    "            7: \"Truck\"      # Truck\n",
    "        }\n",
    "\n",
    "        for *box, conf, cls in predictions:\n",
    "            print(f\"Class detected: {cls}\")\n",
    "            cls = int(cls)  # Convert cls to an integer for checking\n",
    "\n",
    "            if cls in vehicle_class_map:  # Only process valid vehicle classes\n",
    "                vehicle_type = vehicle_class_map[cls]\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # Calculate the center of the bounding box\n",
    "                vehicle_center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "                vehicle_detected = True  # Set the flag to True since we detected a vehicle\n",
    "\n",
    "                # Use the center position to track vehicle movement\n",
    "                if vehicle_type not in self.vehicle_positions:\n",
    "                    # If this is a new vehicle, start tracking it\n",
    "                    self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                    self.stationary_alert_generated[vehicle_type] = False\n",
    "                else:\n",
    "                    # Get the previous position and detection start time\n",
    "                    prev_position, start_time = self.vehicle_positions[vehicle_type]\n",
    "\n",
    "                    # Calculate movement distance between current and previous position\n",
    "                    movement_distance = np.linalg.norm(np.array(vehicle_center) - np.array(prev_position))\n",
    "\n",
    "                    # Define a movement threshold (e.g., 20 pixels)\n",
    "                    movement_threshold = 20\n",
    "\n",
    "                    if movement_distance < movement_threshold:\n",
    "                        # If the vehicle has not moved significantly, check how long it's been stationary\n",
    "                        stationary_duration = current_time - start_time\n",
    "\n",
    "                        # Update the time duration label\n",
    "                        self.duration_label.setText(f\"Vehicle {vehicle_type} stationary for {int(stationary_duration)} seconds\")\n",
    "\n",
    "                        # Check if the stationary duration exceeds 5 minutes (300 seconds)\n",
    "                        if stationary_duration > 5 and not self.stationary_alert_generated[vehicle_type]:\n",
    "                            self.alert_label.setText(f\"ALERT: {vehicle_type} stationary for over 5 minutes!\")\n",
    "                            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.stationary_alert_generated[vehicle_type] = True  # Mark that the alert has been generated\n",
    "                    else:\n",
    "                        # If the vehicle moved, reset the detection start time\n",
    "                        self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                        self.stationary_alert_generated[vehicle_type] = False\n",
    "\n",
    "                # Draw the bounding box and label the vehicle\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'{vehicle_type} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # If no vehicle is detected, reset the alert and duration labels\n",
    "        if not vehicle_detected:\n",
    "            self.alert_label.setText(\"No vehicle detected\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setText(\"No detection duration: 00 seconds\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "\n",
    "    def toggle_vehicle_detection(self):\n",
    "        \"\"\"Toggle vehicle detection on and off.\"\"\"\n",
    "        self.detecting_vehicle = not self.detecting_vehicle  # Toggle the state\n",
    "        if self.detecting_vehicle:\n",
    "            self.vehicle_detection_btn.setText(\"Stop vehicle Detection\")\n",
    "        else:\n",
    "            self.vehicle_detection_btn.setText(\"Start vehicle Detection\")\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "   # Define the three separate play video functions\n",
    "    def play_video_1(self):\n",
    "        print(\"Playing Video 1\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = True  # Set instance variable\n",
    "        self.video2 = False  # Reset other video flags\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_2(self):\n",
    "        print(\"Playing Video 2\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = True\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_3(self):\n",
    "        print(\"Playing Video 3\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = False\n",
    "        self.video3 = True\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    # Add your logic to play video 3 here\n",
    "    def show_analytics(self):\n",
    "        print(\"Showing analytics\")\n",
    "\n",
    "    def record_options(self):\n",
    "        print(\"Recording options selected\")\n",
    "\n",
    "    def option1_action(self):\n",
    "        print(\"Option 1 selected\")\n",
    "\n",
    "    def option2_action(self):\n",
    "        print(\"Option 2 selected\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    window = VizOptilyticsApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad4bb1",
   "metadata": {},
   "source": [
    "# FAULTY VIDEO DOWNLOAD I AM WORKING ON THIS  23/10/2024 WEDNESDAY 3:37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae7a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time video stream\n",
      "An unexpected error occurred: [WinError 32] The process cannot access the file because it is being used by another process: '2024-10-23_03-48-41.avi'\n",
      "Stopping video stream\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import sys\n",
    "import cv2\n",
    "from PyQt5.QtCore import QTimer\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QMessageBox\n",
    "import os\n",
    "\n",
    "\n",
    "# Database connection\n",
    "def connect_db():\n",
    "    return mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"\",  # Replace with your password\n",
    "        database=\"video_db\"  # Replace with your database name\n",
    "    )\n",
    "\n",
    "# Save video to database with name\n",
    "def save_video_to_db(video_path, video_name):\n",
    "    try:\n",
    "        conn = connect_db()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        with open(video_path, 'rb') as f:\n",
    "            video_data = f.read()\n",
    "\n",
    "        query = \"INSERT INTO videos (video_name, video_file) VALUES (%s, %s)\"\n",
    "        cursor.execute(query, (video_name, video_data))\n",
    "        conn.commit()\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        os.remove(video_path)  # Delete the file from the local system\n",
    "        print(\"Video saved to database and deleted from the computer successfully.\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The video file was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "        \n",
    "class VizOptilyticsApp(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Viz Optilytics\")\n",
    "        self.setGeometry(0, 0, 1200, 700)\n",
    "\n",
    "        # Initialize QTimer (fixing the timer error)\n",
    "        self.timer = QTimer(self)  # Initialize the QTimer object\n",
    "\n",
    "        # Initialize video capture object\n",
    "        self.video_capture = None\n",
    "\n",
    "        # Detection states\n",
    "        self.detecting_person = False\n",
    "        self.detecting_vehicle = False\n",
    "\n",
    "         # Track detected persons and duration\n",
    "        self.person_detected_time = None  # To track when a person is first detected\n",
    "        self.alert_generated = False  # To track if alert is already generated\n",
    "        \n",
    "        # Load YOLOv5 model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 model\n",
    "\n",
    "         # Initialize alert labels for dynamic updates\n",
    "        self.alert_label = None\n",
    "        self.duration_label = None\n",
    "        \n",
    "        self.detection_grace_period = 10  # Number of frames to maintain detection box\n",
    "        self.frames_since_last_detection = 0\n",
    "        self.last_detection_box = None  # Store the last detected bounding box\n",
    "\n",
    "        \n",
    "        self.vehicle_positions = {}  # Stores vehicle ID -> (last position, detection start time)\n",
    "        self.stationary_alert_generated = {}  # To track if stationary alert has been generated\n",
    "\n",
    "        self.video1 = False\n",
    "        self.video2 = False\n",
    "        self.video3 = False\n",
    "        self.real_time_stream = False\n",
    "        self.record_flag=True\n",
    "\n",
    "        self.initUI()\n",
    "    def initUI(self):\n",
    "        # Main Layout\n",
    "        main_layout = QtWidgets.QVBoxLayout(self)\n",
    "        main_layout.setContentsMargins(50, 50, 50, 30)\n",
    "\n",
    "        # Top Section: Title and Menu\n",
    "        top_bar = QtWidgets.QHBoxLayout()\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        title = QtWidgets.QLabel(\"Viz Optilytics\")\n",
    "        title.setFont(QtGui.QFont(\"Arial\", 28, QtGui.QFont.Bold))\n",
    "        title.setStyleSheet(\"color: #FFD700;\")\n",
    "        title.setAlignment(QtCore.Qt.AlignCenter)\n",
    "\n",
    "        top_bar.addWidget(title)\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        menu_button = QtWidgets.QPushButton(\"â˜°\")\n",
    "        menu_button.setStyleSheet(\"background-color: #333333; color: #FFD700; font-size: 20px; border-radius: 10px;\")\n",
    "        menu_button.setFixedSize(50, 50)\n",
    "        menu_button.setMenu(self.create_menu())\n",
    "\n",
    "        top_bar.addWidget(menu_button)\n",
    "        main_layout.addLayout(top_bar)\n",
    "\n",
    "        title_spacer = QtWidgets.QSpacerItem(20, 40, QtWidgets.QSizePolicy.Minimum, QtWidgets.QSizePolicy.Fixed)\n",
    "        main_layout.addItem(title_spacer)\n",
    "\n",
    "        # Horizontal layout split: Videos, Buttons, Alerts, etc.\n",
    "        content_layout = QtWidgets.QHBoxLayout()\n",
    "        content_layout.setSpacing(20)\n",
    "\n",
    "        # Video Buttons and Info Section\n",
    "        main_vertical_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_list_layout = QtWidgets.QVBoxLayout()\n",
    "        recorded_videos_label = QtWidgets.QLabel(\"Recorded Videos\")\n",
    "        recorded_videos_label.setFont(QtGui.QFont(\"Arial\", 16, QtGui.QFont.Bold))\n",
    "        recorded_videos_label.setStyleSheet(\"color: white;\")\n",
    "        video_list_layout.addWidget(recorded_videos_label)\n",
    "\n",
    "        # Video buttons\n",
    "        for i in range(3):\n",
    "            video_btn = QtWidgets.QPushButton(f\"Video {i + 1}\")\n",
    "            video_btn.setStyleSheet(\"background-color: #555555; color: white; border-radius: 8px; padding: 8px;\")\n",
    "            video_btn.setFixedWidth(200)\n",
    "            video_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "\n",
    "            # Connect each button to a separate function based on the index\n",
    "            if i == 0:\n",
    "                video_btn.clicked.connect(self.play_video_1)\n",
    "            elif i == 1:\n",
    "                video_btn.clicked.connect(self.play_video_2)\n",
    "            elif i == 2:\n",
    "                video_btn.clicked.connect(self.play_video_3)\n",
    "\n",
    "            video_list_layout.addWidget(video_btn)\n",
    "\n",
    "        main_vertical_layout.addLayout(video_list_layout)\n",
    "\n",
    "        detection_info_widget = QtWidgets.QWidget()\n",
    "        detection_info_layout = QtWidgets.QVBoxLayout(detection_info_widget)\n",
    "        detection_info_layout.setSpacing(0)\n",
    "        detection_info_widget.setStyleSheet(\"background-color: #444444; border-radius: 10px;\")\n",
    "\n",
    "        date_time_label = QtWidgets.QLabel(\"Date: 2024-10-11\\nTime: 12:45 PM\")\n",
    "        date_time_label.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(date_time_label)\n",
    "\n",
    "        detection_status = QtWidgets.QLabel(\"Status: Vehicle Detected\")\n",
    "        detection_status.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(detection_status)\n",
    "        main_vertical_layout.addWidget(detection_info_widget)\n",
    "\n",
    "        # Real-time streaming button\n",
    "        real_time_btn = QtWidgets.QPushButton(\"Real-Time Streaming\")\n",
    "        real_time_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        real_time_btn.setFixedWidth(200)\n",
    "        real_time_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        real_time_btn.clicked.connect(self.start_real_time_stream)\n",
    "        main_vertical_layout.addWidget(real_time_btn)\n",
    "\n",
    "        # Analytics button\n",
    "        analytics_btn = QtWidgets.QPushButton(\"Analytics\")\n",
    "        analytics_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        analytics_btn.setFixedWidth(200)\n",
    "        analytics_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        analytics_btn.clicked.connect(self.show_analytics)\n",
    "        main_vertical_layout.addWidget(analytics_btn)\n",
    "\n",
    "        content_layout.addLayout(main_vertical_layout)\n",
    "\n",
    "        # Video display area and other buttons\n",
    "        main_left_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_layout = QtWidgets.QVBoxLayout()\n",
    "        video_controls_layout = QtWidgets.QHBoxLayout()\n",
    "\n",
    "        # Create a layout to position the close button at the top-right corner\n",
    "        close_button = QtWidgets.QPushButton(\"X\")\n",
    "        close_button.setStyleSheet(\"background-color: red; color: white; font-size: 18px; border-radius: 10px;\")\n",
    "        close_button.setFixedSize(30, 30)\n",
    "        close_button.clicked.connect(self.stop_stream)\n",
    "\n",
    "        # Create a wrapper for the video feed and the button\n",
    "        video_feed_wrapper = QtWidgets.QWidget()\n",
    "        video_feed_layout = QtWidgets.QVBoxLayout(video_feed_wrapper)\n",
    "        video_feed_layout.setContentsMargins(0, 0, 0, 0)  # Remove margins for precise alignment\n",
    "\n",
    "        # Create a layout for the close button at the top-right corner\n",
    "        close_button_layout = QtWidgets.QHBoxLayout()\n",
    "        close_button_layout.addStretch(1)  # Add stretch to push the button to the right\n",
    "        close_button_layout.addWidget(close_button)\n",
    "\n",
    "        video_feed_layout.addLayout(close_button_layout)  # Add the close button layout\n",
    "        self.video_feed = QtWidgets.QLabel(\"Video Stream\")\n",
    "        self.video_feed.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.video_feed.setStyleSheet(\"background-color: white; color: black; border: 2px solid;\")\n",
    "        self.video_feed.setFixedSize(970, 480)  # Set a fixed size for the video feed\n",
    "        self.video_feed.setScaledContents(True)\n",
    "        self.video_feed.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)\n",
    "\n",
    "        video_feed_layout.addWidget(self.video_feed)  # Add the video feed below the button\n",
    "\n",
    "        # Add the video feed wrapper to the main video layout\n",
    "        video_layout.addWidget(video_feed_wrapper)\n",
    "\n",
    "        # Detection buttons\n",
    "        detection_buttons_layout = QtWidgets.QHBoxLayout()\n",
    "        self.record_btn = QtWidgets.QPushButton(\"Record\")\n",
    "        self.record_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.record_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.record_btn.clicked.connect(self.toggle_record)\n",
    "        detection_buttons_layout.addWidget(self.record_btn)\n",
    "\n",
    "       # Assigning person_detection_btn as an instance variable\n",
    "        self.person_detection_btn = QtWidgets.QPushButton(\"Person Detection\")\n",
    "        self.person_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.person_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.person_detection_btn.clicked.connect(self.toggle_person_detection)\n",
    "        detection_buttons_layout.addWidget(self.person_detection_btn)\n",
    "\n",
    "        self.vehicle_detection_btn = QtWidgets.QPushButton(\"Vehicle Detection\")\n",
    "        self.vehicle_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.vehicle_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.vehicle_detection_btn.clicked.connect(self.toggle_vehicle_detection)\n",
    "        detection_buttons_layout.addWidget(self.vehicle_detection_btn)\n",
    "\n",
    "        video_layout.addLayout(detection_buttons_layout)\n",
    "        main_left_layout.addLayout(video_layout)\n",
    "\n",
    "        # Alerts section\n",
    "        alerts_layout = QtWidgets.QHBoxLayout()\n",
    "        self.alert_label = QtWidgets.QLabel(\"Alert: No Detection\")\n",
    "        self.alert_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.alert_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.alert_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.alert_label)\n",
    "\n",
    "        self.duration_label = QtWidgets.QLabel(\"Time Duration: 00:00:00\")\n",
    "        self.duration_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.duration_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.duration_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.duration_label)\n",
    "\n",
    "        main_left_layout.addLayout(alerts_layout)\n",
    "        content_layout.addLayout(main_left_layout)\n",
    "\n",
    "        main_layout.addLayout(content_layout)\n",
    "        self.setStyleSheet(\"background-color: #222222; color: white;\")\n",
    "\n",
    "        self.setLayout(main_layout)\n",
    "\n",
    "    def toggle_record(self):\n",
    "        if not self.record_flag:\n",
    "            self.record_flag = True\n",
    "            self.record_btn.setText(\"Stop Record\")\n",
    "            self.record_options()\n",
    "        else:\n",
    "            self.record_flag = False\n",
    "            self.record_btn.setText(\"Start Record\")\n",
    "\n",
    "    def record_options(self):\n",
    "        msg_box = QMessageBox()\n",
    "        msg_box.setWindowTitle(\"Choose Recording Type\")\n",
    "        msg_box.setText(\"Do you want to record the full video or use optimal storage?\")\n",
    "        full_button = msg_box.addButton(\"Full Video\", QMessageBox.YesRole)\n",
    "        optimal_button = msg_box.addButton(\"Optimal Storage\", QMessageBox.NoRole)\n",
    "        msg_box.exec_()\n",
    "\n",
    "        if msg_box.clickedButton() == full_button:\n",
    "            self.record_and_save_video(optimal=False)\n",
    "        elif msg_box.clickedButton() == optimal_button:\n",
    "            self.record_and_save_video(optimal=True)\n",
    "\n",
    "    def record_and_save_video(self, optimal):\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        output_file = f\"{now}.avi\"\n",
    "        self.video_display(output_file, optimal)\n",
    "        save_video_to_db(output_file, now)\n",
    "\n",
    "        QMessageBox.information(self, \"Recording Complete\", \"Video recorded and saved to database successfully.\")\n",
    "\n",
    "    def video_display(self, output_path=None, optimal=False):\n",
    "        self.optimal = optimal\n",
    "        if self.real_time_stream:\n",
    "            self.video_capture = cv2.VideoCapture(0)\n",
    "        elif self.video1:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\853889-hd_1920_1080_25fps.mp4\")\n",
    "        elif self.video2:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\2273134-hd_1280_720_30fps.mp4\")\n",
    "        elif self.video3:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\12494476_3840_2160_50fps.mp4\")\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        self.out = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))\n",
    "\n",
    "        ret, prev_frame = self.video_capture.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to access the camera\")\n",
    "            return\n",
    "\n",
    "        self.prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) if optimal else None\n",
    "\n",
    "        # Connect the update_frame method properly without calling it immediately\n",
    "        self.timer.timeout.connect(lambda: self.update_frame(optimal))\n",
    "        self.timer.start(20)\n",
    "\n",
    "    def update_frame(self, optimal=False):\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if ret:\n",
    "            if optimal:\n",
    "                # Convert current frame to grayscale for comparison\n",
    "                curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                # Compute the absolute difference between current and previous frames\n",
    "                diff = cv2.absdiff(self.prev_gray, curr_gray)\n",
    "                _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "                non_zero_count = cv2.countNonZero(thresh)\n",
    "\n",
    "                # Save the frame only if there is significant change\n",
    "                if non_zero_count > 10000:\n",
    "                    self.out.write(frame)\n",
    "                    self.prev_gray = curr_gray  # Update the previous frame\n",
    "            else:\n",
    "                # Full video recording, save every frame\n",
    "                self.out.write(frame)\n",
    "\n",
    "            # Convert the frame to RGB for display\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if self.detecting_person:\n",
    "                frame = self.detect_person(frame)\n",
    "            if self.detecting_vehicle:\n",
    "                frame = self.detect_vehicle(frame)\n",
    "\n",
    "            # Get the frame dimensions and convert to QImage\n",
    "            h, w, ch = frame.shape\n",
    "            bytes_per_line = ch * w\n",
    "            qimg = QImage(frame.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "\n",
    "            self.video_feed.setPixmap(QPixmap.fromImage(qimg))\n",
    "        else:\n",
    "            print(\"Error: Unable to read video frame\")\n",
    "\n",
    "    def stop_stream(self):\n",
    "        print(\"Stopping video stream\")\n",
    "        if self.video_capture is not None:\n",
    "            self.video_capture.release()\n",
    "            self.out.release()\n",
    "            self.video_capture = None\n",
    "            self.video_feed.clear()\n",
    "            self.timer.stop()\n",
    "\n",
    "    def toggle_person_detection(self):\n",
    "        self.detecting_person = not self.detecting_person  # Toggle the state\n",
    "        if self.detecting_person:\n",
    "            self.person_detection_btn.setText(\"Stop Person Detection\")\n",
    "        else:\n",
    "            self.person_detection_btn.setText(\"Start Person Detection\")\n",
    "            self.person_detected_time = None  # Reset timer when detection stops\n",
    "            self.alert_generated = False\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "    def detect_person(self, frame):\n",
    "        # Perform inference\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        person_detected = False\n",
    "        for *box, conf, cls in predictions:\n",
    "            if int(cls) == 0:  # Class index for person\n",
    "                person_detected = True\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "                cv2.putText(frame, f'Person {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        if person_detected:\n",
    "            self.update_person_detection_time()\n",
    "        else:\n",
    "            self.reset_person_detection()\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def update_person_detection_time(self):\n",
    "        \"\"\"Track how long a person has been in the frame and update alert if needed.\"\"\"\n",
    "        if self.person_detected_time is None:\n",
    "            self.person_detected_time = time.time()  # Start the timer\n",
    "\n",
    "        duration = time.time() - self.person_detected_time\n",
    "\n",
    "        # Format the duration\n",
    "        hours, rem = divmod(duration, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        duration_str = \"{:02}:{:02}:{:02}\".format(int(hours), int(minutes), int(seconds))\n",
    "\n",
    "        # Update duration label\n",
    "        self.duration_label.setText(f\"Time Duration: {duration_str}\")\n",
    "\n",
    "        # Check if detection duration exceeds the threshold (e.g., 5 minutes or 300 seconds)\n",
    "        if duration > 10 and not self.alert_generated:  # 300 seconds = 5 minutes\n",
    "            self.alert_label.setText(\"ALERT: Person detected for over 5 minutes!\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.alert_generated = True\n",
    "\n",
    "    def reset_person_detection(self):\n",
    "        \"\"\"Reset the detection if no person is found.\"\"\"\n",
    "        if self.person_detected_time is not None:\n",
    "            self.person_detected_time = None  # Reset the timer\n",
    "            self.duration_label.setText(\"Time Duration: 00:00:00\")\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.alert_generated = False\n",
    "\n",
    "            \n",
    "    def detect_vehicle(self, frame):\n",
    "        # Perform inference using YOLOv5 model\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        current_time = time.time()  # Get the current time\n",
    "        vehicle_detected = False  # Flag to check if any vehicle is detected\n",
    "\n",
    "        # Dictionary to map class ids to vehicle types\n",
    "        vehicle_class_map = {\n",
    "            2: \"Car\",        # Car\n",
    "            3: \"Motorcycle\", # Motorcycle\n",
    "            5: \"Bus\",       # Bus\n",
    "            7: \"Truck\"      # Truck\n",
    "        }\n",
    "\n",
    "        for *box, conf, cls in predictions:\n",
    "            print(f\"Class detected: {cls}\")\n",
    "            cls = int(cls)  # Convert cls to an integer for checking\n",
    "\n",
    "            if cls in vehicle_class_map:  # Only process valid vehicle classes\n",
    "                vehicle_type = vehicle_class_map[cls]\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # Calculate the center of the bounding box\n",
    "                vehicle_center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "                vehicle_detected = True  # Set the flag to True since we detected a vehicle\n",
    "\n",
    "                # Use the center position to track vehicle movement\n",
    "                if vehicle_type not in self.vehicle_positions:\n",
    "                    # If this is a new vehicle, start tracking it\n",
    "                    self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                    self.stationary_alert_generated[vehicle_type] = False\n",
    "                else:\n",
    "                    # Get the previous position and detection start time\n",
    "                    prev_position, start_time = self.vehicle_positions[vehicle_type]\n",
    "\n",
    "                    # Calculate movement distance between current and previous position\n",
    "                    movement_distance = np.linalg.norm(np.array(vehicle_center) - np.array(prev_position))\n",
    "\n",
    "                    # Define a movement threshold (e.g., 20 pixels)\n",
    "                    movement_threshold = 20\n",
    "\n",
    "                    if movement_distance < movement_threshold:\n",
    "                        # If the vehicle has not moved significantly, check how long it's been stationary\n",
    "                        stationary_duration = current_time - start_time\n",
    "\n",
    "                        # Update the time duration label\n",
    "                        self.duration_label.setText(f\"Vehicle {vehicle_type} stationary for {int(stationary_duration)} seconds\")\n",
    "\n",
    "                        # Check if the stationary duration exceeds 5 minutes (300 seconds)\n",
    "                        if stationary_duration > 5 and not self.stationary_alert_generated[vehicle_type]:\n",
    "                            self.alert_label.setText(f\"ALERT: {vehicle_type} stationary for over 5 minutes!\")\n",
    "                            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.stationary_alert_generated[vehicle_type] = True  # Mark that the alert has been generated\n",
    "                    else:\n",
    "                        # If the vehicle moved, reset the detection start time\n",
    "                        self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                        self.stationary_alert_generated[vehicle_type] = False\n",
    "\n",
    "                # Draw the bounding box and label the vehicle\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'{vehicle_type} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # If no vehicle is detected, reset the alert and duration labels\n",
    "        if not vehicle_detected:\n",
    "            self.alert_label.setText(\"No vehicle detected\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setText(\"No detection duration: 00 seconds\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "        return frame\n",
    "    \n",
    "    def toggle_vehicle_detection(self):\n",
    "        \"\"\"Toggle vehicle detection on and off.\"\"\"\n",
    "        self.detecting_vehicle = not self.detecting_vehicle  # Toggle the state\n",
    "        if self.detecting_vehicle:\n",
    "            self.vehicle_detection_btn.setText(\"Stop vehicle Detection\")\n",
    "        else:\n",
    "            self.vehicle_detection_btn.setText(\"Start vehicle Detection\")\n",
    "\n",
    "    def create_menu(self):\n",
    "        menu = QtWidgets.QMenu(self)\n",
    "        menu.addAction(\"Option 1\", self.option1_action)\n",
    "        menu.addAction(\"Option 2\", self.option2_action)\n",
    "        return menu\n",
    "\n",
    "    def start_real_time_stream(self):\n",
    "        print(\"Starting real-time video stream\")\n",
    "        self.real_time_stream=True\n",
    "        # Start video capture (assuming you have a webcam at index 0)\n",
    "        self.video_display()  # Call video_feed to initiate video capture\n",
    "            \n",
    "   # Define the three separate play video functions\n",
    "    def play_video_1(self):\n",
    "        print(\"Playing Video 1\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = True  # Set instance variable\n",
    "        self.video2 = False  # Reset other video flags\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_2(self):\n",
    "        print(\"Playing Video 2\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = True\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_3(self):\n",
    "        print(\"Playing Video 3\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = False\n",
    "        self.video3 = True\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    # Add your logic to play video 3 here\n",
    "    def show_analytics(self):\n",
    "        print(\"Showing analytics\")\n",
    "\n",
    "    def option1_action(self):\n",
    "        print(\"Option 1 selected\")\n",
    "\n",
    "    def option2_action(self):\n",
    "        print(\"Option 2 selected\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    window = VizOptilyticsApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7512be2e",
   "metadata": {},
   "source": [
    "# VIDEO HAS BEEN SAVED NOW BUT THREIS SOME OTHER ISSUE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c43130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time video stream\n",
      "Video data size: 2260356 bytes\n",
      "2024-10-28_14-58-10 : 2260356 bytes\n",
      "Inserted 1 row(s) into the database.\n",
      "Video saved to database and deleted from the computer successfully.\n",
      "Stopping video stream\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import sys\n",
    "import cv2\n",
    "from PyQt5.QtCore import QTimer\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QMessageBox\n",
    "import os\n",
    "\n",
    "\n",
    "# Database connection\n",
    "def connect_db():\n",
    "    return mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"\",  # Replace with your password\n",
    "        database=\"testing_database\"  # Replace with your database name\n",
    "    )\n",
    "\n",
    "# Save video to database with name\n",
    "def save_video_to_db(video_path, video_name):\n",
    "    try:\n",
    "        conn = connect_db()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        with open(video_path, 'rb') as f:\n",
    "            video_data = f.read()\n",
    "        print(f\"Video data size: {len(video_data)} bytes\")\n",
    "\n",
    "        query = \"INSERT INTO videos (video_name, video_file) VALUES (%s, %s)\"\n",
    "        cursor.execute(query, (video_name, video_data))\n",
    "        conn.commit()\n",
    "        print(f\"{video_name} : {len(video_data)} bytes\")  # Fixed syntax error\n",
    "        print(f\"Inserted {cursor.rowcount} row(s) into the database.\")  # Debug print\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "#         os.remove(video_path)  # Delete the file from the local system\n",
    "        print(\"Video saved to database and deleted from the computer successfully.\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The video file was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "        \n",
    "class VizOptilyticsApp(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Viz Optilytics\")\n",
    "        self.setGeometry(0, 0, 1200, 700)\n",
    "\n",
    "        # Initialize QTimer (fixing the timer error)\n",
    "        self.timer = QTimer(self)  # Initialize the QTimer object\n",
    "\n",
    "        # Initialize video capture object\n",
    "        self.video_capture = None\n",
    "\n",
    "        # Detection states\n",
    "        self.detecting_person = False\n",
    "        self.detecting_vehicle = False\n",
    "\n",
    "         # Track detected persons and duration\n",
    "        self.person_detected_time = None  # To track when a person is first detected\n",
    "        self.alert_generated = False  # To track if alert is already generated\n",
    "        \n",
    "        # Load YOLOv5 model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 model\n",
    "\n",
    "         # Initialize alert labels for dynamic updates\n",
    "        self.alert_label = None\n",
    "        self.duration_label = None\n",
    "        \n",
    "        self.detection_grace_period = 10  # Number of frames to maintain detection box\n",
    "        self.frames_since_last_detection = 0\n",
    "        self.last_detection_box = None  # Store the last detected bounding box\n",
    "\n",
    "        \n",
    "        self.vehicle_positions = {}  # Stores vehicle ID -> (last position, detection start time)\n",
    "        self.stationary_alert_generated = {}  # To track if stationary alert has been generated\n",
    "\n",
    "        self.video1 = False\n",
    "        self.video2 = False\n",
    "        self.video3 = False\n",
    "        self.real_time_stream = False\n",
    "        self.record_flag = False\n",
    "\n",
    "        self.initUI()\n",
    "    def initUI(self):\n",
    "        # Main Layout\n",
    "        main_layout = QtWidgets.QVBoxLayout(self)\n",
    "        main_layout.setContentsMargins(50, 50, 50, 30)\n",
    "\n",
    "        # Top Section: Title and Menu\n",
    "        top_bar = QtWidgets.QHBoxLayout()\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        title = QtWidgets.QLabel(\"Viz Optilytics\")\n",
    "        title.setFont(QtGui.QFont(\"Arial\", 28, QtGui.QFont.Bold))\n",
    "        title.setStyleSheet(\"color: #FFD700;\")\n",
    "        title.setAlignment(QtCore.Qt.AlignCenter)\n",
    "\n",
    "        top_bar.addWidget(title)\n",
    "        top_bar.addStretch(1)\n",
    "\n",
    "        menu_button = QtWidgets.QPushButton(\"â˜°\")\n",
    "        menu_button.setStyleSheet(\"background-color: #333333; color: #FFD700; font-size: 20px; border-radius: 10px;\")\n",
    "        menu_button.setFixedSize(50, 50)\n",
    "        menu_button.setMenu(self.create_menu())\n",
    "\n",
    "        top_bar.addWidget(menu_button)\n",
    "        main_layout.addLayout(top_bar)\n",
    "\n",
    "        title_spacer = QtWidgets.QSpacerItem(20, 40, QtWidgets.QSizePolicy.Minimum, QtWidgets.QSizePolicy.Fixed)\n",
    "        main_layout.addItem(title_spacer)\n",
    "\n",
    "        # Horizontal layout split: Videos, Buttons, Alerts, etc.\n",
    "        content_layout = QtWidgets.QHBoxLayout()\n",
    "        content_layout.setSpacing(20)\n",
    "\n",
    "        # Video Buttons and Info Section\n",
    "        main_vertical_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_list_layout = QtWidgets.QVBoxLayout()\n",
    "        recorded_videos_label = QtWidgets.QLabel(\"Recorded Videos\")\n",
    "        recorded_videos_label.setFont(QtGui.QFont(\"Arial\", 16, QtGui.QFont.Bold))\n",
    "        recorded_videos_label.setStyleSheet(\"color: white;\")\n",
    "        video_list_layout.addWidget(recorded_videos_label)\n",
    "\n",
    "        # Video buttons\n",
    "        for i in range(3):\n",
    "            video_btn = QtWidgets.QPushButton(f\"Video {i + 1}\")\n",
    "            video_btn.setStyleSheet(\"background-color: #555555; color: white; border-radius: 8px; padding: 8px;\")\n",
    "            video_btn.setFixedWidth(200)\n",
    "            video_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "\n",
    "            # Connect each button to a separate function based on the index\n",
    "            if i == 0:\n",
    "                video_btn.clicked.connect(self.play_video_1)\n",
    "            elif i == 1:\n",
    "                video_btn.clicked.connect(self.play_video_2)\n",
    "            elif i == 2:\n",
    "                video_btn.clicked.connect(self.play_video_3)\n",
    "\n",
    "            video_list_layout.addWidget(video_btn)\n",
    "\n",
    "        main_vertical_layout.addLayout(video_list_layout)\n",
    "\n",
    "        detection_info_widget = QtWidgets.QWidget()\n",
    "        detection_info_layout = QtWidgets.QVBoxLayout(detection_info_widget)\n",
    "        detection_info_layout.setSpacing(0)\n",
    "        detection_info_widget.setStyleSheet(\"background-color: #444444; border-radius: 10px;\")\n",
    "\n",
    "        date_time_label = QtWidgets.QLabel(\"Date: 2024-10-11\\nTime: 12:45 PM\")\n",
    "        date_time_label.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(date_time_label)\n",
    "\n",
    "        detection_status = QtWidgets.QLabel(\"Status: Vehicle Detected\")\n",
    "        detection_status.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(detection_status)\n",
    "        main_vertical_layout.addWidget(detection_info_widget)\n",
    "\n",
    "        # Real-time streaming button\n",
    "        real_time_btn = QtWidgets.QPushButton(\"Real-Time Streaming\")\n",
    "        real_time_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        real_time_btn.setFixedWidth(200)\n",
    "        real_time_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        real_time_btn.clicked.connect(self.start_real_time_stream)\n",
    "        main_vertical_layout.addWidget(real_time_btn)\n",
    "\n",
    "        # Analytics button\n",
    "        analytics_btn = QtWidgets.QPushButton(\"Analytics\")\n",
    "        analytics_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        analytics_btn.setFixedWidth(200)\n",
    "        analytics_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        analytics_btn.clicked.connect(self.show_analytics)\n",
    "        main_vertical_layout.addWidget(analytics_btn)\n",
    "\n",
    "        content_layout.addLayout(main_vertical_layout)\n",
    "\n",
    "        # Video display area and other buttons\n",
    "        main_left_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_layout = QtWidgets.QVBoxLayout()\n",
    "        video_controls_layout = QtWidgets.QHBoxLayout()\n",
    "\n",
    "        # Create a layout to position the close button at the top-right corner\n",
    "        close_button = QtWidgets.QPushButton(\"X\")\n",
    "        close_button.setStyleSheet(\"background-color: red; color: white; font-size: 18px; border-radius: 10px;\")\n",
    "        close_button.setFixedSize(30, 30)\n",
    "        close_button.clicked.connect(self.stop_stream)\n",
    "\n",
    "        # Create a wrapper for the video feed and the button\n",
    "        video_feed_wrapper = QtWidgets.QWidget()\n",
    "        video_feed_layout = QtWidgets.QVBoxLayout(video_feed_wrapper)\n",
    "        video_feed_layout.setContentsMargins(0, 0, 0, 0)  # Remove margins for precise alignment\n",
    "\n",
    "        # Create a layout for the close button at the top-right corner\n",
    "        close_button_layout = QtWidgets.QHBoxLayout()\n",
    "        close_button_layout.addStretch(1)  # Add stretch to push the button to the right\n",
    "        close_button_layout.addWidget(close_button)\n",
    "\n",
    "        video_feed_layout.addLayout(close_button_layout)  # Add the close button layout\n",
    "        self.video_feed = QtWidgets.QLabel(\"Video Stream\")\n",
    "        self.video_feed.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.video_feed.setStyleSheet(\"background-color: white; color: black; border: 2px solid;\")\n",
    "        self.video_feed.setFixedSize(970, 480)  # Set a fixed size for the video feed\n",
    "        self.video_feed.setScaledContents(True)\n",
    "        self.video_feed.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)\n",
    "\n",
    "        video_feed_layout.addWidget(self.video_feed)  # Add the video feed below the button\n",
    "\n",
    "        # Add the video feed wrapper to the main video layout\n",
    "        video_layout.addWidget(video_feed_wrapper)\n",
    "\n",
    "        # Detection buttons\n",
    "        detection_buttons_layout = QtWidgets.QHBoxLayout()\n",
    "        self.record_btn = QtWidgets.QPushButton(\"Start Record\")\n",
    "        self.record_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.record_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.record_btn.clicked.connect(self.toggle_record)\n",
    "        detection_buttons_layout.addWidget(self.record_btn)\n",
    "\n",
    "       # Assigning person_detection_btn as an instance variable\n",
    "        self.person_detection_btn = QtWidgets.QPushButton(\"Person Detection\")\n",
    "        self.person_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.person_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.person_detection_btn.clicked.connect(self.toggle_person_detection)\n",
    "        detection_buttons_layout.addWidget(self.person_detection_btn)\n",
    "\n",
    "        self.vehicle_detection_btn = QtWidgets.QPushButton(\"Vehicle Detection\")\n",
    "        self.vehicle_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.vehicle_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.vehicle_detection_btn.clicked.connect(self.toggle_vehicle_detection)\n",
    "        detection_buttons_layout.addWidget(self.vehicle_detection_btn)\n",
    "\n",
    "        video_layout.addLayout(detection_buttons_layout)\n",
    "        main_left_layout.addLayout(video_layout)\n",
    "\n",
    "        # Alerts section\n",
    "        alerts_layout = QtWidgets.QHBoxLayout()\n",
    "        self.alert_label = QtWidgets.QLabel(\"Alert: No Detection\")\n",
    "        self.alert_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.alert_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.alert_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.alert_label)\n",
    "\n",
    "        self.duration_label = QtWidgets.QLabel(\"Time Duration: 00:00:00\")\n",
    "        self.duration_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.duration_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.duration_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.duration_label)\n",
    "\n",
    "        main_left_layout.addLayout(alerts_layout)\n",
    "        content_layout.addLayout(main_left_layout)\n",
    "\n",
    "        main_layout.addLayout(content_layout)\n",
    "        self.setStyleSheet(\"background-color: #222222; color: white;\")\n",
    "\n",
    "        self.setLayout(main_layout)\n",
    "\n",
    "    def toggle_record(self):\n",
    "        if not self.record_flag:\n",
    "            self.record_flag = True\n",
    "            self.record_btn.setText(\"Stop Record\")\n",
    "            self.record_options()\n",
    "        else:\n",
    "            self.record_flag = False\n",
    "            self.record_btn.setText(\"Start Record\")\n",
    "\n",
    "    def record_options(self):\n",
    "        msg_box = QMessageBox()\n",
    "        msg_box.setWindowTitle(\"Choose Recording Type\")\n",
    "        msg_box.setText(\"Do you want to record the full video or use optimal storage?\")\n",
    "        full_button = msg_box.addButton(\"Full Video\", QMessageBox.YesRole)\n",
    "        optimal_button = msg_box.addButton(\"Optimal Storage\", QMessageBox.NoRole)\n",
    "        msg_box.exec_()\n",
    "\n",
    "        if msg_box.clickedButton() == full_button:\n",
    "            self.record_and_save_video(optimal=False)\n",
    "        elif msg_box.clickedButton() == optimal_button:\n",
    "            self.record_and_save_video(optimal=True)\n",
    "\n",
    "    def record_and_save_video(self, optimal):\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        output_file = f\"{now}.avi\"\n",
    "        self.video_display(output_file, optimal)\n",
    "#         save_video_to_db(output_file, now)\n",
    "\n",
    "#         QMessageBox.information(self, \"Recording Complete\", \"Video recorded and saved to database successfully.\")\n",
    "        # QMessageBox to notify that the video recording has begun\n",
    "        QMessageBox.information(self, \"Recording Started\", \"Video recording in progress...\")\n",
    "\n",
    "        # After recording stops, save the video to the database\n",
    "        self.timer.timeout.connect(lambda: self.check_recording_status(output_file, now))\n",
    "\n",
    "    def check_recording_status(self, output_file, video_name):\n",
    "        if not self.record_flag:\n",
    "            self.timer.stop()  # Stop the timer when recording is done\n",
    "            \n",
    "            # Release video writer and save to database\n",
    "            if hasattr(self, 'out') and self.out is not None:\n",
    "                self.out.release()\n",
    "            # Save video to the database\n",
    "            save_video_to_db(output_file, video_name)\n",
    "\n",
    "            QMessageBox.information(self, \"Recording Complete\", \"Video recorded and saved to database successfully.\")\n",
    "\n",
    "    def video_display(self, output_path=None, optimal=False):\n",
    "#         self.optimal = optimal\n",
    "        if self.real_time_stream:\n",
    "            self.video_capture = cv2.VideoCapture(0)\n",
    "        elif self.video1:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\853889-hd_1920_1080_25fps.mp4\")\n",
    "        elif self.video2:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\2273134-hd_1280_720_30fps.mp4\")\n",
    "        elif self.video3:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\12494476_3840_2160_50fps.mp4\")\n",
    "\n",
    "        if self.record_flag:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            self.out = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))\n",
    "\n",
    "        ret, prev_frame = self.video_capture.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to access the camera\")\n",
    "            return\n",
    "\n",
    "        if self.record_flag:\n",
    "            self.prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) if optimal else None\n",
    "\n",
    "        # Connect the update_frame method properly without calling it immediately\n",
    "        self.timer.timeout.connect(lambda: self.update_frame(optimal))\n",
    "        self.timer.start(20)\n",
    "\n",
    "    def update_frame(self, optimal=False):\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if ret:\n",
    "            if self.record_flag and hasattr(self, 'out') and self.out is not None:\n",
    "                if optimal:\n",
    "                    # Convert current frame to grayscale for comparison\n",
    "                    curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                    # Compute the absolute difference between current and previous frames\n",
    "                    diff = cv2.absdiff(self.prev_gray, curr_gray)\n",
    "                    _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "                    non_zero_count = cv2.countNonZero(thresh)\n",
    "\n",
    "                    # Save the frame only if there is significant change\n",
    "                    if non_zero_count > 10000:\n",
    "                        self.out.write(frame)\n",
    "                        self.prev_gray = curr_gray  # Update the previous frame\n",
    "                else:\n",
    "                    # Full video recording, save every frame\n",
    "                    self.out.write(frame)\n",
    "\n",
    "            # Convert the frame to RGB for display\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if self.detecting_person:\n",
    "                frame = self.detect_person(frame)\n",
    "            if self.detecting_vehicle:\n",
    "                frame = self.detect_vehicle(frame)\n",
    "\n",
    "            # Get the frame dimensions and convert to QImage\n",
    "            h, w, ch = frame.shape\n",
    "            bytes_per_line = ch * w\n",
    "            qimg = QImage(frame.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "\n",
    "            self.video_feed.setPixmap(QPixmap.fromImage(qimg))\n",
    "        else:\n",
    "            print(\"Error: Unable to read video frame\")\n",
    "\n",
    "    def stop_stream(self):\n",
    "        print(\"Stopping video stream\")\n",
    "        if self.video_capture is not None:\n",
    "            self.video_capture.release()\n",
    "            self.out.release()\n",
    "            self.video_capture = None\n",
    "            self.video_feed.clear()\n",
    "            self.timer.stop()\n",
    "\n",
    "    def toggle_person_detection(self):\n",
    "        self.detecting_person = not self.detecting_person  # Toggle the state\n",
    "        if self.detecting_person:\n",
    "            self.person_detection_btn.setText(\"Stop Person Detection\")\n",
    "        else:\n",
    "            self.person_detection_btn.setText(\"Start Person Detection\")\n",
    "            self.person_detected_time = None  # Reset timer when detection stops\n",
    "            self.alert_generated = False\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "    def detect_person(self, frame):\n",
    "        # Perform inference\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        person_detected = False\n",
    "        for *box, conf, cls in predictions:\n",
    "            if int(cls) == 0:  # Class index for person\n",
    "                person_detected = True\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "                cv2.putText(frame, f'Person {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        if person_detected:\n",
    "            self.update_person_detection_time()\n",
    "        else:\n",
    "            self.reset_person_detection()\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def update_person_detection_time(self):\n",
    "        \"\"\"Track how long a person has been in the frame and update alert if needed.\"\"\"\n",
    "        if self.person_detected_time is None:\n",
    "            self.person_detected_time = time.time()  # Start the timer\n",
    "\n",
    "        duration = time.time() - self.person_detected_time\n",
    "\n",
    "        # Format the duration\n",
    "        hours, rem = divmod(duration, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        duration_str = \"{:02}:{:02}:{:02}\".format(int(hours), int(minutes), int(seconds))\n",
    "\n",
    "        # Update duration label\n",
    "        self.duration_label.setText(f\"Time Duration: {duration_str}\")\n",
    "\n",
    "        # Check if detection duration exceeds the threshold (e.g., 5 minutes or 300 seconds)\n",
    "        if duration > 10 and not self.alert_generated:  # 300 seconds = 5 minutes\n",
    "            self.alert_label.setText(\"ALERT: Person detected for over 5 minutes!\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.alert_generated = True\n",
    "\n",
    "    def reset_person_detection(self):\n",
    "        \"\"\"Reset the detection if no person is found.\"\"\"\n",
    "        if self.person_detected_time is not None:\n",
    "            self.person_detected_time = None  # Reset the timer\n",
    "            self.duration_label.setText(\"Time Duration: 00:00:00\")\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.alert_generated = False\n",
    "\n",
    "            \n",
    "    def detect_vehicle(self, frame):\n",
    "        # Perform inference using YOLOv5 model\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        current_time = time.time()  # Get the current time\n",
    "        vehicle_detected = False  # Flag to check if any vehicle is detected\n",
    "\n",
    "        # Dictionary to map class ids to vehicle types\n",
    "        vehicle_class_map = {\n",
    "            2: \"Car\",        # Car\n",
    "            3: \"Motorcycle\", # Motorcycle\n",
    "            5: \"Bus\",       # Bus\n",
    "            7: \"Truck\"      # Truck\n",
    "        }\n",
    "\n",
    "        for *box, conf, cls in predictions:\n",
    "            print(f\"Class detected: {cls}\")\n",
    "            cls = int(cls)  # Convert cls to an integer for checking\n",
    "\n",
    "            if cls in vehicle_class_map:  # Only process valid vehicle classes\n",
    "                vehicle_type = vehicle_class_map[cls]\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # Calculate the center of the bounding box\n",
    "                vehicle_center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "                vehicle_detected = True  # Set the flag to True since we detected a vehicle\n",
    "\n",
    "                # Use the center position to track vehicle movement\n",
    "                if vehicle_type not in self.vehicle_positions:\n",
    "                    # If this is a new vehicle, start tracking it\n",
    "                    self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                    self.stationary_alert_generated[vehicle_type] = False\n",
    "                else:\n",
    "                    # Get the previous position and detection start time\n",
    "                    prev_position, start_time = self.vehicle_positions[vehicle_type]\n",
    "\n",
    "                    # Calculate movement distance between current and previous position\n",
    "                    movement_distance = np.linalg.norm(np.array(vehicle_center) - np.array(prev_position))\n",
    "\n",
    "                    # Define a movement threshold (e.g., 20 pixels)\n",
    "                    movement_threshold = 20\n",
    "\n",
    "                    if movement_distance < movement_threshold:\n",
    "                        # If the vehicle has not moved significantly, check how long it's been stationary\n",
    "                        stationary_duration = current_time - start_time\n",
    "\n",
    "                        # Update the time duration label\n",
    "                        self.duration_label.setText(f\"Vehicle {vehicle_type} stationary for {int(stationary_duration)} seconds\")\n",
    "\n",
    "                        # Check if the stationary duration exceeds 5 minutes (300 seconds)\n",
    "                        if stationary_duration > 5 and not self.stationary_alert_generated[vehicle_type]:\n",
    "                            self.alert_label.setText(f\"ALERT: {vehicle_type} stationary for over 5 minutes!\")\n",
    "                            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.stationary_alert_generated[vehicle_type] = True  # Mark that the alert has been generated\n",
    "                    else:\n",
    "                        # If the vehicle moved, reset the detection start time\n",
    "                        self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                        self.stationary_alert_generated[vehicle_type] = False\n",
    "\n",
    "                # Draw the bounding box and label the vehicle\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'{vehicle_type} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # If no vehicle is detected, reset the alert and duration labels\n",
    "        if not vehicle_detected:\n",
    "            self.alert_label.setText(\"No vehicle detected\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setText(\"No detection duration: 00 seconds\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "        return frame\n",
    "    \n",
    "    def toggle_vehicle_detection(self):\n",
    "        \"\"\"Toggle vehicle detection on and off.\"\"\"\n",
    "        self.detecting_vehicle = not self.detecting_vehicle  # Toggle the state\n",
    "        if self.detecting_vehicle:\n",
    "            self.vehicle_detection_btn.setText(\"Stop vehicle Detection\")\n",
    "        else:\n",
    "            self.vehicle_detection_btn.setText(\"Start vehicle Detection\")\n",
    "\n",
    "    def create_menu(self):\n",
    "        menu = QtWidgets.QMenu(self)\n",
    "        menu.addAction(\"Option 1\", self.option1_action)\n",
    "        menu.addAction(\"Option 2\", self.option2_action)\n",
    "        return menu\n",
    "\n",
    "    def start_real_time_stream(self):\n",
    "        print(\"Starting real-time video stream\")\n",
    "        self.real_time_stream=True\n",
    "        # Start video capture (assuming you have a webcam at index 0)\n",
    "        self.video_display()  # Call video_feed to initiate video capture\n",
    "            \n",
    "   # Define the three separate play video functions\n",
    "    def play_video_1(self):\n",
    "        print(\"Playing Video 1\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = True  # Set instance variable\n",
    "        self.video2 = False  # Reset other video flags\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_2(self):\n",
    "        print(\"Playing Video 2\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = True\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_3(self):\n",
    "        print(\"Playing Video 3\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = False\n",
    "        self.video3 = True\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    # Add your logic to play video 3 here\n",
    "    def show_analytics(self):\n",
    "        print(\"Showing analytics\")\n",
    "\n",
    "    def option1_action(self):\n",
    "        print(\"Option 1 selected\")\n",
    "\n",
    "    def option2_action(self):\n",
    "        print(\"Option 2 selected\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    window = VizOptilyticsApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9c170",
   "metadata": {},
   "source": [
    "# MAJOR CODE I AM WORKING ON ONLY VISUALIZATION LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ed80ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing Video 1\n",
      "Stopping video stream\n",
      "The number of head count is 39\n",
      "The time of detection is 2024-11-04 12:48:26\n",
      "Playing Video 3\n",
      "Stopping video stream\n",
      "Bounding Box: [3, 951, 1010, 1746], Type: <class 'list'>\n",
      "Bounding Box: [1222, 1142, 2310, 1677], Type: <class 'list'>\n",
      "Bounding Box: [1903, 1207, 2173, 1697], Type: <class 'list'>\n",
      "Bounding Box: [1, 984, 1011, 1754], Type: <class 'list'>\n",
      "Bounding Box: [2954, 1332, 3131, 1564], Type: <class 'list'>\n",
      "Bounding Box: [2092, 1191, 2235, 1317], Type: <class 'list'>\n",
      "Bounding Box: [2742, 1069, 3146, 1563], Type: <class 'list'>\n",
      "Bounding Box: [1244, 1134, 2294, 1671], Type: <class 'list'>\n",
      "Bounding Box: [1925, 1204, 2218, 1698], Type: <class 'list'>\n",
      "Bounding Box: [3, 950, 1012, 1746], Type: <class 'list'>\n",
      "Bounding Box: [0, 985, 1011, 1753], Type: <class 'list'>\n",
      "Bounding Box: [2955, 1332, 3129, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2657, 1027, 2979, 1564], Type: <class 'list'>\n",
      "Bounding Box: [2092, 1194, 2232, 1311], Type: <class 'list'>\n",
      "Bounding Box: [1262, 1135, 2340, 1673], Type: <class 'list'>\n",
      "Bounding Box: [1943, 1197, 2219, 1695], Type: <class 'list'>\n",
      "Bounding Box: [3, 948, 1015, 1744], Type: <class 'list'>\n",
      "Bounding Box: [0, 985, 1013, 1752], Type: <class 'list'>\n",
      "Bounding Box: [2955, 1332, 3132, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2587, 1036, 3016, 1559], Type: <class 'list'>\n",
      "Bounding Box: [1964, 1198, 2242, 1686], Type: <class 'list'>\n",
      "Bounding Box: [1282, 1136, 2403, 1679], Type: <class 'list'>\n",
      "Bounding Box: [0, 983, 1013, 1751], Type: <class 'list'>\n",
      "Bounding Box: [3, 947, 1017, 1743], Type: <class 'list'>\n",
      "Bounding Box: [2956, 1327, 3133, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2588, 1025, 3054, 1559], Type: <class 'list'>\n",
      "Bounding Box: [1928, 1195, 2263, 1695], Type: <class 'list'>\n",
      "Bounding Box: [1308, 1138, 2398, 1690], Type: <class 'list'>\n",
      "Bounding Box: [0, 982, 1011, 1752], Type: <class 'list'>\n",
      "Bounding Box: [1989, 1207, 2268, 1686], Type: <class 'list'>\n",
      "Bounding Box: [2, 948, 1017, 1744], Type: <class 'list'>\n",
      "Bounding Box: [2958, 1325, 3134, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2707, 1054, 3146, 1563], Type: <class 'list'>\n",
      "Bounding Box: [1523, 1166, 2398, 1702], Type: <class 'list'>\n",
      "Bounding Box: [2008, 1203, 2304, 1689], Type: <class 'list'>\n",
      "Bounding Box: [1319, 1139, 2394, 1685], Type: <class 'list'>\n",
      "Bounding Box: [0, 983, 1010, 1752], Type: <class 'list'>\n",
      "Bounding Box: [5, 948, 1015, 1744], Type: <class 'list'>\n",
      "Bounding Box: [2958, 1327, 3133, 1562], Type: <class 'list'>\n",
      "Bounding Box: [2660, 1022, 3010, 1567], Type: <class 'list'>\n",
      "Bounding Box: [2283, 1207, 2373, 1353], Type: <class 'list'>\n",
      "Bounding Box: [1713, 1171, 2392, 1694], Type: <class 'list'>\n",
      "Bounding Box: [2033, 1199, 2324, 1676], Type: <class 'list'>\n",
      "Bounding Box: [0, 981, 1009, 1752], Type: <class 'list'>\n",
      "Bounding Box: [1328, 1140, 2369, 1681], Type: <class 'list'>\n",
      "Bounding Box: [3, 945, 1015, 1743], Type: <class 'list'>\n",
      "Bounding Box: [2958, 1330, 3131, 1562], Type: <class 'list'>\n",
      "Bounding Box: [2281, 1204, 2348, 1338], Type: <class 'list'>\n",
      "Bounding Box: [2651, 1022, 3018, 1566], Type: <class 'list'>\n",
      "Bounding Box: [1754, 1169, 2411, 1688], Type: <class 'list'>\n",
      "Bounding Box: [2047, 1191, 2341, 1668], Type: <class 'list'>\n",
      "Bounding Box: [1, 981, 1009, 1752], Type: <class 'list'>\n",
      "Bounding Box: [1341, 1140, 2383, 1679], Type: <class 'list'>\n",
      "Bounding Box: [4, 946, 1013, 1743], Type: <class 'list'>\n",
      "Bounding Box: [2957, 1327, 3132, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2281, 1205, 2349, 1329], Type: <class 'list'>\n",
      "Bounding Box: [2638, 1021, 3012, 1566], Type: <class 'list'>\n",
      "Bounding Box: [1333, 1141, 2530, 1689], Type: <class 'list'>\n",
      "Bounding Box: [1, 981, 1010, 1753], Type: <class 'list'>\n",
      "Bounding Box: [2086, 1205, 2399, 1657], Type: <class 'list'>\n",
      "Bounding Box: [2, 948, 1013, 1744], Type: <class 'list'>\n",
      "Bounding Box: [2281, 1208, 2347, 1324], Type: <class 'list'>\n",
      "Bounding Box: [2957, 1332, 3131, 1564], Type: <class 'list'>\n",
      "Bounding Box: [2613, 1018, 3048, 1568], Type: <class 'list'>\n",
      "Bounding Box: [1378, 1140, 2559, 1673], Type: <class 'list'>\n",
      "Bounding Box: [1, 981, 1010, 1753], Type: <class 'list'>\n",
      "Bounding Box: [2107, 1204, 2443, 1650], Type: <class 'list'>\n",
      "Bounding Box: [3, 947, 1014, 1745], Type: <class 'list'>\n",
      "Bounding Box: [2957, 1325, 3135, 1564], Type: <class 'list'>\n",
      "Bounding Box: [2107, 1215, 2535, 1647], Type: <class 'list'>\n",
      "Bounding Box: [2583, 1020, 3043, 1560], Type: <class 'list'>\n",
      "Bounding Box: [2278, 1209, 2347, 1319], Type: <class 'list'>\n",
      "Bounding Box: [1404, 1143, 2606, 1672], Type: <class 'list'>\n",
      "Bounding Box: [2125, 1201, 2451, 1648], Type: <class 'list'>\n",
      "Bounding Box: [1, 981, 1010, 1753], Type: <class 'list'>\n",
      "Bounding Box: [3, 947, 1014, 1745], Type: <class 'list'>\n",
      "Bounding Box: [2956, 1332, 3132, 1564], Type: <class 'list'>\n",
      "Bounding Box: [2591, 1018, 3036, 1561], Type: <class 'list'>\n",
      "Bounding Box: [1414, 1145, 2632, 1674], Type: <class 'list'>\n",
      "Bounding Box: [1, 981, 1011, 1753], Type: <class 'list'>\n",
      "Bounding Box: [2, 946, 1015, 1745], Type: <class 'list'>\n",
      "Bounding Box: [2149, 1192, 2457, 1640], Type: <class 'list'>\n",
      "Bounding Box: [2957, 1333, 3130, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2596, 1023, 3013, 1560], Type: <class 'list'>\n",
      "Bounding Box: [1434, 1143, 2641, 1664], Type: <class 'list'>\n",
      "Bounding Box: [1, 981, 1012, 1754], Type: <class 'list'>\n",
      "Bounding Box: [2187, 1200, 2540, 1626], Type: <class 'list'>\n",
      "Bounding Box: [2, 947, 1015, 1746], Type: <class 'list'>\n",
      "Bounding Box: [2196, 1227, 2659, 1602], Type: <class 'list'>\n",
      "Bounding Box: [2956, 1328, 3135, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2592, 1023, 2990, 1561], Type: <class 'list'>\n",
      "Bounding Box: [1622, 1176, 2694, 1657], Type: <class 'list'>\n",
      "Bounding Box: [2213, 1205, 2530, 1621], Type: <class 'list'>\n",
      "Bounding Box: [1, 981, 1011, 1753], Type: <class 'list'>\n",
      "Saving the data in database\n",
      "color: Old Lavender/n type: Truck /n timestamp : 2024-11-04_12-48-44 lat_lon : 33.9059 _ 72.3692\n",
      "I am savng the data to database\n",
      "The data has been saved successfully\n",
      "Bounding Box: [1455, 1140, 2640, 1663], Type: <class 'list'>\n",
      "Bounding Box: [2, 947, 1015, 1745], Type: <class 'list'>\n",
      "Bounding Box: [2956, 1321, 3138, 1563], Type: <class 'list'>\n",
      "Bounding Box: [1417, 1176, 1498, 1364], Type: <class 'list'>\n",
      "Bounding Box: [2354, 1291, 2702, 1571], Type: <class 'list'>\n",
      "Bounding Box: [2609, 1023, 2983, 1560], Type: <class 'list'>\n",
      "Bounding Box: [2729, 1072, 3154, 1562], Type: <class 'list'>\n",
      "Bounding Box: [1482, 1142, 2629, 1657], Type: <class 'list'>\n",
      "Bounding Box: [1, 980, 1011, 1753], Type: <class 'list'>\n",
      "Bounding Box: [2229, 1203, 2536, 1627], Type: <class 'list'>\n",
      "Bounding Box: [2, 946, 1015, 1745], Type: <class 'list'>\n",
      "Bounding Box: [1418, 1177, 1507, 1367], Type: <class 'list'>\n",
      "Bounding Box: [2346, 1279, 2734, 1581], Type: <class 'list'>\n",
      "Bounding Box: [2956, 1320, 3137, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2735, 1064, 3151, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2623, 1027, 2985, 1560], Type: <class 'list'>\n",
      "Bounding Box: [1505, 1140, 2724, 1661], Type: <class 'list'>\n",
      "Bounding Box: [1, 980, 1012, 1753], Type: <class 'list'>\n",
      "Bounding Box: [1414, 1176, 1507, 1369], Type: <class 'list'>\n",
      "Bounding Box: [2, 945, 1017, 1746], Type: <class 'list'>\n",
      "Bounding Box: [2278, 1208, 2669, 1613], Type: <class 'list'>\n",
      "Bounding Box: [2289, 1223, 2776, 1595], Type: <class 'list'>\n",
      "Bounding Box: [2957, 1321, 3138, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2713, 1061, 3148, 1564], Type: <class 'list'>\n",
      "Bounding Box: [1507, 1143, 2777, 1659], Type: <class 'list'>\n",
      "Bounding Box: [2308, 1207, 2597, 1614], Type: <class 'list'>\n",
      "Bounding Box: [1413, 1174, 1505, 1367], Type: <class 'list'>\n",
      "Bounding Box: [1, 980, 1010, 1753], Type: <class 'list'>\n",
      "Bounding Box: [1, 946, 1016, 1746], Type: <class 'list'>\n",
      "Bounding Box: [2956, 1332, 3135, 1563], Type: <class 'list'>\n",
      "Bounding Box: [2298, 1224, 2696, 1609], Type: <class 'list'>\n",
      "Bounding Box: [1502, 1490, 3672, 2160], Type: <class 'list'>\n",
      "Bounding Box: [1517, 1142, 2773, 1658], Type: <class 'list'>\n",
      "Bounding Box: [1, 980, 1011, 1753], Type: <class 'list'>\n",
      "Bounding Box: [1412, 1175, 1506, 1368], Type: <class 'list'>\n",
      "Bounding Box: [2322, 1201, 2657, 1619], Type: <class 'list'>\n",
      "Bounding Box: [3, 945, 1015, 1747], Type: <class 'list'>\n",
      "Bounding Box: [2360, 1266, 2865, 1602], Type: <class 'list'>\n",
      "Bounding Box: [2956, 1332, 3137, 1565], Type: <class 'list'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding Box: [1544, 1145, 2812, 1657], Type: <class 'list'>\n",
      "Bounding Box: [2376, 1210, 2756, 1608], Type: <class 'list'>\n",
      "Bounding Box: [1, 979, 1011, 1753], Type: <class 'list'>\n",
      "Bounding Box: [2, 945, 1015, 1747], Type: <class 'list'>\n",
      "Bounding Box: [1412, 1175, 1507, 1369], Type: <class 'list'>\n",
      "Bounding Box: [2372, 1217, 2879, 1605], Type: <class 'list'>\n",
      "Bounding Box: [2961, 1371, 3123, 1565], Type: <class 'list'>\n",
      "Bounding Box: [1500, 1504, 3737, 2160], Type: <class 'list'>\n",
      "Stopping video stream\n",
      "Starting real-time video stream\n",
      "Video saved to database and deleted from the computer successfully.\n",
      "the vdeo name is : 2024-11-04_12-49-12\n",
      "Stopping video stream\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import sys\n",
    "import cv2\n",
    "from PyQt5.QtCore import QTimer\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QMessageBox, QLineEdit,QComboBox, QListWidget\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_geolocation(ip_address):\n",
    "    url = f\"http://ip-api.com/json/{ip_address}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if data[\"status\"] == \"success\":\n",
    "        # Return latitude and longitude as floats\n",
    "        return float(data['lat']), float(data['lon'])\n",
    "    else:\n",
    "        print(\"Could not retrieve geolocation information.\")\n",
    "        return None, None  # Return None values if there's an error\n",
    "\n",
    "\n",
    "\n",
    "# Database connection\n",
    "def connect_db():\n",
    "    return mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"\",  # Replace with your password\n",
    "        database=\"viz_optilytics\"  # Replace with your database name\n",
    "    )\n",
    "######################################################################################\n",
    "# Load color reference CSV and create color lookup\n",
    "index = [\"color\", \"color_name\", \"hex\", \"R\", \"G\", \"B\"]\n",
    "csv = pd.read_csv('colors.csv', names=index, header=None)\n",
    "\n",
    "def get_color_name(bgr):\n",
    "    b, g, r = bgr\n",
    "    minimum = 10000\n",
    "    cname = \"Unknown\"\n",
    "    for i in range(len(csv)):\n",
    "        d = abs(r - int(csv.loc[i, \"R\"])) + abs(g - int(csv.loc[i, \"G\"])) + abs(b - int(csv.loc[i, \"B\"]))\n",
    "        if d <= minimum:\n",
    "            minimum = d\n",
    "            cname = csv.loc[i, \"color_name\"]\n",
    "    return cname\n",
    "\n",
    "# Function to extract color from bounding box\n",
    "def extract_color(frame, bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    average_color = np.mean(roi, axis=(0, 1))\n",
    "    color_name = get_color_name(average_color)\n",
    "    return color_name\n",
    "###############################################  \n",
    "\n",
    "def save_vehicle_data(color, vehicle_type, timestamp, loc_lat, loc_lon):\n",
    "    try:\n",
    "        conn = connect_db()\n",
    "        cursor = conn.cursor()\n",
    "        print(\"I am savng the data to database\")\n",
    "        query = \"INSERT INTO vehicles (color, vehicle_type, timestamp, loc_lat, loc_lon) VALUES (%s,%s,%s,%s,%s)\"\n",
    "        cursor.execute(query, (color, vehicle_type, timestamp, loc_lat,loc_lon))\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(\"The data has been saved successfully\")\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "\n",
    "def save_people_data(people_count, timestamp, loc_lat, loc_lon):\n",
    "    try:\n",
    "        conn = connect_db()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        query = \"INSERT INTO people (people_count, timestamp, loc_lat, loc_lon) VALUES (%s,%s,%s,%s)\"\n",
    "        cursor.execute(query, (people_count, timestamp, loc_lat,loc_lon))\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "# Save video to database with name\n",
    "def save_video_to_db(video_path, video_name, start_time, duration, end_time):\n",
    "    try:\n",
    "        conn = connect_db()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Read video file data\n",
    "        with open(video_path, 'rb') as f:\n",
    "            video_data = f.read()\n",
    "\n",
    "        # Insert video data into database\n",
    "        query = \"\"\"\n",
    "            INSERT INTO video_storage (video_name, video_file, start_time, duration, end_time)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (video_name, video_data, start_time, duration, end_time))\n",
    "        \n",
    "        # Commit transaction\n",
    "        conn.commit()\n",
    "        # Clean up\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        # Optionally remove the file from the system\n",
    "        os.remove(video_path)\n",
    "        print(\"Video saved to database and deleted from the computer successfully.\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The video file was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "# Search for videos based on date and time\n",
    "def search_videos(search_term):\n",
    "    try:\n",
    "        conn = connect_db()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # SQL query to search for videos by date or specific datetime\n",
    "        query = \"SELECT video_name FROM video_storage WHERE video_name LIKE %s\"\n",
    "        cursor.execute(query, (f\"{search_term}%\",))  # '%' allows for matching any suffix\n",
    "\n",
    "        results = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        return [result[0] for result in results]  # Extract video names from results\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "        return []\n",
    "\n",
    "class VizOptilyticsApp(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Viz Optilytics\")\n",
    "        self.setGeometry(0, 0, 1200, 700)\n",
    "\n",
    "        # Initialize QTimer (fixing the timer error)\n",
    "        self.timer = QTimer(self)  # Initialize the QTimer object\n",
    "\n",
    "        # Initialize video capture object\n",
    "        self.video_capture = None\n",
    "\n",
    "        # Detection states\n",
    "        self.detecting_person = False\n",
    "        self.detecting_vehicle = False\n",
    "\n",
    "         # Track detected persons and duration\n",
    "        self.person_detected_time = None  # To track when a person is first detected\n",
    "        self.alert_generated = False  # To track if alert is already generated\n",
    "        \n",
    "        # Load YOLOv5 model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 model\n",
    "\n",
    "         # Initialize alert labels for dynamic updates\n",
    "        self.alert_label = None\n",
    "        self.duration_label = None\n",
    "        \n",
    "        self.detection_grace_period = 10  # Number of frames to maintain detection box\n",
    "        self.frames_since_last_detection = 0\n",
    "        self.last_detection_box = None  # Store the last detected bounding box\n",
    "\n",
    "        \n",
    "        self.vehicle_positions = {}  # Stores vehicle ID -> (last position, detection start time)\n",
    "        self.stationary_alert_generated = {}  # To track if stationary alert has been generated\n",
    "\n",
    "        self.video1 = False\n",
    "        self.video2 = False\n",
    "        self.video3 = False\n",
    "        self.real_time_stream = False\n",
    "        self.record_flag = False\n",
    "\n",
    "        self.person_count = 0  # Track number of people detected\n",
    "        \n",
    "        self.initUI()\n",
    "    def initUI(self):\n",
    "        # Main Layout\n",
    "        main_layout = QtWidgets.QVBoxLayout(self)\n",
    "        main_layout.setContentsMargins(50, 50, 50, 30)\n",
    "\n",
    "        # Top Section: Title and Menu\n",
    "        top_bar = QtWidgets.QHBoxLayout()\n",
    "#         top_bar.addStretch(1)\n",
    "\n",
    "        title = QtWidgets.QLabel(\"Viz Optilytics\")\n",
    "        title.setFont(QtGui.QFont(\"Arial\", 28, QtGui.QFont.Bold))\n",
    "        title.setStyleSheet(\"color: #FFD700;\")\n",
    "        title.setAlignment(QtCore.Qt.AlignLeft)\n",
    "\n",
    "        top_bar.addWidget(title)\n",
    "        top_bar.addStretch(2)\n",
    "\n",
    "        # Create an editable QComboBox for search\n",
    "        self.search_combo = QComboBox()\n",
    "        self.search_combo.setEditable(True)  # Make the combo box editable\n",
    "        self.search_combo.setPlaceholderText(\"Enter date (YYYY-MM-DD) or datetime (YYYY-MM-DD_HH-MM-SS)\")\n",
    "        top_bar.addWidget(self.search_combo)\n",
    "\n",
    "        top_bar.addStretch(1)\n",
    "        \n",
    "        menu_button = QtWidgets.QPushButton(\"â˜°\")\n",
    "        menu_button.setStyleSheet(\"background-color: #333333; color: #FFD700; font-size: 20px; border-radius: 10px;\")\n",
    "        menu_button.setFixedSize(50, 50)\n",
    "        menu_button.setMenu(self.create_menu())\n",
    "\n",
    "        top_bar.addWidget(menu_button)\n",
    "        main_layout.addLayout(top_bar)\n",
    "\n",
    "        title_spacer = QtWidgets.QSpacerItem(20, 40, QtWidgets.QSizePolicy.Minimum, QtWidgets.QSizePolicy.Fixed)\n",
    "        main_layout.addItem(title_spacer)\n",
    "\n",
    "        # Horizontal layout split: Videos, Buttons, Alerts, etc.\n",
    "        content_layout = QtWidgets.QHBoxLayout()\n",
    "        content_layout.setSpacing(20)\n",
    "\n",
    "        # Video Buttons and Info Section\n",
    "        main_vertical_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_list_layout = QtWidgets.QVBoxLayout()\n",
    "        recorded_videos_label = QtWidgets.QLabel(\"Recorded Videos\")\n",
    "        recorded_videos_label.setFont(QtGui.QFont(\"Arial\", 16, QtGui.QFont.Bold))\n",
    "        recorded_videos_label.setStyleSheet(\"color: white;\")\n",
    "        video_list_layout.addWidget(recorded_videos_label)\n",
    "\n",
    "        # Video buttons\n",
    "        for i in range(3):\n",
    "            video_btn = QtWidgets.QPushButton(f\"Video {i + 1}\")\n",
    "            video_btn.setStyleSheet(\"background-color: #555555; color: white; border-radius: 8px; padding: 8px;\")\n",
    "            video_btn.setFixedWidth(200)\n",
    "            video_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "\n",
    "            # Connect each button to a separate function based on the index\n",
    "            if i == 0:\n",
    "                video_btn.clicked.connect(self.play_video_1)\n",
    "            elif i == 1:\n",
    "                video_btn.clicked.connect(self.play_video_2)\n",
    "            elif i == 2:\n",
    "                video_btn.clicked.connect(self.play_video_3)\n",
    "\n",
    "            video_list_layout.addWidget(video_btn)\n",
    "\n",
    "        main_vertical_layout.addLayout(video_list_layout)\n",
    "\n",
    "        detection_info_widget = QtWidgets.QWidget()\n",
    "        detection_info_layout = QtWidgets.QVBoxLayout(detection_info_widget)\n",
    "        detection_info_layout.setSpacing(0)\n",
    "        detection_info_widget.setStyleSheet(\"background-color: #444444; border-radius: 10px;\")\n",
    "\n",
    "        date_time_label = QtWidgets.QLabel(\"Date: 2024-10-11\\nTime: 12:45 PM\")\n",
    "        date_time_label.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(date_time_label)\n",
    "\n",
    "        detection_status = QtWidgets.QLabel(\"Status: Vehicle Detected\")\n",
    "        detection_status.setStyleSheet(\"color: white; font-size: 14px; padding: 8px;\")\n",
    "        detection_info_layout.addWidget(detection_status)\n",
    "        main_vertical_layout.addWidget(detection_info_widget)\n",
    "\n",
    "        # Real-time streaming button\n",
    "        real_time_btn = QtWidgets.QPushButton(\"Real-Time Streaming\")\n",
    "        real_time_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        real_time_btn.setFixedWidth(200)\n",
    "        real_time_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        real_time_btn.clicked.connect(self.start_real_time_stream)\n",
    "        main_vertical_layout.addWidget(real_time_btn)\n",
    "\n",
    "        # Analytics button\n",
    "        analytics_btn = QtWidgets.QPushButton(\"Analytics\")\n",
    "        analytics_btn.setStyleSheet(\"background-color: #666666; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        analytics_btn.setFixedWidth(200)\n",
    "        analytics_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        analytics_btn.clicked.connect(self.show_analytics)\n",
    "        main_vertical_layout.addWidget(analytics_btn)\n",
    "\n",
    "        content_layout.addLayout(main_vertical_layout)\n",
    "\n",
    "        # Video display area and other buttons\n",
    "        main_left_layout = QtWidgets.QVBoxLayout()\n",
    "\n",
    "        video_layout = QtWidgets.QVBoxLayout()\n",
    "        video_controls_layout = QtWidgets.QHBoxLayout()\n",
    "\n",
    "        # Create a layout to position the close button at the top-right corner\n",
    "        close_button = QtWidgets.QPushButton(\"X\")\n",
    "        close_button.setStyleSheet(\"background-color: red; color: white; font-size: 18px; border-radius: 10px;\")\n",
    "        close_button.setFixedSize(30, 30)\n",
    "        close_button.clicked.connect(self.stop_stream)\n",
    "\n",
    "        # Create a wrapper for the video feed and the button\n",
    "        video_feed_wrapper = QtWidgets.QWidget()\n",
    "        video_feed_layout = QtWidgets.QVBoxLayout(video_feed_wrapper)\n",
    "        video_feed_layout.setContentsMargins(0, 0, 0, 0)  # Remove margins for precise alignment\n",
    "\n",
    "        # Create a layout for the close button at the top-right corner\n",
    "        close_button_layout = QtWidgets.QHBoxLayout()\n",
    "        close_button_layout.addStretch(1)  # Add stretch to push the button to the right\n",
    "        close_button_layout.addWidget(close_button)\n",
    "\n",
    "        video_feed_layout.addLayout(close_button_layout)  # Add the close button layout\n",
    "        self.video_feed = QtWidgets.QLabel(\"Video Stream\")\n",
    "        self.video_feed.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.video_feed.setStyleSheet(\"background-color: white; color: black; border: 2px solid;\")\n",
    "        self.video_feed.setFixedSize(970, 480)  # Set a fixed size for the video feed\n",
    "        self.video_feed.setScaledContents(True)\n",
    "        self.video_feed.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)\n",
    "\n",
    "        video_feed_layout.addWidget(self.video_feed)  # Add the video feed below the button\n",
    "\n",
    "        # Add the video feed wrapper to the main video layout\n",
    "        video_layout.addWidget(video_feed_wrapper)\n",
    "\n",
    "        # Detection buttons\n",
    "        detection_buttons_layout = QtWidgets.QHBoxLayout()\n",
    "        self.record_btn = QtWidgets.QPushButton(\"Start Record\")\n",
    "        self.record_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.record_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.record_btn.clicked.connect(self.toggle_record)\n",
    "        detection_buttons_layout.addWidget(self.record_btn)\n",
    "\n",
    "       # Assigning person_detection_btn as an instance variable\n",
    "        self.person_detection_btn = QtWidgets.QPushButton(\"Person Detection\")\n",
    "        self.person_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.person_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.person_detection_btn.clicked.connect(self.toggle_person_detection)\n",
    "        detection_buttons_layout.addWidget(self.person_detection_btn)\n",
    "\n",
    "        self.vehicle_detection_btn = QtWidgets.QPushButton(\"Vehicle Detection\")\n",
    "        self.vehicle_detection_btn.setStyleSheet(\"background-color: #666666; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        self.vehicle_detection_btn.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Fixed)\n",
    "        self.vehicle_detection_btn.clicked.connect(self.toggle_vehicle_detection)\n",
    "        detection_buttons_layout.addWidget(self.vehicle_detection_btn)\n",
    "\n",
    "        video_layout.addLayout(detection_buttons_layout)\n",
    "        main_left_layout.addLayout(video_layout)\n",
    "\n",
    "        # Alerts section\n",
    "        alerts_layout = QtWidgets.QHBoxLayout()\n",
    "        self.alert_label = QtWidgets.QLabel(\"Alert: No Detection\")\n",
    "        self.alert_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.alert_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.alert_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.alert_label)\n",
    "\n",
    "        self.duration_label = QtWidgets.QLabel(\"Time Duration: 00:00:00\")\n",
    "        self.duration_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.duration_label.setFixedSize(450, 37)  # Set fixed size for the alert label\n",
    "        self.duration_label.setStyleSheet(\"background-color: green; color: white; padding: 10px; font-size: 14px; border-radius: 8px;\")\n",
    "        alerts_layout.addWidget(self.duration_label)\n",
    "\n",
    "        main_left_layout.addLayout(alerts_layout)\n",
    "        content_layout.addLayout(main_left_layout)\n",
    "\n",
    "        main_layout.addLayout(content_layout)\n",
    "        self.setStyleSheet(\"background-color: #222222; color: white;\")\n",
    "\n",
    "        self.setLayout(main_layout)\n",
    "        # Connect the textChanged signal to the search function with a timer\n",
    "        self.search_combo.lineEdit().textChanged.connect(self.on_text_changed)\n",
    "        self.search_timer = QTimer()\n",
    "        self.search_timer.setInterval(300)  # Delay in milliseconds (300ms)\n",
    "        self.search_timer.timeout.connect(self.perform_search)\n",
    "        self.search_term = \"\"\n",
    "        \n",
    "    def on_text_changed(self):\n",
    "        self.search_term = self.search_combo.lineEdit().text()\n",
    "        self.search_timer.start()  # Restart the timer on every keystroke\n",
    "\n",
    "    def perform_search(self):\n",
    "        self.search_timer.stop()  # Stop the timer\n",
    "        if not self.search_term:\n",
    "            self.search_combo.clear()  # Clear results if input is empty\n",
    "            return\n",
    "\n",
    "        results = search_videos(self.search_term)\n",
    "        self.search_combo.clear()  # Clear previous results\n",
    "        if results:\n",
    "            self.search_combo.addItems(results)  # Add results to the dropdown\n",
    "            self.search_combo.showPopup()  # Show the dropdown with results\n",
    "        else:\n",
    "            self.search_combo.addItem(\"No videos found matching the search criteria.\")\n",
    "\n",
    "    \n",
    "    def toggle_record(self):\n",
    "        if not self.record_flag:\n",
    "            self.record_flag = True\n",
    "            self.record_btn.setText(\"Stop Record\")\n",
    "            self.record_options()\n",
    "        else:\n",
    "            self.record_flag = False\n",
    "            self.record_btn.setText(\"Start Record\")\n",
    "\n",
    "    def record_options(self):\n",
    "        msg_box = QMessageBox()\n",
    "        msg_box.setWindowTitle(\"Choose Recording Type\")\n",
    "        msg_box.setText(\"Do you want to record the full video or use optimal storage?\")\n",
    "        full_button = msg_box.addButton(\"Full Video\", QMessageBox.YesRole)\n",
    "        optimal_button = msg_box.addButton(\"Optimal Storage\", QMessageBox.NoRole)\n",
    "        msg_box.exec_()\n",
    "        \n",
    "        if msg_box.clickedButton() == full_button:\n",
    "            self.record_and_save_video(optimal=False)\n",
    "        elif msg_box.clickedButton() == optimal_button:\n",
    "            self.record_and_save_video(optimal=True)\n",
    "\n",
    "    def record_and_save_video(self, optimal):\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        output_file = f\"{now}.avi\"\n",
    "        self.video_display(output_file, optimal)  # Start the video display and recording\n",
    "\n",
    "        # Notify the user that the recording has started\n",
    "        QMessageBox.information(self, \"Recording Started\", \"Video recording in progress...\")\n",
    "        # Store the start time\n",
    "        self.start_time = datetime.now()\n",
    "        # Connect the timer to check recording status\n",
    "        self.timer.timeout.connect(lambda: self.check_recording_status(output_file, now, self.start_time))\n",
    "\n",
    "    def check_recording_status(self, output_file, video_name, start_time):\n",
    "        if not self.record_flag:  # Check if recording is complete\n",
    "            self.timer.stop()  # Stop the timer\n",
    "            # Calculate the end time and duration\n",
    "            end_time = datetime.now()\n",
    "            duration = end_time - start_time  # Calculate duration as a timedelta\n",
    "            # Release the video writer if it's active\n",
    "            if hasattr(self, 'out') and self.out is not None:\n",
    "                self.out.release()\n",
    "            # Format times for display or database storage\n",
    "            start_time_str = start_time.strftime(\"%H:%M:%S\")\n",
    "            end_time_str = end_time.strftime(\"%H:%M:%S\")\n",
    "            duration_seconds = int(duration.total_seconds())\n",
    "            duration_hours, remainder = divmod(duration_seconds, 3600)\n",
    "            duration_minutes, duration_seconds = divmod(remainder, 60)\n",
    "            duration_str = f\"{duration_hours:02}:{duration_minutes:02}:{duration_seconds:02}\"\n",
    "\n",
    "            # Save the video to the database with details\n",
    "            save_video_to_db(output_file, video_name, start_time_str, duration_str, end_time_str)\n",
    "            print(f\"the vdeo name is : {video_name}\")\n",
    "            # Notify the user that recording is complete\n",
    "            QMessageBox.information(self, \"Recording Complete\", \"Video recorded and saved to database successfully.\")\n",
    "            self.video_display()\n",
    "            \n",
    "    def video_display(self, output_path=None, optimal=False):\n",
    "#         self.optimal = optimal\n",
    "        if self.real_time_stream:\n",
    "            self.video_capture = cv2.VideoCapture(0)\n",
    "        elif self.video1:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\853889-hd_1920_1080_25fps.mp4\")\n",
    "        elif self.video2:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\2273134-hd_1280_720_30fps.mp4\")\n",
    "        elif self.video3:\n",
    "            self.video_capture = cv2.VideoCapture(r\"C:\\Users\\mahnoor\\Downloads\\12494476_3840_2160_50fps.mp4\")\n",
    "        if self.record_flag:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            self.out = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))\n",
    "        ret, prev_frame = self.video_capture.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to access the camera\")\n",
    "            return\n",
    "        if self.record_flag:\n",
    "            self.prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) if optimal else None\n",
    "        # Disconnect previous connections to avoid duplicates\n",
    "        try:\n",
    "            self.timer.timeout.disconnect()\n",
    "        except TypeError:\n",
    "            # No existing connection to disconnect\n",
    "            pass\n",
    "        # Connect the update_frame method properly without calling it immediately\n",
    "        self.timer.timeout.connect(lambda: self.update_frame(optimal))\n",
    "        self.timer.start(20)\n",
    "    \n",
    "    def update_frame(self, optimal=False):\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if ret:\n",
    "            if self.record_flag and hasattr(self, 'out') and self.out is not None:\n",
    "                if optimal:\n",
    "                    # Convert current frame to grayscale for comparison\n",
    "                    curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                    # Compute the absolute difference between current and previous frames\n",
    "                    diff = cv2.absdiff(self.prev_gray, curr_gray)\n",
    "                    _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "                    non_zero_count = cv2.countNonZero(thresh)\n",
    "\n",
    "                    # Save the frame only if there is significant change\n",
    "                    if non_zero_count > 10000:\n",
    "                        self.out.write(frame)\n",
    "                        self.prev_gray = curr_gray  # Update the previous frame\n",
    "                else:\n",
    "                    # Full video recording, save every frame\n",
    "                    self.out.write(frame)\n",
    "\n",
    "            # Convert the frame to RGB for display\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if self.detecting_person:\n",
    "                frame = self.detect_person(frame)\n",
    "            if self.detecting_vehicle:\n",
    "                frame = self.detect_vehicle(frame)\n",
    "\n",
    "            # Get the frame dimensions and convert to QImage\n",
    "            h, w, ch = frame.shape\n",
    "            bytes_per_line = ch * w\n",
    "            qimg = QImage(frame.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "\n",
    "            self.video_feed.setPixmap(QPixmap.fromImage(qimg))\n",
    "        else:\n",
    "            print(\"Error: Unable to read video frame\")\n",
    "\n",
    "    def stop_stream(self):\n",
    "        print(\"Stopping video stream\")\n",
    "        if self.video_capture is not None:\n",
    "            self.video_capture.release()\n",
    "#             self.out.release()\n",
    "            self.video_capture = None\n",
    "            self.video_feed.clear()\n",
    "            self.timer.stop()\n",
    "\n",
    "    def toggle_person_detection(self):\n",
    "        \"\"\"Toggle person detection on or off.\"\"\"\n",
    "        self.detecting_person = not self.detecting_person  # Toggle detection state\n",
    "\n",
    "        if self.detecting_person:\n",
    "            self.person_detection_btn.setText(\"Stop Person Detection\")\n",
    "        else:\n",
    "            self.person_detection_btn.setText(\"Start Person Detection\")\n",
    "\n",
    "            # Reset detection states when stopping\n",
    "            self.person_detected_time = None\n",
    "            self.alert_generated = False\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setText(\"Time Duration: 00:00:00\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "\n",
    "    def detect_person(self, frame):\n",
    "        # Perform inference\n",
    "        results = self.model(frame)\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        person_count = 0\n",
    "        for *box, conf, cls in predictions:\n",
    "            if int(cls) == 0:  # Class index for person\n",
    "                person_count += 1\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "                cv2.putText(frame, f'Person {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Update the number of people detected in the frame\n",
    "        self.person_count = person_count\n",
    "\n",
    "        # Check if person count exceeds 2\n",
    "        if person_count > 2:\n",
    "            self.update_person_detection_time()\n",
    "        else:\n",
    "            self.reset_person_detection()\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def update_person_detection_time(self):\n",
    "        \"\"\"Track how long multiple people have been in the frame and update alert if needed.\"\"\"\n",
    "        if self.person_detected_time is None:\n",
    "            self.person_detected_time = time.time()  # Start the timer\n",
    "\n",
    "        duration = time.time() - self.person_detected_time\n",
    "\n",
    "        # Format the duration\n",
    "        hours, rem = divmod(duration, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        duration_str = \"{:02}:{:02}:{:02}\".format(int(hours), int(minutes), int(seconds))\n",
    "\n",
    "        # Update duration label\n",
    "        self.duration_label.setText(f\"Time Duration: {duration_str}\")\n",
    "\n",
    "        # Check if detection duration exceeds the threshold (5 minutes or 300 seconds)\n",
    "        if duration > 5 and not self.alert_generated:\n",
    "            self.alert_label.setText(\"ALERT: More than 2 people detected for over 5 seconds!\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "            self.alert_generated = True\n",
    "            # Print head count and formatted time\n",
    "            formatted_time = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\"The number of head count is {self.person_count}\")\n",
    "            print(f\"The time of detection is {formatted_time}\")\n",
    "            loc_lat, loc_lon = get_geolocation(\"182.183.56.95\")\n",
    "            save_people_data(self.person_count, formatted_time, loc_lat, loc_lon)\n",
    "\n",
    "    def reset_person_detection(self):\n",
    "        \"\"\"Reset the detection state if fewer than three people are found.\"\"\"\n",
    "        if self.person_detected_time is not None:\n",
    "            self.person_detected_time = None  # Reset the timer\n",
    "            self.duration_label.setText(\"Time Duration: 00:00:00\")\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.alert_generated = False\n",
    "            self.person_count = 0  # Reset the person count to zero\n",
    "\n",
    "\n",
    "            \n",
    "    def detect_vehicle(self, frame):\n",
    "        # Perform inference using YOLOv5 model\n",
    "        results = self.model(frame)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = results.pred[0]\n",
    "\n",
    "        current_time = time.time()  # Get the current time\n",
    "        vehicle_detected = False  # Flag to check if any vehicle is detected\n",
    "\n",
    "        # Dictionary to map class ids to vehicle types\n",
    "        vehicle_class_map = {\n",
    "            2: \"Car\",        # Car\n",
    "            3: \"Motorcycle\", # Motorcycle\n",
    "            5: \"Bus\",       # Bus\n",
    "            7: \"Truck\"      # Truck\n",
    "        }\n",
    "\n",
    "        for *box, conf, cls in predictions:\n",
    "            cls = int(cls)  # Convert cls to an integer for checking\n",
    "            box = list(map(int, box))  # Ensure box contains integers\n",
    "#             print(f\"Bounding Box: {box}, Type: {type(box)}\")  # Debugging statement\n",
    "\n",
    "            if cls in vehicle_class_map:  # Only process valid vehicle classes\n",
    "                vehicle_type = vehicle_class_map[cls]\n",
    "                x1, y1, x2, y2 = box  # Unpacking the corrected box\n",
    "                \n",
    "                # Calculate the center of the bounding box\n",
    "                vehicle_center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "                vehicle_detected = True  # Set the flag to True since we detected a vehicle\n",
    "\n",
    "                # Use the center position to track vehicle movement\n",
    "                if vehicle_type not in self.vehicle_positions:\n",
    "                    # If this is a new vehicle, start tracking it\n",
    "                    self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                    self.stationary_alert_generated[vehicle_type] = False\n",
    "                else:\n",
    "                    # Get the previous position and detection start time\n",
    "                    prev_position, start_time = self.vehicle_positions[vehicle_type]\n",
    "\n",
    "                    # Calculate movement distance between current and previous position\n",
    "                    movement_distance = np.linalg.norm(np.array(vehicle_center) - np.array(prev_position))\n",
    "\n",
    "                    # Define a movement threshold (e.g., 20 pixels)\n",
    "                    movement_threshold = 20\n",
    "\n",
    "                    if movement_distance < movement_threshold:\n",
    "                        # If the vehicle has not moved significantly, check how long it's been stationary\n",
    "                        stationary_duration = current_time - start_time\n",
    "\n",
    "                        # Update the time duration label\n",
    "                        self.duration_label.setText(f\"Vehicle {vehicle_type} stationary for {int(stationary_duration)} seconds\")\n",
    "\n",
    "                        # Check if the stationary duration exceeds 5 minutes (300 seconds)\n",
    "                        if stationary_duration > 5 and not self.stationary_alert_generated[vehicle_type]:\n",
    "                            self.alert_label.setText(f\"ALERT: {vehicle_type} stationary for over 5 minutes!\")\n",
    "                            self.alert_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.duration_label.setStyleSheet(\"background-color: red; color: white;\")\n",
    "                            self.stationary_alert_generated[vehicle_type] = True  # Mark that the alert has been generated\n",
    "                            color = extract_color(frame, box)  # Corrected bounding box variable\n",
    "                            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "                            loc_lat, loc_lon = get_geolocation(\"182.183.56.95\")  # Consider making IP dynamic\n",
    "                            print(\"Saving the data in database\")\n",
    "                            print(f\"color: {color}/n type: {vehicle_type} /n timestamp : {timestamp} lat_lon : {loc_lat} _ {loc_lon}\")\n",
    "                            save_vehicle_data(color, vehicle_type, timestamp, loc_lat, loc_lon)\n",
    "                    else:\n",
    "                        # If the vehicle moved, reset the detection start time\n",
    "                        self.vehicle_positions[vehicle_type] = (vehicle_center, current_time)\n",
    "                        self.stationary_alert_generated[vehicle_type] = False\n",
    "\n",
    "                # Draw the bounding box and label the vehicle\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'{vehicle_type} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # If no vehicle is detected, reset the alert and duration labels\n",
    "        if not vehicle_detected:\n",
    "            self.alert_label.setText(\"No vehicle detected\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setText(\"No detection duration: 00 seconds\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "        return frame\n",
    "    \n",
    "    \n",
    "    def toggle_vehicle_detection(self):\n",
    "        \"\"\"Toggle vehicle detection on and off.\"\"\"\n",
    "        self.detecting_vehicle = not self.detecting_vehicle  # Toggle the state\n",
    "        if self.detecting_vehicle:\n",
    "            self.vehicle_detection_btn.setText(\"Stop vehicle Detection\")\n",
    "        else:\n",
    "            self.vehicle_detection_btn.setText(\"Start vehicle Detection\")\n",
    "            self.alert_generated = False\n",
    "            self.alert_label.setText(\"Alert: No Detection\")\n",
    "            self.alert_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "            self.duration_label.setText(\"Time Duration: 00:00:00\")\n",
    "            self.duration_label.setStyleSheet(\"background-color: green; color: white;\")\n",
    "\n",
    "    def create_menu(self):\n",
    "        menu = QtWidgets.QMenu(self)\n",
    "        menu.addAction(\"Option 1\", self.option1_action)\n",
    "        menu.addAction(\"Option 2\", self.option2_action)\n",
    "        return menu\n",
    "\n",
    "    def start_real_time_stream(self):\n",
    "        print(\"Starting real-time video stream\")\n",
    "        self.real_time_stream=True\n",
    "        # Start video capture (assuming you have a webcam at index 0)\n",
    "        self.video_display()  # Call video_feed to initiate video capture\n",
    "            \n",
    "   # Define the three separate play video functions\n",
    "    def play_video_1(self):\n",
    "        print(\"Playing Video 1\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = True  # Set instance variable\n",
    "        self.video2 = False  # Reset other video flags\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_2(self):\n",
    "        print(\"Playing Video 2\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = True\n",
    "        self.video3 = False\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    def play_video_3(self):\n",
    "        print(\"Playing Video 3\")\n",
    "        self.real_time_stream=False\n",
    "        self.video1 = False  # Reset other video flags\n",
    "        self.video2 = False\n",
    "        self.video3 = True\n",
    "        self.stop_stream()  # Stop any current stream before starting a video\n",
    "        self.video_display()  # Call video_feed to start playing the video\n",
    "\n",
    "    # Add your logic to play video 3 here\n",
    "    def show_analytics(self):\n",
    "        print(\"Showing analytics\")\n",
    "\n",
    "    def option1_action(self):\n",
    "        print(\"Option 1 selected\")\n",
    "\n",
    "    def option2_action(self):\n",
    "        print(\"Option 2 selected\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    window = VizOptilyticsApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e22e39",
   "metadata": {},
   "source": [
    "# CHECKING THE VIDEOS RECORDED BY RETRIVING THEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068c93a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video retrieved successfully.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import mysql.connector\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QLabel, QFileDialog, QMessageBox, QLineEdit\n",
    "from PyQt5.QtMultimedia import QMediaPlayer, QMediaContent\n",
    "from PyQt5.QtCore import QUrl\n",
    "from PyQt5.QtMultimediaWidgets import QVideoWidget\n",
    "import os\n",
    "\n",
    "# Database connection\n",
    "def connect_db():\n",
    "    return mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"\",  # Replace with your password\n",
    "        database=\"viz_optilytics\"  # Replace with your database name\n",
    "    )\n",
    "\n",
    "\n",
    "# Retrieve video from database using video name\n",
    "def retrieve_video_from_db(video_name):\n",
    "    try:\n",
    "        conn = connect_db()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        query = \"SELECT video_file FROM video_storage WHERE video_name = %s\"\n",
    "        cursor.execute(query, (video_name,))\n",
    "        result = cursor.fetchone()\n",
    "\n",
    "        if result:\n",
    "            video_data = result[0]\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"Video retrieved successfully.\")\n",
    "            return video_data\n",
    "        else:\n",
    "            print(\"No video found with that name.\")\n",
    "            return None\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# PyQt5 UI\n",
    "class VideoApp(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle('Video Recorder and Player')\n",
    "        self.setGeometry(100, 100, 800, 600)\n",
    "\n",
    "        self.layout = QVBoxLayout()\n",
    "\n",
    "        \n",
    "\n",
    "        self.retrieve_btn = QPushButton('Retrieve and Play Video')\n",
    "        self.retrieve_btn.clicked.connect(self.retrieve_and_play_video)\n",
    "\n",
    "        self.stop_btn = QPushButton('Stop Video')  # Stop video button\n",
    "        self.stop_btn.clicked.connect(self.stop_video)\n",
    "\n",
    "        self.player = QMediaPlayer(None, QMediaPlayer.VideoSurface)\n",
    "        self.video_widget = QVideoWidget()\n",
    "\n",
    "        # Text input for video name\n",
    "        self.video_name_input = QLineEdit()\n",
    "        self.video_name_input.setPlaceholderText(\"Enter video name\")\n",
    "\n",
    "\n",
    "        self.layout.addWidget(self.video_name_input)  # Add the input field to the layout\n",
    "        self.layout.addWidget(self.retrieve_btn)\n",
    "        self.layout.addWidget(self.stop_btn)  # Add stop button to layout\n",
    "        self.layout.addWidget(self.video_widget)\n",
    "\n",
    "        self.setLayout(self.layout)\n",
    "        self.player.setVideoOutput(self.video_widget)\n",
    "\n",
    "    def retrieve_and_play_video(self):\n",
    "        # Get the video name from the input field\n",
    "        video_name = self.video_name_input.text()\n",
    "\n",
    "        if video_name:\n",
    "            video_data = retrieve_video_from_db(video_name)  # Get the video data\n",
    "            if video_data:\n",
    "                # Save the video data temporarily to play it\n",
    "                output_path = \"temp_video.avi\"  # Use a temporary file name\n",
    "                with open(output_path, 'wb') as f:\n",
    "                    f.write(video_data)\n",
    "                self.play_video(output_path)  # Play the video from the temporary file\n",
    "            else:\n",
    "                QMessageBox.warning(self, \"Error\", \"No video found.\")\n",
    "        else:\n",
    "            QMessageBox.warning(self, \"Invalid Input\", \"Please enter a valid video name.\")\n",
    "\n",
    "    def play_video(self, video_path):\n",
    "        self.player.setMedia(QMediaContent(QUrl.fromLocalFile(video_path)))\n",
    "        self.player.play()\n",
    "\n",
    "    def stop_video(self):\n",
    "        self.player.stop()\n",
    "        self.player.setMedia(QMediaContent())  # Clear the media\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    app = QApplication(sys.argv)\n",
    "    window = VideoApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
